{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82890954",
   "metadata": {},
   "source": [
    "\n",
    "<a id='optgrowth'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99804d8",
   "metadata": {},
   "source": [
    "# 最优增长 I：随机最优增长模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f095c",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [最优增长 I：随机最优增长模型](#最优增长-I：随机最优增长模型)  \n",
    "  - [概述](#概述)  \n",
    "  - [模型](#模型)  \n",
    "  - [计算](#计算)  \n",
    "  - [练习](#练习)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a5969",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "在本讲座中，我们将研究一个包含单个代理人的简单最优增长模型。\n",
    "\n",
    "该模型是标准的单部门无限期增长模型的一个版本，这在以下文献中有研究：\n",
    "\n",
    "- [[Stokey *et al.*, 1989](https://python.quantecon.org/zreferences.html#id222)]，第2章  \n",
    "- [[Ljungqvist and Sargent, 2018](https://python.quantecon.org/zreferences.html#id186)]，第3.1节  \n",
    "- [EDTC](http://johnstachurski.net/edtc.html)，第1章  \n",
    "- [[Sundaram, 1996](https://python.quantecon.org/zreferences.html#id224)]，第12章  \n",
    "\n",
    "\n",
    "这是对我们之前研究的简单[蛋糕食用问题](https://python.quantecon.org/cake_eating_problem.html)的扩展。\n",
    "\n",
    "这个扩展包括\n",
    "\n",
    "- 通过生产函数实现的非线性储蓄回报，以及  \n",
    "- 由于生产冲击导致的随机回报。  \n",
    "\n",
    "\n",
    "尽管有这些添加，这个模型仍然相对简单。\n",
    "\n",
    "我们将其视为通向更复杂模型的垫脚石。\n",
    "\n",
    "我们使用动态规划和一系列数值技术来求解这个模型。\n",
    "\n",
    "在这第一节最优增长课程中，解决方法将是值函数迭代（VFI）。\n",
    "\n",
    "虽然这第一节课中的代码运行较慢，但在接下来的几节课中，我们将使用各种技术来大幅提高执行速度。\n",
    "\n",
    "让我们从一些导入开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749091f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "FONTPATH = \"fonts/SourceHanSerifSC-SemiBold.otf\"\n",
    "mpl.font_manager.fontManager.addfont(FONTPATH)\n",
    "plt.rcParams['font.family'] = ['Source Han Serif SC']\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #设置默认图形大小\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbeab1c",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "\n",
    "<a id='index-1'></a>\n",
    "考虑一个主体在时间 $ t $ 拥有数量为 $ y_t \\in \\mathbb R_+ := [0, \\infty) $ 的消费品。\n",
    "\n",
    "这些产出可以被消费或投资。\n",
    "\n",
    "当商品被投资时，它会一比一地转化为资本。\n",
    "\n",
    "由此产生的资本存量，在此用 $ k_{t+1} $ 表示，随后将用于生产。\n",
    "\n",
    "生产是随机的，因为它还取决于在当前期末实现的冲击 $ \\xi_{t+1} $。\n",
    "\n",
    "下一期的产出为\n",
    "\n",
    "$$\n",
    "y_{t+1} := f(k_{t+1}) \\xi_{t+1}\n",
    "$$\n",
    "\n",
    "其中 $ f \\colon \\mathbb R_+ \\to \\mathbb R_+ $ 被称为生产函数。\n",
    "\n",
    "资源约束为\n",
    "\n",
    "\n",
    "<a id='equation-outcsdp0'></a>\n",
    "$$\n",
    "k_{t+1} + c_t \\leq y_t \\tag{40.1}\n",
    "$$\n",
    "\n",
    "且所有变量都必须为非负数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c05a9",
   "metadata": {},
   "source": [
    "### 假设和说明\n",
    "\n",
    "在接下来的内容中，\n",
    "\n",
    "- 序列 $ \\{\\xi_t\\} $ 被假定为独立同分布(IID)。  \n",
    "- 每个 $ \\xi_t $ 的共同分布将用 $ \\phi $ 表示。  \n",
    "- 假设生产函数$ f $是递增且连续的。  \n",
    "- 资本折旧并未明确表示，但可以被整合到生产函数中。  \n",
    "\n",
    "\n",
    "虽然许多其他随机增长模型的处理方法使用$ k_t $作为状态变量，我们将使用$ y_t $。\n",
    "\n",
    "这将使我们能够处理随机模型的同时仅保持一个状态变量。\n",
    "\n",
    "我们在其他一些讲座中考虑了替代的状态和时序规范。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d39de",
   "metadata": {},
   "source": [
    "### 优化\n",
    "\n",
    "给定$ y_0 $，代理人希望最大化\n",
    "\n",
    "\n",
    "<a id='equation-texs0-og2'></a>\n",
    "$$\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\right] \\tag{40.2}\n",
    "$$\n",
    "\n",
    "约束条件为\n",
    "\n",
    "\n",
    "<a id='equation-og-conse'></a>\n",
    "$$\n",
    "y_{t+1} = f(y_t - c_t) \\xi_{t+1}\n",
    "\\quad \\text{和} \\quad\n",
    "0 \\leq c_t \\leq y_t\n",
    "\\quad \\text{对所有} t \\tag{40.3}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "- $ u $是有界、连续且严格递增的效用函数，且  \n",
    "- $ \\beta \\in (0, 1) $是贴现因子。  \n",
    "\n",
    "\n",
    "在[(40.3)](#equation-og-conse)中，我们假设资源约束[(40.1)](#equation-outcsdp0)是以等式形式成立的——这是合理的，因为$ u $是严格递增的，在最优状态下不会浪费任何产出。\n",
    "\n",
    "总的来说，代理人的目标是选择一个消费路径$ c_0, c_1, c_2, \\ldots $，该路径需要：\n",
    "\n",
    "1. 非负，  \n",
    "1. 在[(40.1)](#equation-outcsdp0)意义上可行，  \n",
    "1. 最优，即相对于所有其他可行的消费序列，最大化[(40.2)](#equation-texs0-og2)，以及  \n",
    "1. *适应性*，即行动$ c_t $只依赖于可观察的结果，而不依赖于未来的结果，如$ \\xi_{t+1} $。  \n",
    "\n",
    "\n",
    "在当前情况下：\n",
    "\n",
    "- $ y_t $被称为*状态*变量——它概括了每个时期开始时的”世界状态”。  \n",
    "- $ c_t $被称为*控制*变量——是代理人在观察状态后每期选择的值。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d1de5",
   "metadata": {},
   "source": [
    "### 策略函数方法\n",
    "\n",
    "\n",
    "<a id='index-2'></a>\n",
    "解决这个问题的一种方法是寻找最佳的**策略函数**。\n",
    "\n",
    "策略函数是一个从过去和现在的可观察变量映射到当前行动的函数。\n",
    "\n",
    "我们将特别关注**马尔可夫策略**，它是从当前状态 $ y_t $ 映射到当前行动 $ c_t $ 的函数。\n",
    "\n",
    "对于像这样的动态规划问题（实际上对于任何[马尔可夫决策过程](https://en.wikipedia.org/wiki/Markov_decision_process)），最优策略总是一个马尔可夫策略。\n",
    "\n",
    "换句话说，当前状态 $ y_t $ 为历史提供了一个[充分统计量](https://en.wikipedia.org/wiki/Sufficient_statistic)，用于做出当前的最优决策。\n",
    "\n",
    "这很直观，但如果你想要证明，可以在[[Stokey *et al.*, 1989](https://python.quantecon.org/zreferences.html#id222)]（第4.1节）等教材中找到。\n",
    "\n",
    "此后我们将专注于寻找最佳马尔可夫策略。\n",
    "\n",
    "在我们的情况下，马尔可夫策略是一个函数 $ \\sigma \\colon $\n",
    "\n",
    "\\mathbb R_+ \\to \\mathbb R_+\\$，其中状态通过以下方式映射到行动\n",
    "\n",
    "$$\n",
    "c_t = \\sigma(y_t) \\quad \\text{对所有 } t\n",
    "$$\n",
    "\n",
    "在下文中，如果 $ \\sigma $ 满足以下条件，我们称之为*可行消费策略*\n",
    "\n",
    "\n",
    "<a id='equation-idp-fp-og2'></a>\n",
    "$$\n",
    "0 \\leq \\sigma(y) \\leq y\n",
    "\\quad \\text{对所有} \\quad\n",
    "y \\in \\mathbb R_+ \\tag{40.4}\n",
    "$$\n",
    "\n",
    "换句话说，可行消费策略是一个遵守资源约束的马尔可夫策略。\n",
    "\n",
    "所有可行消费策略的集合将用 $ \\Sigma $ 表示。\n",
    "\n",
    "每个 $ \\sigma \\in \\Sigma $ 都通过以下方式确定一个[连续状态马尔可夫过程](https://python-advanced.quantecon.org/stationary_densities.html) $ \\{y_t\\} $ 来表示产出\n",
    "\n",
    "\n",
    "<a id='equation-firstp0-og2'></a>\n",
    "$$\n",
    "y_{t+1} = f(y_t - \\sigma(y_t)) \\xi_{t+1},\n",
    "\\quad y_0 \\text{ 给定} \\tag{40.5}\n",
    "$$\n",
    "\n",
    "这是当我们选择并坚持策略 $ \\sigma $ 时产出的时间路径。\n",
    "\n",
    "我们将这个过程代入目标函数得到\n",
    "\n",
    "\n",
    "<a id='equation-texss'></a>\n",
    "$$\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\,\n",
    "\\right] =\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(y_t)) \\,\n",
    "\\right] \\tag{40.6}\n",
    "$$\n",
    "\n",
    "这是永远遵循策略 $ \\sigma $ 的总期望现值，给定初始收入 $ y_0 $。\n",
    "\n",
    "目标是选择一个能使这个数值尽可能大的策略。\n",
    "\n",
    "下一节将更正式地介绍这些概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd54218",
   "metadata": {},
   "source": [
    "### 最优性\n",
    "\n",
    "与给定策略 $ \\sigma $ 相关的 $ \\sigma $ 是由以下映射定义的\n",
    "\n",
    "\n",
    "<a id='equation-vfcsdp00'></a>\n",
    "$$\n",
    "v_{\\sigma}(y) =\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(y_t)) \\right] \\tag{40.7}\n",
    "$$\n",
    "\n",
    "其中 $ \\{y_t\\} $ 由方程 [(40.5)](#equation-firstp0-og2) 给出，且 $ y_0 = y $。\n",
    "\n",
    "换句话说，这是从初始条件 $ y $ 开始遵循策略 $ \\sigma $ 的终身价值。\n",
    "\n",
    "**价值函数**定义为\n",
    "\n",
    "\n",
    "<a id='equation-vfcsdp0'></a>\n",
    "$$\n",
    "v^*(y) := \\sup_{\\sigma \\in \\Sigma} \\; v_{\\sigma}(y) \\tag{40.8}\n",
    "$$\n",
    "\n",
    "价值函数给出了在考虑所有可行策略后，从状态 $ y $ 可以获得的最大价值。\n",
    "\n",
    "如果一个策略 $ \\sigma \\in \\Sigma $ 在所有 $ y \\in \\mathbb R_+ $ 上都能达到 [(40.8)](#equation-vfcsdp0) 中的上确界，则称其为**最优**策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f29639",
   "metadata": {},
   "source": [
    "### 贝尔曼方程\n",
    "\n",
    "在我们对效用函数和生产函数的假设下，在 [(40.8)](#equation-vfcsdp0) 中定义的值函数也满足一个**贝尔曼方程**。\n",
    "\n",
    "对于这个问题，贝尔曼方程的形式为\n",
    "\n",
    "\n",
    "<a id='equation-fpb30'></a>\n",
    "$$\n",
    "v(y) = \\max_{0 \\leq c \\leq y}\n",
    "    \\left\\{\n",
    "        u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "    \\right\\}\n",
    "\\qquad (y \\in \\mathbb R_+) \\tag{40.9}\n",
    "$$\n",
    "\n",
    "这是一个关于 $ v $ 的*泛函方程*。\n",
    "\n",
    "项 $ \\int v(f(y - c) z) \\phi(dz) $ 可以理解为在以下条件下的预期下一期价值：\n",
    "\n",
    "- 使用 $ v $ 来衡量价值  \n",
    "- 状态为 $ y $  \n",
    "- 消费设定为 $ c $  \n",
    "\n",
    "\n",
    "如 [EDTC](http://johnstachurski.net/edtc.html) 定理10.1.11和其他多个文献所示：\n",
    "\n",
    "> *值函数* $ v^* $ *满足贝尔曼方程*\n",
    "\n",
    "\n",
    "换句话说，当 $ v=v^* $ 时，[(40.9)](#equation-fpb30) 成立。\n",
    "\n",
    "直观上来说，从给定状态获得的最大价值可以通过以下两者的最优权衡得到：\n",
    "\n",
    "- 当前行动带来的即时回报，与  \n",
    "- 该行动导致的未来状态的折现期望价值  \n",
    "\n",
    "\n",
    "贝尔曼方程很重要，因为它为我们提供了关于价值函数的更多信息。\n",
    "\n",
    "它还提示了一种计算价值函数的方法，我们将在下面讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0846ffc",
   "metadata": {},
   "source": [
    "### 贪婪策略\n",
    "\n",
    "价值函数的主要重要性在于我们可以用它来计算最优策略。\n",
    "\n",
    "具体细节如下。\n",
    "\n",
    "给定在 $ \\mathbb R_+ $ 上的连续函数 $ v $，如果对于每个 $ y \\in \\mathbb R_+ $，\n",
    "$ \\sigma(y) $ 是以下问题的解，我们就说 $ \\sigma \\in \\Sigma $ 是 $ v $-**贪婪**的：\n",
    "\n",
    "\n",
    "<a id='equation-defgp20'></a>\n",
    "$$\n",
    "\\max_{0 \\leq c \\leq y}\n",
    "    \\left\\{\n",
    "    u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "    \\right\\} \\tag{40.10}\n",
    "$$\n",
    "\n",
    "换句话说，当 $ v $ 被视为价值函数时，如果 $ \\sigma \\in \\Sigma $ 能够最优地权衡当前和未来回报，那么它就是 $ v $-贪婪的。\n",
    "\n",
    "在我们的设定中，我们有以下关键结果\n",
    "\n",
    "- 一个可行的消费政策是最优的，当且仅当它是$ v^* $-贪婪的。  \n",
    "\n",
    "\n",
    "这个直觉与贝尔曼方程的直觉类似，这在[(40.9)](#equation-fpb30)之后已经提供。\n",
    "\n",
    "参见[EDTC](http://johnstachurski.net/edtc.html)的定理10.1.11。\n",
    "\n",
    "因此，一旦我们对$ v^* $有了很好的近似，我们就可以通过计算相应的贪婪策略来计算（近似）最优策略。\n",
    "\n",
    "这样做的优势在于我们现在求解的是一个维度更低的优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca3a7f",
   "metadata": {},
   "source": [
    "### 贝尔曼算子\n",
    "\n",
    "那么，我们应该如何计算价值函数呢？\n",
    "\n",
    "一种方法是使用所谓的**贝尔曼算子**。\n",
    "\n",
    "（算子是一个将函数映射到函数的映射。）\n",
    "\n",
    "贝尔曼算子用$ T $表示，定义为\n",
    "\n",
    "\n",
    "<a id='equation-fcbell20-optgrowth'></a>\n",
    "$$\n",
    "Tv(y) := \\max_{0 \\leq c \\leq y}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "\\qquad (y \\in \\mathbb R_+) \\tag{40.11}\n",
    "$$\n",
    "\n",
    "换句话说，$ T $ 将函数 $ v $ 转换为由[(40.11)](#equation-fcbell20-optgrowth)定义的新函数 $ Tv $。\n",
    "\n",
    "根据构造，Bellman方程[(40.9)](#equation-fpb30)的解集*恰好等于* $ T $ 的不动点集。\n",
    "\n",
    "例如，如果 $ Tv = v $，那么对于任意 $ y \\geq 0 $，\n",
    "\n",
    "$$\n",
    "v(y)\n",
    "= Tv(y)\n",
    "= \\max_{0 \\leq c \\leq y}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v^*(f(y - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "这正好说明 $ v $ 是Bellman方程的一个解。\n",
    "\n",
    "由此可知 $ v^* $ 是 $ T $ 的一个不动点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213a191",
   "metadata": {},
   "source": [
    "### 理论结果回顾\n",
    "\n",
    "\n",
    "<a id='index-3'></a>\n",
    "还可以证明，在上确界距离下，$ T $ 是定义在 $ \\mathbb R_+ $ 上的连续有界函数集上的压缩映射\n",
    "\n",
    "$$\n",
    "\\rho(g, h) = \\sup_{y \\geq 0} |g(y) - h(y)|\n",
    "$$\n",
    "\n",
    "参见 [EDTC](http://johnstachurski.net/edtc.html)，引理10.1.18。\n",
    "\n",
    "因此，在这个集合中它有唯一的不动点，我们知道这就等于价值函数。\n",
    "\n",
    "由此可知\n",
    "\n",
    "- 值函数 $ v^* $ 是有界且连续的。  \n",
    "- 从任何有界且连续的 $ v $ 开始，通过迭代应用 $ T $ 生成的序列 $ v, Tv, T^2v, \\ldots $ 将一致收敛到 $ v^* $。  \n",
    "\n",
    "\n",
    "这种迭代方法被称为**值函数迭代**。\n",
    "\n",
    "我们还知道，一个可行策略是最优的，当且仅当它是 $ v^* $-贪婪的。\n",
    "\n",
    "证明存在 $ v^* $-贪婪策略并不太难\n",
    "（如果你遇到困难，可以参考 [EDTC](http://johnstachurski.net/edtc.html) 定理10.1.11）。\n",
    "\n",
    "因此，至少存在一个最优策略。\n",
    "\n",
    "我们现在的问题是如何计算它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504281a7",
   "metadata": {},
   "source": [
    "### 无界效用\n",
    "\n",
    "\n",
    "<a id='index-5'></a>\n",
    "上述结果假设效用函数是有界的。\n",
    "\n",
    "在实践中，经济学家经常使用无界效用函数——我们也将这样做。\n",
    "\n",
    "在无界设定下，存在各种最优性理论。\n",
    "\n",
    "遗憾的是,这些结论往往是针对具体情况的,而不是适用于广泛的应用场景。\n",
    "\n",
    "尽管如此,它们的主要结论通常与上述有界情况的结论一致(只要我们去掉”有界”这个词)。\n",
    "\n",
    "可以参考 [EDTC](http://johnstachurski.net/edtc.html) 第12.2节、[[Kamihigashi, 2012](https://python.quantecon.org/zreferences.html#id181)] 或 [[Martins-da-Rocha and Vailakis, 2010](https://python.quantecon.org/zreferences.html#id192)]。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2344d",
   "metadata": {},
   "source": [
    "## 计算\n",
    "\n",
    "\n",
    "<a id='index-6'></a>\n",
    "现在让我们来看看如何计算值函数和最优策略。\n",
    "\n",
    "本讲中的实现将着重于清晰性和灵活性。\n",
    "\n",
    "这两点都很有帮助,但会牺牲一些运行速度 —— 当你运行代码时就会看到这一点。\n",
    "\n",
    "[后续](https://python.quantecon.org/optgrowth_fast.html) 我们将牺牲一些清晰性和灵活性,通过即时(JIT)编译来加速代码。\n",
    "\n",
    "我们将使用的算法是拟合值函数迭代法,这是\n",
    "\n",
    "在前面的讲座中描述的[McCall模型](https://python.quantecon.org/mccall_fitted_vfi.html)和[蛋糕食用问题](https://python.quantecon.org/cake_eating_numerical.html)。\n",
    "\n",
    "算法将是\n",
    "\n",
    "\n",
    "<a id='fvi-alg'></a>\n",
    "1. 从一组值$ \\{ v_1, \\ldots, v_I \\} $开始，这些值代表初始函数$ v $在网格点$ \\{ y_1, \\ldots, y_I \\} $上的值。  \n",
    "1. 基于这些数据点，通过线性插值在状态空间$ \\mathbb R_+ $上构建函数$ \\hat v $。  \n",
    "1. 通过重复求解[(40.11)](#equation-fcbell20-optgrowth)，获取并记录每个网格点$ y_i $上的值$ T \\hat v(y_i) $。  \n",
    "1. 除非满足某些停止条件，否则设置$ \\{ v_1, \\ldots, v_I \\} = \\{ T \\hat v(y_1), \\ldots, T \\hat v(y_I) \\} $并返回步骤2。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04803ba",
   "metadata": {},
   "source": [
    "### 标量最大化\n",
    "\n",
    "为了最大化贝尔曼方程[(40.9)](#equation-fpb30)的右侧，我们将使用SciPy中的`minimize_scalar`程序。\n",
    "\n",
    "由于我们是在最大化而不是最小化，我们将利用这样一个事实：在区间$ [a, b] $上$ g $的最大值点是\n",
    "\n",
    "在相同区间上的 $ -g $。\n",
    "\n",
    "为此，并保持接口整洁，我们将把 `minimize_scalar` 封装在一个外部函数中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d1530",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    在区间 [a, b] 上最大化函数 g。\n",
    "\n",
    "    我们利用了在任何区间上 g 的最大值点也是 -g 的最小值点这一事实。\n",
    "    元组 args 收集了传递给 g 的任何额外参数。\n",
    "\n",
    "    返回最大值和最大值点。\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f3e26",
   "metadata": {},
   "source": [
    "### 最优增长模型\n",
    "\n",
    "我们暂且假设 $ \\phi $ 是 $ \\xi := \\exp(\\mu + s \\zeta) $ 的分布，其中\n",
    "\n",
    "- $ \\zeta $ 是标准正态分布，  \n",
    "- $ \\mu $ 是冲击位置参数，  \n",
    "- $ s $ 是冲击规模参数。  \n",
    "\n",
    "\n",
    "我们将这些和最优增长模型的其他基本要素存储在一个类中。\n",
    "\n",
    "下面定义的类结合了参数和一个实现贝尔曼方程[(40.9)](#equation-fpb30)右侧的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b42a0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class OptimalGrowthModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 u,            # 效用函数\n",
    "                 f,            # 生产函数\n",
    "                 β=0.96,       # 贴现因子\n",
    "                 μ=0,          # 冲击位置参数\n",
    "                 s=0.1,        # 冲击规模参数\n",
    "                 grid_max=4,\n",
    "                 grid_size=120,\n",
    "                 shock_size=250,\n",
    "                 seed=1234):\n",
    "\n",
    "        self.u, self.f, self.β, self.μ, self.s = u, f, β, μ, s\n",
    "\n",
    "        # 设置网格\n",
    "        self.grid = np.linspace(1e-4, grid_max, grid_size)\n",
    "\n",
    "        # 存储冲击（设定随机种子，使结果可重现）\n",
    "        np.random.seed(seed)\n",
    "        self.shocks = np.exp(μ + s * np.random.randn(shock_size))\n",
    "\n",
    "    def state_action_value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        贝尔曼方程的右侧。\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, β, shocks = self.u, self.f, self.β, self.shocks\n",
    "\n",
    "        v = interp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c) + β * np.mean(v(f(y - c) * shocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257f46d",
   "metadata": {},
   "source": [
    "在倒数第二行中，我们使用线性插值。\n",
    "\n",
    "在最后一行中，[(40.11)](#equation-fcbell20-optgrowth)中的期望值通过[蒙特卡洛](https://en.wikipedia.org/wiki/Monte_Carlo_integration)方法计算，使用以下近似：\n",
    "\n",
    "$$\n",
    "\\int v(f(y - c) z) \\phi(dz) \\approx \\frac{1}{n} \\sum_{i=1}^n v(f(y - c) \\xi_i)\n",
    "$$\n",
    "\n",
    "其中$ \\{\\xi_i\\}_{i=1}^n $是从$ \\phi $中独立同分布抽取的样本。\n",
    "\n",
    "蒙特卡洛并不总是计算积分最有效的数值方法，但在当前情况下确实具有一些理论优势。\n",
    "\n",
    "（例如，它保持了贝尔曼算子的压缩映射性质 — 参见[[Pál and Stachurski, 2013](https://python.quantecon.org/zreferences.html#id131)]。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218b580",
   "metadata": {},
   "source": [
    "### 贝尔曼算子\n",
    "\n",
    "下面的函数实现了贝尔曼算子。\n",
    "\n",
    "（我们本可以将其作为`OptimalGrowthModel`类的方法添加，但对于这种数值计算工作，我们更倾向于使用小型类而不是单体类。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c59026",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def T(v, og):\n",
    "    \"\"\"\n",
    "    贝尔曼算子。更新值函数的猜测值，\n",
    "    并同时计算v-贪婪策略。\n",
    "\n",
    "      * og是OptimalGrowthModel的一个实例\n",
    "      * v是表示值函数猜测的数组\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "    v_greedy = np.empty_like(v)\n",
    "\n",
    "    for i in range(len(grid)):\n",
    "        y = grid[i]\n",
    "\n",
    "        # 在状态y下最大化贝尔曼方程右侧\n",
    "        c_star, v_max = maximize(og.state_action_value, 1e-10, y, (y, v))\n",
    "        v_new[i] = v_max\n",
    "        v_greedy[i] = c_star\n",
    "\n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005e4d3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='benchmark-growth-mod'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d931b",
   "metadata": {},
   "source": [
    "### 一个示例\n",
    "\n",
    "假设现在\n",
    "\n",
    "$$\n",
    "f(k) = k^{\\alpha}\n",
    "\\quad \\text{和} \\quad\n",
    "u(c) = \\ln c\n",
    "$$\n",
    "\n",
    "对于这个特定问题，存在精确的解析解（参见[[Ljungqvist and Sargent, 2018](https://python.quantecon.org/zreferences.html#id186)]第3.1.2节），其中\n",
    "\n",
    "\n",
    "<a id='equation-dpi-tv'></a>\n",
    "$$\n",
    "v^*(y) =\n",
    "\\frac{\\ln (1 - \\alpha \\beta) }{ 1 - \\beta} +\n",
    "\\frac{(\\mu + \\alpha \\ln (\\alpha \\beta))}{1 - \\alpha}\n",
    " \\left[\n",
    "     \\frac{1}{1- \\beta} - \\frac{1}{1 - \\alpha \\beta}\n",
    " \\right] +\n",
    " \\frac{1}{1 - \\alpha \\beta} \\ln y \\tag{40.12}\n",
    "$$\n",
    "\n",
    "和最优消费策略\n",
    "\n",
    "$$\n",
    "\\sigma^*(y) = (1 - \\alpha \\beta ) y\n",
    "$$\n",
    "\n",
    "有这些封闭形式的解是很有价值的，因为它让我们能够检验我们的代码在这个特定情况下是否正确。\n",
    "\n",
    "在Python中，上述函数可以表示为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b22f53",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def v_star(y, α, β, μ):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(y)\n",
    "\n",
    "def σ_star(y, α, β):\n",
    "    \"\"\"\n",
    "    True optimal policy\n",
    "    \"\"\"\n",
    "    return (1 - α * β) * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db9ec6",
   "metadata": {},
   "source": [
    "接下来让我们用上述基本要素创建一个模型实例，并将其赋值给变量`og`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ae003",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α = 0.4\n",
    "def fcd(k):\n",
    "    return k**α\n",
    "\n",
    "og = OptimalGrowthModel(u=np.log, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc3422",
   "metadata": {},
   "source": [
    "现在让我们看看当我们将贝尔曼算子应用于这种情况下的精确解$ v^* $时会发生什么。\n",
    "\n",
    "理论上，由于$ v^* $是一个不动点，得到的函数应该仍然是$ v^* $。\n",
    "\n",
    "在实践中，我们预计会有一些小的数值误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f829958",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "grid = og.grid\n",
    "\n",
    "v_init = v_star(grid, α, og.β, og.μ)    # 从解开始\n",
    "v_greedy, v = T(v_init, og)             # 应用一次T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(-35, -24)\n",
    "ax.plot(grid, v, lw=2, alpha=0.6, label='$Tv^*$')\n",
    "ax.plot(grid, v_init, lw=2, alpha=0.6, label='$v^*$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b570d",
   "metadata": {},
   "source": [
    "这两个函数本质上没有区别，所以我们开始得很顺利。\n",
    "\n",
    "现在让我们看看从任意初始条件开始，如何用贝尔曼算子进行迭代。\n",
    "\n",
    "我们选择的初始条件是，有点随意地设定为 $ v(y) = 5 \\ln (y) $。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13923d5b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v = 5 * np.log(grid)  # 初始条件\n",
    "n = 35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v, color=plt.cm.jet(0),\n",
    "        lw=2, alpha=0.6, label='初始条件')\n",
    "\n",
    "for i in range(n):\n",
    "    v_greedy, v = T(v, og)  # 应用贝尔曼算子\n",
    "    ax.plot(grid, v, color=plt.cm.jet(i / n), lw=2, alpha=0.6)\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, og.β, og.μ), 'k-', lw=2,\n",
    "        alpha=0.8, label='真实值函数')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(ylim=(-40, 10), xlim=(np.min(grid), np.max(grid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be6d35",
   "metadata": {},
   "source": [
    "图中显示了\n",
    "\n",
    "1. 由拟合值迭代算法生成的前36个函数，颜色越热表示迭代次数越高  \n",
    "1. 用黑色线条绘制的真实值函数$ v^* $  \n",
    "\n",
    "\n",
    "迭代序列逐渐收敛于$ v^* $。\n",
    "\n",
    "我们显然正在接近目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d77f5d",
   "metadata": {},
   "source": [
    "### 迭代至收敛\n",
    "\n",
    "我们可以编写一个函数，使其迭代直到差异小于特定的容差水平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc2824",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def solve_model(og,\n",
    "                tol=1e-4,\n",
    "                max_iter=1000,\n",
    "                verbose=True,\n",
    "                print_skip=25):\n",
    "    \"\"\"\n",
    "    Solve model by iterating with the Bellman operator.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up loop\n",
    "    v = og.u(og.grid)  # Initial condition\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_greedy, v_new = T(v, og)\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        v = v_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"Failed to converge!\")\n",
    "    elif verbose:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9de844",
   "metadata": {},
   "source": [
    "让我们使用这个函数在默认设置下计算一个近似解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aacd8c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v_greedy, v_solution = solve_model(og)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd027f2",
   "metadata": {},
   "source": [
    "现在我们通过将结果与真实值进行对比绘图来检验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19366f82",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_solution, lw=2, alpha=0.6,\n",
    "        label='近似值函数')\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, og.β, og.μ), lw=2,\n",
    "        alpha=0.6, label='真实值函数')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(-35, -24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48154b",
   "metadata": {},
   "source": [
    "图表显示我们的结果非常准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31410d47",
   "metadata": {},
   "source": [
    "### 策略函数\n",
    "\n",
    "\n",
    "<a id='index-7'></a>\n",
    "上面计算的策略`v_greedy`对应于一个近似最优策略。\n",
    "\n",
    "下图将其与精确解进行比较，如上所述，精确解为$ \\sigma(y) = (1 - \\alpha \\beta) y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445a4d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='approximate policy function')\n",
    "\n",
    "ax.plot(grid, σ_star(grid, α, og.β), '--',\n",
    "        lw=2, alpha=0.6, label='true policy function')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a8522",
   "metadata": {},
   "source": [
    "图表显示我们在这个例子中很好地近似了真实的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11904039",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebce298",
   "metadata": {},
   "source": [
    "## Exercise 40.1\n",
    "\n",
    "在这类工作中，效用函数的常见选择是CRRA规格\n",
    "\n",
    "$$\n",
    "u(c) = \\frac{c^{1 - \\gamma}} {1 - \\gamma}\n",
    "$$\n",
    "\n",
    "保持其他默认设置（包括柯布-道格拉斯生产函数），用这个效用函数规格求解最优增长模型。\n",
    "\n",
    "设定 $ \\gamma = 1.5 $，计算并绘制最优策略的估计值。\n",
    "\n",
    "记录这个函数运行所需的时间，以便与[下一讲](https://python.quantecon.org/optgrowth_fast.html)中开发的更快代码进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6bc38",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 40.1](https://python.quantecon.org/#og_ex1)\n",
    "\n",
    "这里我们设置模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c0699",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "γ = 1.5   # 偏好参数\n",
    "\n",
    "def u_crra(c):\n",
    "    return (c**(1 - γ) - 1) / (1 - γ)\n",
    "\n",
    "og = OptimalGrowthModel(u=u_crra, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d4a1c",
   "metadata": {},
   "source": [
    "现在让我们运行它，并计时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24f8f8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "v_greedy, v_solution = solve_model(og)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffffbab",
   "metadata": {},
   "source": [
    "让我们绘制策略函数看看它的样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a36128",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='近似最优策略')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b359c",
   "metadata": {},
   "source": [
    "## Exercise 40.2\n",
    "\n",
    "计时从初始条件 $ v(y) = u(y) $ 开始，使用贝尔曼算子迭代20次所需的时间。\n",
    "\n",
    "使用前一个练习中的模型规格。\n",
    "\n",
    "(和之前一样，我们会将这个数字与[下一讲](https://python.quantecon.org/optgrowth_fast.html)中更快的代码进行比较。)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52d0e1",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 40.2](https://python.quantecon.org/#og_ex2)\n",
    "\n",
    "让我们设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb286f38",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "og = OptimalGrowthModel(u=u_crra, f=fcd)\n",
    "v = og.u(og.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fdfb9",
   "metadata": {},
   "source": [
    "这是计时结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4376476",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(20):\n",
    "    v_greedy, v_new = T(v, og)\n",
    "    v = v_new"
   ]
  }
 ],
 "metadata": {
  "date": 1747894131.9067647,
  "filename": "optgrowth.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "最优增长 I：随机最优增长模型"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}