{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ca0ced",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7154079",
   "metadata": {},
   "source": [
    "# 错误模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94844600",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install numpyro jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44a5a2",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "这是[这个 quantecon 讲座](https://python.quantecon.org/likelihood_bayes.html)的续篇。\n",
    "\n",
    "我们将讨论创建复合彩票的两种方式及其后果。\n",
    "\n",
    "复合彩票可以说是创建了一个_混合分布_。\n",
    "\n",
    "我们构建复合彩票的两种方式在**时间安排**上有所不同。\n",
    "\n",
    "- 其中一种方式是在时间开始时一次性在两个可能的概率分布之间进行混合  \n",
    "- 另一种方式是在每个时期都在相同的两个可能的概率分布之间进行混合  \n",
    "\n",
    "\n",
    "这个统计设定与那个 quantecon 讲座中研究的问题相近但不完全相同。\n",
    "\n",
    "在那个讲座中,有两个可能支配非负随机变量 $ W $ 连续抽取的独立同分布过程。\n",
    "\n",
    "自然界一劳永逸地决定是从分布 $ f $ 还是从分布 $ g $ 中进行一系列独立同分布抽取。\n",
    "\n",
    "那个讲座研究了一个同时知道 $ f $ 和 $ g $ 但不知道自然界在时间 $ -1 $ 选择了哪个分布的代理人。\n",
    "\n",
    "该代理人通过假设自然界抛一个不公平的硬币来选择 $ f $ 或 $ g $,其中选择概率分布 $ f $ 的概率为 $ \\pi_{-1} $,来表示这种无知状态。\n",
    "\n",
    "这个假设使得代理人能够构建一个关于随机序列 $ \\{W_t\\}_{t=0}^\\infty $ 的主观联合概率分布。\n",
    "\n",
    "我们研究了代理人如何使用条件概率法则和观察到的历史 $ w^t =\\{w_s\\}_{s=0}^t $ 来形成\n",
    "\n",
    "$$\n",
    "\\pi_t = E [ \\textrm{自然选择分布} f | w^t] , \\quad  t = 0, 1, 2, \\ldots\n",
    "$$\n",
    "\n",
    "然而，在本讲座的设定中，该规则为智能体赋予了一个错误的模型。\n",
    "\n",
    "原因是现在工资序列实际上是由一个不同的统计模型描述的。\n",
    "\n",
    "因此，我们需要以下列方式改变[quantecon讲座](https://python.quantecon.org/likelihood_bayes.html)的规范。\n",
    "\n",
    "现在，在**每个时期**$ t \\geq 0 $，自然投掷一个可能不公平的硬币，以概率$ \\alpha $出现$ f $，以概率$ 1-\\alpha $出现$ g $。\n",
    "\n",
    "因此，自然持续地从具有以下累积分布函数的**混合分布**中抽取：\n",
    "\n",
    "$$\n",
    "H(w) = \\alpha F(w) + (1-\\alpha) G(w), \\quad \\alpha \\in (0,1)\n",
    "$$\n",
    "\n",
    "我们将研究两种试图学习工资过程的智能体，他们使用不同的统计模型。\n",
    "\n",
    "两种类型的智能体都知道$ f $和$ g $，但都不知道$ \\alpha $。\n",
    "\n",
    "我们的第一类智能体错误地认为在时间$ -1 $时，自然一次性选择了$ f $或$ g $，此后永久地从该分布中抽取。\n",
    "\n",
    "我们的第二类智能体正确地知道，自然在每个时期以混合概率$ \\alpha \\in (0,1) $混合$ f $和$ g $，尽管智能体不知道混合参数。\n",
    "\n",
    "我们的第一类智能体应用[这个quantecon讲座](https://python.quantecon.org/likelihood_bayes.html)中描述的学习算法。\n",
    "\n",
    "在该讲座中所涉及的统计模型的背景下，这是一个好的学习算法，它使贝叶斯学习者能够\n",
    "\n",
    "最终学习自然在时间 $ -1 $ 时所选择的分布。\n",
    "\n",
    "这是因为该智能体的统计模型在与数据生成过程一致的意义上是*正确的*。\n",
    "\n",
    "但在当前情况下，我们的第一类决策者的模型是错误的，因为实际生成数据的模型 $ h $ 既不是 $ f $ 也不是 $ g $，因此超出了该智能体认为可能的模型支持范围。\n",
    "\n",
    "尽管如此，我们将看到我们的第一类智能体仍然能够摸索前进，并最终学到一些有趣且有用的东西，即使这些并不是*真实的*。\n",
    "\n",
    "相反，事实证明我们这个配备了错误统计模型的第一类智能体，最终会学习到 $ f $ 或 $ g $ 中在特定意义上与实际生成数据的 $ h $ *最接近*的那个概率分布。\n",
    "\n",
    "我们将说明它在什么意义上是最接近的。\n",
    "\n",
    "我们的第二类智能体理解自然在每个时期以固定的混合概率 $ \\alpha $ 在 $ f $ 和 $ g $ 之间进行混合。\n",
    "\n",
    "但该智能体不知道 $ \\alpha $ 的值。\n",
    "\n",
    "该智能体着手使用贝叶斯法则应用于其模型来学习 $ \\alpha $。\n",
    "\n",
    "他的模型是正确的，因为它包含了实际的数据生成过程 $ h $ 作为一个可能的分布。\n",
    "\n",
    "在本讲中，我们将学习\n",
    "\n",
    "- 自然如何在两个分布 $ f $ 和 $ g $ 之间进行*混合*以创建一个新的分布 $ h $。  \n",
    "- Kullback-Leibler统计散度 [https://en.wikipedia.org/wiki/Kullback–Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) 用于控制在不正确统计模型下的统计学习  \n",
    "- 一个有用的Python函数`numpy.searchsorted`，它与均匀随机数生成器结合使用时，可以用来从任意分布中进行采样  \n",
    "\n",
    "\n",
    "像往常一样，我们先导入一些Python工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1bd78",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "FONTPATH = \"fonts/SourceHanSerifSC-SemiBold.otf\"\n",
    "mpl.font_manager.fontManager.addfont(FONTPATH)\n",
    "plt.rcParams['font.family'] = ['Source Han Serif SC']\n",
    "\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "import pandas as pd\n",
    "import scipy.stats as sp\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette()\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "@jit\n",
    "def set_seed():\n",
    "    np.random.seed(0)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d84183",
   "metadata": {},
   "source": [
    "让我们用Python生成两个贝塔分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7eabf5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 两个贝塔分布的参数\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# 两个密度函数\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035ad4c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def simulate(a, b, T=50, N=500):\n",
    "    '''\n",
    "    生成N组T个似然比观测值，\n",
    "    以N x T矩阵形式返回。\n",
    "\n",
    "    '''\n",
    "\n",
    "    l_arr = np.empty((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f(w) / g(w)\n",
    "\n",
    "    return l_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20784c",
   "metadata": {},
   "source": [
    "我们还将使用以下Python代码来准备一些信息丰富的模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac22b7c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g = simulate(G_a, G_b, N=50000)\n",
    "l_seq_g = np.cumprod(l_arr_g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac649b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_f = simulate(F_a, F_b, N=50000)\n",
    "l_seq_f = np.cumprod(l_arr_f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2948c3",
   "metadata": {},
   "source": [
    "## 从复合彩票 $ H $ 中抽样\n",
    "\n",
    "我们实现两种方法从混合模型 $ \\alpha F + (1-\\alpha) G $ 中抽取样本。\n",
    "\n",
    "我们将使用这两种方法生成样本，并验证它们是否匹配良好。\n",
    "\n",
    "以下是从复合彩票中抽样的”方法1”的伪代码：\n",
    "\n",
    "- 第一步：  \n",
    "  - 使用 numpy.random.choice 函数抛一个不公平的硬币，以概率 $ \\alpha $ 选择分布 $ F $，以概率 $ 1-\\alpha $ 选择 $ G $  \n",
    "- 第二步：  \n",
    "  - 根据硬币抛掷的结果，从 $ F $ 或 $ G $ 中抽样  \n",
    "- 第三步：  \n",
    "  - 将前两步放在一个大循环中，对 $ w $ 的每个实现都执行这些步骤  \n",
    "\n",
    "\n",
    "我们的第二种方法使用均匀分布和以下在 quantecon 讲座 [https://python.quantecon.org/prob_matrix.html](https://python.quantecon.org/prob_matrix.html) 中描述和使用的事实：\n",
    "\n",
    "- 如果随机变量 $ X $ 的累积分布函数为 $ F $，那么随机变量 $ F^{-1}(U) $ 也具有累积分布函数 $ F $，其中 $ U $ 是 $ [0,1] $ 上的均匀随机变量。  \n",
    "\n",
    "\n",
    "换句话说，如果 $ X \\sim F(x) $，我们可以通过从 $ [0,1] $ 上的均匀分布中抽取随机样本并计算 $ F^{-1}(U) $ 来生成来自 $ F $ 的随机样本。\n",
    "\n",
    "我们将结合 `numpy.searchsorted` 命令使用这个事实来直接从 $ H $ 中抽样。\n",
    "\n",
    "关于 `searchsorted` 函数的说明，请参见 [https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html)。\n",
    "\n",
    "观看[Mr. P Solver关于蒙特卡洛模拟的视频](https://www.google.com/search?q=Mr.+P+Solver+video+on+Monte+Carlo+simulation&amp;oq=Mr.+P+Solver+video+on+Monte+Carlo+simulation)，了解这个强大技巧的其他应用。\n",
    "\n",
    "在下面的Python代码中，我们将使用两种方法，并确认它们都能很好地从我们的目标混合分布中进行采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69a631",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def draw_lottery(p, N):\n",
    "    \"直接从复合彩票中抽取。\"\n",
    "\n",
    "    draws = []\n",
    "    for i in range(0, N):\n",
    "        if np.random.rand()<=p:\n",
    "            draws.append(np.random.beta(F_a, F_b))\n",
    "        else:\n",
    "            draws.append(np.random.beta(G_a, G_b))\n",
    "    return np.array(draws)\n",
    "\n",
    "def draw_lottery_MC(p, N):\n",
    "    \"使用蒙特卡洛技巧从复合彩票中抽取。\"\n",
    "\n",
    "    xs = np.linspace(1e-8,1-(1e-8),10000)\n",
    "    CDF = p*sp.beta.cdf(xs, F_a, F_b) + (1-p)*sp.beta.cdf(xs, G_a, G_b)\n",
    "\n",
    "    Us = np.random.rand(N)\n",
    "    draws = xs[np.searchsorted(CDF[:-1], Us)]\n",
    "    return draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c395e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "N = 100000\n",
    "α = 0.0\n",
    "\n",
    "sample1 = draw_lottery(α, N)\n",
    "sample2 = draw_lottery_MC(α, N)\n",
    "\n",
    "# 绘制抽样和密度函数\n",
    "plt.hist(sample1, 50, density=True, alpha=0.5, label='直接抽样')\n",
    "plt.hist(sample2, 50, density=True, alpha=0.5, label='MC抽样')\n",
    "\n",
    "xs = np.linspace(0,1,1000)\n",
    "plt.plot(xs, α*f(xs)+(1-α)*g(xs), color='red', label='密度')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050255e",
   "metadata": {},
   "source": [
    "## 类型1智能体\n",
    "\n",
    "现在我们来研究类型1智能体学到了什么\n",
    "\n",
    "请记住，我们的类型1智能体使用了错误的统计模型，认为自然在时间-1时就一次性地在$ f $和$ g $之间做出了选择。\n",
    "\n",
    "因此，类型1智能体使用了在[这个quantecon讲座](https://python.quantecon.org/likelihood_bayes.html)中研究的学习算法。\n",
    "\n",
    "我们现在简要回顾一下该学习算法。\n",
    "\n",
    "让$ \\pi_t $表示贝叶斯后验概率，定义为\n",
    "\n",
    "$$\n",
    "\\pi_t = {\\rm Prob}(q=f|w^t)\n",
    "$$\n",
    "\n",
    "似然比过程在控制后验概率$ \\pi_t $演化的公式中起着主要作用，这是**贝叶斯法则**的一个实例。\n",
    "\n",
    "贝叶斯法则意味着$ \\{\\pi_t\\} $遵循以下递归\n",
    "\n",
    "\n",
    "<a id='equation-eq-recur1'></a>\n",
    "$$\n",
    "\\pi_t=\\frac{\\pi_{t-1} l_t(w_t)}{\\pi_{t-1} l_t(w_t)+1-\\pi_{t-1}} \\tag{30.1}\n",
    "$$\n",
    "\n",
    "其中$ \\pi_{0} $是关于$ q = f $的贝叶斯先验概率，即在我们尚未看到任何数据时基于主观判断的个人信念。\n",
    "\n",
    "下面我们定义一个Python函数，该函数根据递归式[(30.1)](#equation-eq-recur1)使用似然比$ \\ell $来更新信念$ \\pi $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad944bd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(π, l):\n",
    "    \"Update π using likelihood l\"\n",
    "\n",
    "    # Update belief\n",
    "    π = π * l / (π * l + 1 - π)\n",
    "\n",
    "    return π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2c367",
   "metadata": {},
   "source": [
    "通过对公式 [(30.1)](#equation-eq-recur1) 进行迭代，我们可以推导出时间 $ t $ 的后验概率 $ \\pi_{t+1} $ 与时间 $ 0 $ 的先验概率 $ \\pi_0 $ 和似然比过程 $ L(w^{t+1}) $ 之间的关系。\n",
    "\n",
    "首先，注意更新规则\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}\n",
    "=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)}\n",
    "{\\pi_{t}\\ell \\left(w_{t+1}\\right)+\\left(1-\\pi_{t}\\right)}\n",
    "$$\n",
    "\n",
    "可推导出\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{\\pi_{t+1}}\n",
    "    &=\\frac{\\pi_{t}\\ell \\left(w_{t+1}\\right)\n",
    "        +\\left(1-\\pi_{t}\\right)}{\\pi_{t}\\ell \\left(w_{t+1}\\right)} \\\\\n",
    "    &=1-\\frac{1}{\\ell \\left(w_{t+1}\\right)}\n",
    "        +\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\frac{1}{\\pi_{t}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\frac{1}{\\pi_{t+1}}-1\n",
    "=\\frac{1}{\\ell \\left(w_{t+1}\\right)}\\left(\\frac{1}{\\pi_{t}}-1\\right).\n",
    "$$\n",
    "\n",
    "因此\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{1}{\\pi_{t+1}}-1\n",
    "    =\\frac{1}{\\prod_{i=1}^{t+1}\\ell \\left(w_{i}\\right)}\n",
    "        \\left(\\frac{1}{\\pi_{0}}-1\\right)\n",
    "    =\\frac{1}{L\\left(w^{t+1}\\right)}\\left(\\frac{1}{\\pi_{0}}-1\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于 $ \\pi_{0}\\in\\left(0,1\\right) $ 且 $ L\\left(w^{t+1}\\right)>0 $，我们可以验证 $ \\pi_{t+1}\\in\\left(0,1\\right) $。\n",
    "\n",
    "通过重新整理上述方程，我们可以将 $ \\pi_{t+1} $ 表示为 $ t+1 $ 时刻的似然比过程 $ L\\left(w^{t+1}\\right) $ 和初始先验概率 $ \\pi_{0} $ 的函数\n",
    "\n",
    "\n",
    "<a id='equation-eq-bayeslaw103'></a>\n",
    "$$\n",
    "\\pi_{t+1}=\\frac{\\pi_{0}L\\left(w^{t+1}\\right)}{\\pi_{0}L\\left(w^{t+1}\\right)+1-\\pi_{0}}. \\tag{30.2}\n",
    "$$\n",
    "\n",
    "公式[(30.2)](#equation-eq-bayeslaw103)是公式[(30.1)](#equation-eq-recur1)的推广。\n",
    "\n",
    "公式[(30.2)](#equation-eq-bayeslaw103)可以被视为在观察到一批数据$ \\left\\{ w_{i}\\right\\} _{i=1}^{t+1} $后对先验概率$ \\pi_0 $的一步修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520d930",
   "metadata": {},
   "source": [
    "## 当混合分布$ H $生成数据时，类型1代理学到什么\n",
    "\n",
    "我们现在研究当混合分布$ h;\\alpha $在每个时期真实生成数据时会发生什么。\n",
    "\n",
    "尽管代理的模型有误设定，序列$ \\pi_t $仍然会收敛，且极限要么是$ 0 $要么是$ 1 $。\n",
    "\n",
    "即使在实际上自然总是在$ f $和$ g $之间混合，这一点依然成立。\n",
    "\n",
    "在验证了关于$ \\pi_t $序列可能极限点的这一说法之后，我们将深入研究决定$ \\pi_t $极限值的根本力量。\n",
    "\n",
    "让我们设定一个$ \\alpha $值，然后观察$ \\pi_t $如何演变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edef98a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_mixed(α, T=50, N=500):\n",
    "    \"\"\"\n",
    "    当真实密度为混合h;α时，生成N组T个似然比观测值，\n",
    "    返回N x T矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    w_s = draw_lottery(α, N*T).reshape(N, T)\n",
    "    l_arr = f(w_s) / g(w_s)\n",
    "\n",
    "    return l_arr\n",
    "\n",
    "def plot_π_seq(α, π1=0.2, π2=0.8, T=200):\n",
    "    \"\"\"\n",
    "    当混合分布支配数据时，计算并绘制π_seq和对数似然比过程\n",
    "    \"\"\"\n",
    "\n",
    "    l_arr_mixed = simulate_mixed(α, T=T, N=50)\n",
    "    l_seq_mixed = np.cumprod(l_arr_mixed, axis=1)\n",
    "\n",
    "    T = l_arr_mixed.shape[1]\n",
    "    π_seq_mixed = np.empty((2, T+1))\n",
    "    π_seq_mixed[:, 0] = π1, π2\n",
    "\n",
    "    for t in range(T):\n",
    "        for i in range(2):\n",
    "            π_seq_mixed[i, t+1] = update(π_seq_mixed[i, t], l_arr_mixed[0, t])\n",
    "\n",
    "    # 绘图\n",
    "    fig, ax1 = plt.subplots()\n",
    "    for i in range(2):\n",
    "        ax1.plot(range(T+1), π_seq_mixed[i, :], label=rf\"$\\pi_0$={π_seq_mixed[i, 0]}\")\n",
    "\n",
    "    ax1.plot(np.nan, np.nan,  '--', color='b', label='对数似然比过程')\n",
    "    ax1.set_ylabel(r\"$\\pi_t$\")\n",
    "    ax1.set_xlabel(\"t\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"当$\\\\alpha F + (1-\\\\alpha)G$支配数据时\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(1, T+1), np.log(l_seq_mixed[0, :]), '--', color='b')\n",
    "    ax2.set_ylabel(\"$log(L(w^{t}))$\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec571760",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_π_seq(α = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bbc03",
   "metadata": {},
   "source": [
    "上图显示了对数似然比过程的一个样本路径（蓝色虚线），以及从两个不同初始条件开始的 $ \\pi_t $ 的样本路径。\n",
    "\n",
    "让我们看看当我们改变 $ \\alpha $ 时会发生什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae2bcc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_π_seq(α = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e93f71",
   "metadata": {},
   "source": [
    "显然，$ \\alpha $ 对 $ \\pi_t $ 在 $ t \\rightarrow + \\infty $ 时的终点有很大影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc4f69",
   "metadata": {},
   "source": [
    "## Kullback-Leibler 散度决定 $ \\pi_t $ 的极限\n",
    "\n",
    "为了理解是什么决定了 $ \\pi_t $ 的极限点是 $ 0 $ 还是 $ 1 $，以及答案如何依赖于生成以下混合分布的真实混合概率 $ \\alpha \\in (0,1) $：\n",
    "\n",
    "$$\n",
    "h(w) \\equiv h(w | \\alpha) = \\alpha f(w) + (1-\\alpha) g(w)\n",
    "$$\n",
    "\n",
    "我们将计算以下两个 Kullback-Leibler 散度：\n",
    "\n",
    "$$\n",
    "KL_g (\\alpha) = \\int \\log\\left(\\frac{h(w)}{g(w)}\\right) h(w) d w\n",
    "$$\n",
    "\n",
    "和\n",
    "\n",
    "$$\n",
    "KL_f (\\alpha) = \\int \\log\\left(\\frac{h(w)}{f(w)}\\right) h(w) d w\n",
    "$$\n",
    "\n",
    "我们将在使用 $ \\alpha $ 改变 $ h(w) = h(w|\\alpha) $ 时，绘制这两个函数关于 $ \\alpha $ 的图像。\n",
    "\n",
    "$ \\pi_t $ 的极限由下式决定：\n",
    "\n",
    "$$\n",
    "\\min_{f,g} \\{KL_g, KL_f\\}\n",
    "$$\n",
    "\n",
    "唯一可能的极限是 $ 0 $ 和 $ 1 $。\n",
    "\n",
    "当且仅当 $ KL_f < KL_g $ 时，$ \\pi_t $ 在 $ t \\rightarrow +\\infty $ 时趋向于 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f691a6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@vectorize\n",
    "def KL_g(α):\n",
    "    \"Compute the KL divergence KL(h, g).\"\n",
    "    err = 1e-8                          # to avoid 0 at end points\n",
    "    ws = np.linspace(err, 1-err, 10000)\n",
    "    gs, fs = g(ws), f(ws)\n",
    "    hs = α*fs + (1-α)*gs\n",
    "    return np.sum(np.log(hs/gs)*hs)/10000\n",
    "\n",
    "@vectorize\n",
    "def KL_f(α):\n",
    "    \"Compute the KL divergence KL(h, f).\"\n",
    "    err = 1e-8                          # to avoid 0 at end points\n",
    "    ws = np.linspace(err, 1-err, 10000)\n",
    "    gs, fs = g(ws), f(ws)\n",
    "    hs = α*fs + (1-α)*gs\n",
    "    return np.sum(np.log(hs/fs)*hs)/10000\n",
    "\n",
    "\n",
    "# compute KL using quad in Scipy\n",
    "def KL_g_quad(α):\n",
    "    \"Compute the KL divergence KL(h, g) using scipy.integrate.\"\n",
    "    h = lambda x: α*f(x) + (1-α)*g(x)\n",
    "    return quad(lambda x: h(x) * np.log(h(x)/g(x)), 0, 1)[0]\n",
    "\n",
    "def KL_f_quad(α):\n",
    "    \"Compute the KL divergence KL(h, f) using scipy.integrate.\"\n",
    "    h = lambda x: α*f(x) + (1-α)*g(x)\n",
    "    return quad(lambda x: h(x) * np.log(h(x)/f(x)), 0, 1)[0]\n",
    "\n",
    "# vectorize\n",
    "KL_g_quad_v = np.vectorize(KL_g_quad)\n",
    "KL_f_quad_v = np.vectorize(KL_f_quad)\n",
    "\n",
    "\n",
    "# Let us find the limit point\n",
    "def π_lim(α, T=5000, π_0=0.4):\n",
    "    \"Find limit of π sequence.\"\n",
    "    π_seq = np.zeros(T+1)\n",
    "    π_seq[0] = π_0\n",
    "    l_arr = simulate_mixed(α, T, N=1)[0]\n",
    "\n",
    "    for t in range(T):\n",
    "        π_seq[t+1] = update(π_seq[t], l_arr[t])\n",
    "    return π_seq[-1]\n",
    "\n",
    "π_lim_v = np.vectorize(π_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17a711",
   "metadata": {},
   "source": [
    "让我们首先绘制每个$ \\alpha $对应的KL散度$ KL_g\\left(\\alpha\\right), KL_f\\left(\\alpha\\right) $。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318f1cf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_arr = np.linspace(0, 1, 100)\n",
    "KL_g_arr = KL_g(α_arr)\n",
    "KL_f_arr = KL_f(α_arr)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=[10, 6])\n",
    "\n",
    "ax.plot(α_arr, KL_g_arr, label='KL(h, g)')\n",
    "ax.plot(α_arr, KL_f_arr, label='KL(h, f)')\n",
    "ax.set_ylabel('KL散度')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99ad9a",
   "metadata": {},
   "source": [
    "让我们计算一个 $ \\alpha $ 值，使得 $ h $ 和 $ g $ 之间的 KL 散度等于 $ h $ 和 $ f $ 之间的 KL 散度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e193",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 当 KL_f = KL_g 时\n",
    "discretion = α_arr[np.argmin(np.abs(KL_g_arr-KL_f_arr))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab68cff",
   "metadata": {},
   "source": [
    "我们可以计算并绘制每个$ \\alpha $的收敛点$ \\pi_{\\infty} $，以验证收敛确实是由KL散度决定的。\n",
    "\n",
    "蓝色圆圈显示了模拟发现的不同$ \\alpha $值（记录在$ x $轴上）对应的$ \\pi_t $的极限值。\n",
    "\n",
    "因此，下图证实了KL散度的最小值如何决定了我们的类型1代理最终学到的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01e9eb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_arr_x = α_arr[(α_arr<discretion)|(α_arr>discretion)]\n",
    "π_lim_arr = π_lim_v(α_arr_x)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, figsize=[10, 6])\n",
    "\n",
    "ax.plot(α_arr, KL_g_arr, label='KL(h, g)')\n",
    "ax.plot(α_arr, KL_f_arr, label='KL(h, f)')\n",
    "ax.set_ylabel('KL散度')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "\n",
    "# plot KL\n",
    "ax2 = ax.twinx()\n",
    "# plot limit point\n",
    "ax2.scatter(α_arr_x, π_lim_arr, \n",
    "            facecolors='none', \n",
    "            edgecolors='tab:blue', \n",
    "            label=r'$\\pi$ lim')\n",
    "ax2.set_ylabel('π lim')\n",
    "\n",
    "ax.legend(loc=[0.85, 0.8])\n",
    "ax2.legend(loc=[0.85, 0.73])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11397424",
   "metadata": {},
   "source": [
    "显然，我们的类型1学习者将贝叶斯定律应用于其错误设定的统计模型集合，最终学习到一个尽可能接近真实模型的近似模型，这种接近程度是通过Kullback-Leibler散度来衡量的：\n",
    "\n",
    "- 当$ \\alpha $较小时，$ KL_g < KL_f $意味着$ g $与$ h $的散度小于$ f $的散度，因此$ \\pi_t $的极限点接近0。  \n",
    "- 当$ \\alpha $较大时，$ KL_f < KL_g $意味着$ f $与$ h $的散度小于$ g $的散度，因此$ \\pi_t $的极限点接近1。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9afdd7",
   "metadata": {},
   "source": [
    "## 类型2代理\n",
    "\n",
    "现在我们来描述类型2代理如何构建他的学习问题以及他最终学到什么。\n",
    "\n",
    "我们的类型2代理理解正确的统计模型但不知道$ \\alpha $。\n",
    "\n",
    "我们应用贝叶斯定律来推导学习$ \\alpha $的算法，假设代理知道\n",
    "\n",
    "$$\n",
    "h(w) = h(w| \\alpha)\n",
    "$$\n",
    "\n",
    "但不知道$ \\alpha $。\n",
    "\n",
    "我们假设此人从$ \\alpha \\in (0,1) $上的先验概率$ \\pi_0(\\alpha) $开始，这个先验具有我们在[这个quantecon讲座](https://python.quantecon.org/bayes_nonconj.html)中使用的形式之一。\n",
    "\n",
    "我们将启动`numpyro`并将其应用于当前情况。\n",
    "\n",
    "贝叶斯定律现在采取以下形式：\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}(\\alpha) = \\frac {h(w_{t+1} | \\alpha) \\pi_t(\\alpha)}\n",
    "       { \\int h(w_{t+1} | \\hat \\alpha) \\pi_t(\\hat \\alpha) d \\hat \\alpha }\n",
    "$$\n",
    "\n",
    "我们将使用numpyro来近似这个方程。\n",
    "\n",
    "我们将创建后验$ \\pi_t(\\alpha) $的图形，随着\n",
    "\n",
    "当 $ t \\rightarrow +\\infty $ 时,对应于 quantecon 讲座中展示的内容 [https://python.quantecon.org/bayes_nonconj.html](https://python.quantecon.org/bayes_nonconj.html)。\n",
    "\n",
    "我们预计后验分布将在 $ t \\rightarrow + \\infty $ 时收敛于真实的 $ \\alpha $ 值周围。\n",
    "\n",
    "让我们先尝试一个均匀先验分布。\n",
    "\n",
    "我们使用 numpyro 中的 `Mixture` 类来构建似然函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752dd02",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α = 0.8\n",
    "\n",
    "# 用真实的 α 模拟数据\n",
    "data = draw_lottery(α, 1000)\n",
    "sizes = [5, 20, 50, 200, 1000, 25000]\n",
    "\n",
    "def model(w):\n",
    "    α = numpyro.sample('α', dist.Uniform(low=0.0, high=1.0))\n",
    "\n",
    "    y_samp = numpyro.sample('w',\n",
    "        dist.Mixture(dist.Categorical(jnp.array([α, 1-α])), [dist.Beta(F_a, F_b), dist.Beta(G_a, G_b)]), obs=w)\n",
    "\n",
    "def MCMC_run(ws):\n",
    "    \"使用 MCMC 计算观测到的 ws 的后验分布\"\n",
    "\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(kernel, num_samples=5000, num_warmup=1000, progress_bar=False)\n",
    "\n",
    "    mcmc.run(rng_key=random.PRNGKey(0), w=jnp.array(ws))\n",
    "    sample = mcmc.get_samples()\n",
    "    return sample['α']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506170ec",
   "metadata": {},
   "source": [
    "以下代码生成了下面的图表，显示了不同历史长度下$ \\alpha $的贝叶斯后验分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056dc55",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    sample = MCMC_run(data[:sizes[i]])\n",
    "    sns.histplot(\n",
    "        data=sample, kde=True, stat='density', alpha=0.2, ax=ax,\n",
    "        color=colors[i], binwidth=0.02, linewidth=0.05, label=f't={sizes[i]}'\n",
    "    )\n",
    "ax.set_title(r'$\\pi_t(\\alpha)$ as $t$ increases')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151c6ea",
   "metadata": {},
   "source": [
    "显然，随着观测历史长度的增长，贝叶斯后验分布逐渐收敛于混合参数的真实值 $ \\alpha = .8 $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316799aa",
   "metadata": {},
   "source": [
    "## 总结性评论\n",
    "\n",
    "我们的第1类人使用了一个错误的统计模型。\n",
    "\n",
    "他认为要么是 $ f $ 要么是 $ g $ 生成了 $ w $ 过程，只是不知道具体是哪一个。\n",
    "\n",
    "这是错误的，因为实际上自然在每个时期都以混合概率 $ \\alpha $ 进行混合。\n",
    "\n",
    "我们的第1类主体最终会相信要么是 $ f $ 要么是 $ g $ 生成了 $ w $ 序列，结果取决于相对于 $ h $ 具有较小 KL 散度的模型（$ f $ 或 $ g $）。\n",
    "\n",
    "我们的第2类主体有一个不同的统计模型，这个模型是正确设定的。\n",
    "\n",
    "他知道统计模型的参数形式，但不知道混合参数 $ \\alpha $。\n",
    "\n",
    "他知道自己不知道这个参数。\n",
    "\n",
    "但是通过结合其统计模型和数据历史使用贝叶斯法则，他最终能够对 $ \\alpha $ 做出越来越准确的推断。\n",
    "\n",
    "这个小实验展示了一些重要的一般原则，这些原则支配着贝叶斯学习错误设定模型的结果。\n",
    "\n",
    "因此，在实证研究中普遍存在以下情况。\n",
    "\n",
    "一个科学家用统计模型流形 $ S $ 来处理数据，其中 $ s (X | \\theta) $ 是随机向量 $ X $ 上的概率分布，$ \\theta \\in \\Theta $ 是参数向量，而 $ \\Theta $ 标记了模型流形。\n",
    "\n",
    "科学家通过观察得到随机向量$ X $的实现值$ x $，想要解决一个**逆问题**，即通过某种方式_求逆_\n",
    "$ s(x | \\theta) $来从$ x $推断$ \\theta $。\n",
    "\n",
    "但科学家的模型是错误设定的，只是自然界用来生成$ X $的未知模型$ h $的一个近似。\n",
    "\n",
    "如果科学家使用贝叶斯定律或相关的基于似然的方法来推断$ \\theta $，通常在大样本情况下，逆问题会推断出一个使科学家模型$ s $相对于自然模型$ h $的KL散度最小化的$ \\theta $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cf501",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46220904",
   "metadata": {},
   "source": [
    "## Exercise 30.1\n",
    "\n",
    "在[似然比过程和贝叶斯学习](https://python.quantecon.org/likelihood_bayes.html)中，我们研究了将似然比和贝叶斯定律应用于错误设定统计模型的后果。\n",
    "\n",
    "在那节课中，我们使用模型选择算法来研究真实数据生成过程是混合分布的情况。\n",
    "\n",
    "在本节课中，我们研究了如何使用贝叶斯方法正确地”学习”由混合过程生成的模型。\n",
    "\n",
    "为了修正我们在[似然比过程和贝叶斯学习](https://python.quantecon.org/likelihood_bayes.html)中使用的算法，正确的贝叶斯方法应该直接对$ x $的不确定性建模，并随着新数据的到来更新对它的信念。\n",
    "\n",
    "这是算法：\n",
    "\n",
    "首先我们指定$ x $的先验分布为$ x \\sim \\text{Beta}(\\alpha_0, \\beta_0) $，其期望值为$ \\mathbb{E}[x] = \\frac{\\alpha_0}{\\alpha_0 + \\beta_0} $。\n",
    "\n",
    "单个观测值 $ w_t $ 的似然函数为 $ p(w_t|x) = x f(w_t) + (1-x) g(w_t) $。\n",
    "\n",
    "对于序列 $ w^t = (w_1, \\dots, w_t) $，似然函数为 $ p(w^t|x) = \\prod_{i=1}^t p(w_i|x) $。\n",
    "\n",
    "后验分布使用 $ p(x|w^t) \\propto p(w^t|x) p(x) $ 进行更新。\n",
    "\n",
    "递归地，$ w_t $ 之后的后验分布为 $ p(x|w^t) \\propto p(w_t|x) p(x|w^{t-1}) $。\n",
    "\n",
    "如果没有共轭先验，我们可以通过将 $ x $ 离散化为网格来近似后验分布。\n",
    "\n",
    "你的任务是用Python实现这个算法。\n",
    "\n",
    "你可以通过检查后验均值是否随着 $ t $ 的增加而收敛到 $ x $ 的真实值来验证你的实现，详见 [似然比过程和贝叶斯学习](https://python.quantecon.org/likelihood_bayes.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ebf6a",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 30.1](https://python.quantecon.org/#mix_model_ex1)\n",
    "\n",
    "这是一个解决方案：\n",
    "\n",
    "首先我们定义混合概率\n",
    "和先验分布的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2347bee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_true = 0.5\n",
    "T_mix = 200\n",
    "\n",
    "# 三个不同的先验，均值分别为0.25、0.5、0.75\n",
    "prior_params = [(1, 3), (1, 1), (3, 1)]\n",
    "prior_means = [a/(a+b) for a, b in prior_params]\n",
    "\n",
    "w_mix = draw_lottery(x_true, T_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caa030",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def learn_x_bayesian(observations, α0, β0, grid_size=2000):\n",
    "    \"\"\"\n",
    "    使用网格近似对混合概率x进行顺序贝叶斯学习。\n",
    "    \"\"\"\n",
    "    w = np.asarray(observations)\n",
    "    T = w.size\n",
    "\n",
    "    x_grid = np.linspace(1e-3, 1 - 1e-3, grid_size)\n",
    "\n",
    "    # 对数先验\n",
    "    log_prior = (α0 - 1) * np.log(x_grid) + (β0 - 1) * np.log1p(-x_grid)\n",
    "\n",
    "    μ_path = np.empty(T + 1)\n",
    "    μ_path[0] = α0 / (α0 + β0)\n",
    "\n",
    "    log_post = log_prior.copy()\n",
    "\n",
    "    for t in range(T):\n",
    "        wt = w[t]\n",
    "        # P(w_t | x) = x f(w_t) + (1 - x) g(w_t)\n",
    "        like = x_grid * f(wt) + (1 - x_grid) * g(wt)\n",
    "        log_post += np.log(like)\n",
    "\n",
    "        # 归一化\n",
    "        log_post -= log_post.max()\n",
    "        post = np.exp(log_post)\n",
    "        post /= post.sum()\n",
    "\n",
    "        μ_path[t + 1] = x_grid @ post\n",
    "\n",
    "    return μ_path\n",
    "\n",
    "x_posterior_means = [learn_x_bayesian(w_mix, α0, β0) for α0, β0 in prior_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c85cb",
   "metadata": {},
   "source": [
    "让我们可视化 $ x $ 的后验均值如何随时间演变，从三个不同的先验信念开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570f424",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (x_means, mean0) in enumerate(zip(x_posterior_means, prior_means)):\n",
    "    ax.plot(range(T_mix + 1), x_means, \n",
    "            label=fr'Prior mean = ${mean0:.2f}$', \n",
    "            color=colors[i], linewidth=2)\n",
    "\n",
    "ax.axhline(y=x_true, color='black', linestyle='--', \n",
    "           label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff2dc8",
   "metadata": {},
   "source": [
    "图表显示，无论初始先验信念如何，所有三个后验均值最终都会收敛到 $ x=0.5 $ 的真实值。\n",
    "\n",
    "接下来，让我们看看更长时间范围内的多次模拟，所有模拟都从均匀先验开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9733a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "n_paths = 20\n",
    "T_long = 10_000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for j in range(n_paths):\n",
    "    w_path = draw_lottery(x_true, T_long) \n",
    "    x_means = learn_x_bayesian(w_path, 1, 1)  # 均匀先验\n",
    "    ax.plot(range(T_long + 1), x_means, alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.axhline(y=x_true, color='red', linestyle='--', \n",
    "            label=f'True x = {x_true}', linewidth=2)\n",
    "ax.set_ylabel('Posterior mean of $x$')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1486a90",
   "metadata": {},
   "source": [
    "我们可以看到 $ x $ 的后验均值收敛到真实值 $ x=0.5 $。"
   ]
  }
 ],
 "metadata": {
  "date": 1761257073.5078523,
  "filename": "mix_model.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "错误模型"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}