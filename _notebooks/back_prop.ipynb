{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b15771",
   "metadata": {},
   "source": [
    "# 人工神经网络简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0ae04",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade jax jaxlib\n",
    "!conda install -y -c plotly plotly plotly-orca retrying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3ccef",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">如果您在Google Colab上运行此代码，上述单元格将出现错误。这是因为Google Colab不使用Anaconda来管理Python包。但是本讲座仍然可以执行，因为Google Colab已安装了`plotly`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfd3ee",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "**机器学习**和**人工智能**的主要部分包括\n",
    "\n",
    "- 用已知函数来近似未知函数  \n",
    "- 从左右变量的数据集中估计已知函数  \n",
    "\n",
    "\n",
    "本讲座描述了一种广泛用于近似函数$ f $的普通**人工神经网络**(ANN)的结构，该函数将空间$ X $中的$ x $映射到空间$ Y $中的$ y $。\n",
    "\n",
    "为了介绍基本概念，我们研究一个$ x $和$ y $都是标量的例子。\n",
    "\n",
    "我们将描述以下神经网络的基本构建概念：\n",
    "\n",
    "- 神经元  \n",
    "- 激活函数  \n",
    "- 神经元网络  \n",
    "- 神经网络作为函数的组合  \n",
    "- 反向传播及其与微积分链式法则的关系  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73226c3",
   "metadata": {},
   "source": [
    "## 一个深度(但不宽)的人工神经网络\n",
    "\n",
    "我们描述一个”深度”神经网络,其”宽度”为一。\n",
    "\n",
    "**深度**意味着网络由组织成图形节点的大量函数组成。\n",
    "\n",
    "**宽度**指的是被近似函数右侧的变量数量。\n",
    "\n",
    "将”宽度”设为一意味着网络仅组合单变量函数。\n",
    "\n",
    "设$ x \\in \\mathbb{R} $为一个标量,$ y \\in \\mathbb{R} $为另一个标量。\n",
    "\n",
    "我们假设$ y $是$ x $的一个非线性函数:\n",
    "\n",
    "$$\n",
    "y = f(x)\n",
    "$$\n",
    "\n",
    "我们想用另一个递归定义的函数来近似$ f(x) $。\n",
    "\n",
    "对于深度为$ N \\geq 1 $的网络,每个**层**$ i =1, \\ldots N $包含\n",
    "\n",
    "- 一个输入$ x_i $  \n",
    "- 一个**仿射函数** $ w_i x_i + bI $，其中 $ w_i $ 是施加在输入 $ x_i $ 上的标量**权重**，$ b_i $ 是标量**偏置**  \n",
    "- 一个**激活函数** $ h_i $，它以 $ (w_i x_i + b_i) $ 为参数并产生输出 $ x_{i+1} $  \n",
    "\n",
    "\n",
    "激活函数 $ h $ 的一个例子是**sigmoid**函数\n",
    "\n",
    "$$\n",
    "h (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "另一个常用的激活函数是**修正线性单元**（ReLU）函数\n",
    "\n",
    "$$\n",
    "h(z) = \\max (0, z)\n",
    "$$\n",
    "\n",
    "还有一个激活函数是恒等函数\n",
    "\n",
    "$$\n",
    "h(z) = z\n",
    "$$\n",
    "\n",
    "作为下面的激活函数，我们将对第1层到第N-1层使用sigmoid函数，对第N层使用恒等函数。\n",
    "\n",
    "为了近似函数 $ f(x) $，我们按如下方式构造 $ \\hat f(x) $。\n",
    "\n",
    "令\n",
    "\n",
    "$$\n",
    "l_{i}\\left(x\\right)=w_{i}x+b_{i} .\n",
    "$$\n",
    "\n",
    "我们通过迭代函数组合 $ h_i \\circ l_i $ 来构造 $ \\hat f $：\n",
    "\n",
    "$$\n",
    "f(x)\\approx\\hat{f}(x)=h_{N}\\circ l_{N}\\circ h_{N-1}\\circ l_{1}\\circ\\cdots\\circ h_{1}\\circ l_{1}(x)\n",
    "$$\n",
    "\n",
    "如果 $ N >1 $，我们称右边为”深度”神经网络。\n",
    "\n",
    "$ N $ 这个整数越大，神经网络就越”深”。\n",
    "\n",
    "显然，如果我们知道参数 $ \\{w_i, b_i\\}_{i=1}^N $，那么对于给定的 $ x = \\tilde x $，我们可以通过迭代以下递归来计算 $ \\hat f(x) $：\n",
    "\n",
    "\n",
    "<a id='equation-eq-recursion'></a>\n",
    "$$\n",
    "x_{i+1} = h_i \\circ l_i(x_i) , \\quad, i = 1, \\ldots N \\tag{14.1}\n",
    "$$\n",
    "\n",
    "从 $ x_1 = \\tilde x $ 开始。\n",
    "\n",
    "从这个迭代方案得出的 $ x_{N+1} $ 的值等于 $ \\hat f(\\tilde x) $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf76454",
   "metadata": {},
   "source": [
    "## 参数校准\n",
    "\n",
    "现在我们考虑一个如上所述的神经网络，其宽度为1，深度为 $ N $，激活函数 $ h_{i} $ （对于 $ 1\\leqslant i\\leqslant N $）将 $ \\mathbb{R} $ 映射到自身。\n",
    "\n",
    "设 $ \\left\\{ \\left(w_{i},b_{i}\\right)\\right\\} _{i=1}^{N} $ 表示权重和偏置的序列。\n",
    "\n",
    "如上所述，对于给定的输入 $ x_{1} $，我们的近似函数 $ \\hat f $ 求值\n",
    "在 $ x_1 $ 等于我们网络的”输出” $ x_{N+1} $ 时，可以通过迭代 $ x_{i+1}=h_{i}\\left(w_{i}x_{i}+b_{i}\\right) $ 来计算。\n",
    "\n",
    "对于给定的**预测值** $ \\hat{y} (x) $ 和**目标值** $ y= f(x) $，考虑损失函数\n",
    "\n",
    "$$\n",
    "\\mathcal{L} \\left(\\hat{y},y\\right)(x)=\\frac{1}{2}\\left(\\hat{y}-y\\right)^{2}(x) .\n",
    "$$\n",
    "\n",
    "这个准则是参数 $ \\left\\{ \\left(w_{i},b_{i}\\right)\\right\\} _{i=1}^{N} $ 和点 $ x $ 的函数。\n",
    "\n",
    "我们感兴趣的是解决以下问题：\n",
    "\n",
    "$$\n",
    "\\min_{\\left\\{ \\left(w_{i},b_{i}\\right)\\right\\} _{i=1}^{N}} \\int {\\mathcal L}\\left(x_{N+1},y\\right)(x) d \\mu(x)\n",
    "$$\n",
    "\n",
    "其中 $ \\mu(x) $ 是某个测度，用于衡量我们希望得到 $ f(x) $ 的良好近似 $ \\hat f(x) $ 的点 $ x \\in \\mathbb{R} $。\n",
    "\n",
    "将权重和偏置堆叠成参数向量 $ p $：\n",
    "\n",
    "$$\n",
    "p = \\begin{bmatrix}     \n",
    "  w_1 \\cr \n",
    "  b_1 \\cr\n",
    "  w_2 \\cr\n",
    "  b_2 \\cr\n",
    "  \\vdots \\cr\n",
    "  w_N \\cr\n",
    "  b_N \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "对**随机梯度下降**算法的一个”简化版本”应用于寻找函数的零点，得到以下参数更新规则：\n",
    "\n",
    "\n",
    "<a id='equation-eq-sgd'></a>\n",
    "$$\n",
    "p_{k+1}=p_k-\\alpha\\frac{d \\mathcal{L}}{dx_{N+1}}\\frac{dx_{N+1}}{dp_k} \\tag{14.2}\n",
    "$$\n",
    "\n",
    "其中 $ \\frac{d {\\mathcal L}}{dx_{N+1}}=-\\left(x_{N+1}-y\\right) $ 且 $ \\alpha > 0 $ 是步长。\n",
    "\n",
    "(参见[这里](https://en.wikipedia.org/wiki/Gradient_descent#Description)和[这里](https://en.wikipedia.org/wiki/Newton%27s_method)以了解随机梯度下降与牛顿法的关系。)\n",
    "\n",
    "要实现这个参数更新规则的一个步骤，我们需要导数向量 $ \\frac{dx_{N+1}}{dp_k} $。\n",
    "\n",
    "在神经网络文献中，这一步是通过所谓的**反向传播**来完成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818edd8",
   "metadata": {},
   "source": [
    "## 反向传播和链式法则\n",
    "\n",
    "得益于\n",
    "\n",
    "- 微分演算中的链式法则和乘积法则的性质，以及  \n",
    "- 下三角矩阵的性质\n",
    "  反向传播实际上可以通过以下两步完成：  \n",
    "- 求下三角矩阵的逆矩阵，以及  \n",
    "- 矩阵乘法  \n",
    "\n",
    "\n",
    "(这个想法来自 MIT 的 Alan Edelman 在这个精彩的 YouTube 视频的最后 7 分钟)\n",
    "\n",
    "下面开始。\n",
    "\n",
    "将 $ h(z) $ 对 $ z $ 在 $ z = z_i $ 处的导数定义为 $ \\delta_i $：\n",
    "\n",
    "$$\n",
    "\\delta_i = \\frac{d}{d z} h(z)|_{z=z_i}\n",
    "$$\n",
    "\n",
    "或\n",
    "\n",
    "$$\n",
    "\\delta_{i}=h'\\left(w_{i}x_{i}+b_{i}\\right).\n",
    "$$\n",
    "\n",
    "对我们的递归式 [(14.1)](#equation-eq-recursion) 重复应用链式法则和乘积法则，可以得到：\n",
    "\n",
    "$$\n",
    "dx_{i+1}=\\delta_{i}\\left(dw_{i}x_{i}+w_{i}dx_{i}+b_{i}\\right)\n",
    "$$\n",
    "\n",
    "在设定 $ dx_{1}=0 $ 后，我们得到以下方程组：\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "dx_{2}\\\\\n",
    "\\vdots\\\\\n",
    "dx_{N+1}\n",
    "\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{ccccc}\n",
    "\\delta_{1}w_{1} & \\delta_{1} & 0 & 0 & 0\\\\\n",
    "0 & 0 & \\ddots & 0 & 0\\\\\n",
    "0 & 0 & 0 & \\delta_{N}w_{N} & \\delta_{N}\n",
    "\\end{array}\\right)}_{D}\\left(\\begin{array}{c}\n",
    "dw_{1}\\\\\n",
    "db_{1}\\\\\n",
    "\\vdots\\\\\n",
    "dw_{N}\\\\\n",
    "db_{N}\n",
    "\\end{array}\\right)+\\underbrace{\\left(\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0\\\\\n",
    "w_{2} & 0 & 0 & 0\\\\\n",
    "0 & \\ddots & 0 & 0\\\\\n",
    "0 & 0 & w_{N} & 0\n",
    "\\end{array}\\right)}_{L}\\left(\\begin{array}{c}\n",
    "dx_{2}\\\\\n",
    "\\vdots\\\\\n",
    "dx_{N+1}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "或者\n",
    "\n",
    "$$\n",
    "d x = D dp + L dx\n",
    "$$\n",
    "\n",
    "这意味着\n",
    "\n",
    "$$\n",
    "dx = (I -L)^{-1} D dp\n",
    "$$\n",
    "\n",
    "进而意味着\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "dx_{N+1}/dw_{1}\\\\\n",
    "dx_{N+1}/db_{1}\\\\\n",
    "\\vdots\\\\\n",
    "dx_{N+1}/dw_{N}\\\\\n",
    "dx_{N+1}/db_{N}\n",
    "\\end{array}\\right)=e_{N}\\left(I-L\\right)^{-1}D.\n",
    "$$\n",
    "\n",
    "然后我们可以通过对一组输入-输出对$ \\left\\{ \\left(x_{1}^{i},y^{i}\\right)\\right\\} _{i=1}^{M} $（我们称之为”训练集”）多次应用我们的$ p $更新来解决上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd8163",
   "metadata": {},
   "source": [
    "## 训练集\n",
    "\n",
    "选择训练集相当于在上述函数逼近问题的最小化问题表述中选择测度$ \\mu $。\n",
    "\n",
    "本着这个精神，我们将使用均匀网格，比如说50或200个点。\n",
    "对于上述最小化问题，有多种可能的解决方案：\n",
    "\n",
    "- 批量梯度下降，使用训练集上的平均梯度  \n",
    "- 随机梯度下降，随机采样点并使用单个梯度  \n",
    "- 介于两者之间的方法（即所谓的”小批量梯度下降”）  \n",
    "\n",
    "\n",
    "上面描述的更新规则 [(14.2)](#equation-eq-sgd) 相当于一个随机梯度下降算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e84a2d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, jacfwd, vmap\n",
    "from jax import random\n",
    "import jax\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c0187",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 一个辅助函数，用于随机初始化密集神经网络层的权重和偏置\n",
    "def random_layer_params(m, n, key, scale=1.):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# 初始化具有\"sizes\"大小的全连接神经网络的所有层\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e75e1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_xδw_seq(params, x):\n",
    "    # 初始化数组\n",
    "    δ = jnp.zeros(len(params))\n",
    "    xs = jnp.zeros(len(params) + 1)\n",
    "    ws = jnp.zeros(len(params))\n",
    "    bs = jnp.zeros(len(params))\n",
    "    \n",
    "    h = jax.nn.sigmoid\n",
    "    \n",
    "    xs = xs.at[0].set(x)\n",
    "    for i, (w, b) in enumerate(params[:-1]):\n",
    "        output = w * xs[i] + b\n",
    "        activation = h(output[0, 0])\n",
    "        \n",
    "        # 存储元素\n",
    "        δ = δ.at[i].set(grad(h)(output[0, 0]))\n",
    "        ws = ws.at[i].set(w[0, 0])\n",
    "        bs = bs.at[i].set(b[0])\n",
    "        xs = xs.at[i+1].set(activation)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    preds = final_w * xs[-2] + final_b\n",
    "    \n",
    "    # 存储元素\n",
    "    δ = δ.at[-1].set(1.)\n",
    "    ws = ws.at[-1].set(final_w[0, 0])\n",
    "    bs = bs.at[-1].set(final_b[0])\n",
    "    xs = xs.at[-1].set(preds[0, 0])\n",
    "    \n",
    "    return xs, δ, ws, bs\n",
    "    \n",
    "\n",
    "def loss(params, x, y):\n",
    "    xs, δ, ws, bs = compute_xδw_seq(params, x)\n",
    "    preds = xs[-1]\n",
    "    \n",
    "    return 1 / 2 * (y - preds) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d556146",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "N = 3  # 层数\n",
    "layer_sizes = [1, ] * (N + 1)\n",
    "param_scale = 0.1\n",
    "step_size = 0.01\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23175e2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x = 5\n",
    "y = 3\n",
    "xs, δ, ws, bs = compute_xδw_seq(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c671dac",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "dxs_ad = jacfwd(lambda params, x: compute_xδw_seq(params, x)[0], argnums=0)(params, x)\n",
    "dxs_ad_mat = jnp.block([dx.reshape((-1, 1)) for dx_tuple in dxs_ad for dx in dx_tuple ])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4be854",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "jnp.block([[δ * xs[:-1]], [δ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079b082",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "L = jnp.diag(δ * ws, k=-1)\n",
    "L = L[1:, 1:]\n",
    "\n",
    "D = jax.scipy.linalg.block_diag(*[row.reshape((1, 2)) for row in jnp.block([[δ * xs[:-1]], [δ]]).T])\n",
    "\n",
    "dxs_la = jax.scipy.linalg.solve_triangular(jnp.eye(N) - L, D, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664fe7a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 检查由线性代数方法生成的`dx`\n",
    "# 是否与使用自动微分生成的结果相同\n",
    "jnp.max(jnp.abs(dxs_ad_mat - dxs_la))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78b057",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "grad_loss_ad = jnp.block([dx.reshape((-1, 1)) for dx_tuple in grad(loss)(params, x, y) for dx in dx_tuple ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd6ba2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 检查两种方法的损失梯度是否相同\n",
    "jnp.max(jnp.abs(-(y - xs[-1]) * dxs_la[-1] - grad_loss_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc05446",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update_ad(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "@jit\n",
    "def update_la(params, x, y):\n",
    "    xs, δ, ws, bs = compute_xδw_seq(params, x)\n",
    "    N = len(params)\n",
    "    L = jnp.diag(δ * ws, k=-1)\n",
    "    L = L[1:, 1:]\n",
    "\n",
    "    D = jax.scipy.linalg.block_diag(*[row.reshape((1, 2)) for row in jnp.block([[δ * xs[:-1]], [δ]]).T])\n",
    "    \n",
    "    dxs_la = jax.scipy.linalg.solve_triangular(jnp.eye(N) - L, D, lower=True)\n",
    "    \n",
    "    grads = -(y - xs[-1]) * dxs_la[-1]\n",
    "    \n",
    "    return [(w - step_size * dw, b - step_size * db) \n",
    "            for (w, b), (dw, db) in zip(params, grads.reshape((-1, 2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5026c7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 检查两次更新是否相同\n",
    "update_la(params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883afdb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "update_ad(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad98600",
   "metadata": {},
   "source": [
    "## 示例 1\n",
    "\n",
    "考虑函数\n",
    "\n",
    "$$\n",
    "f\\left(x\\right)=-3x+2\n",
    "$$\n",
    "\n",
    "在区间 $ \\left[0.5,3\\right] $ 上。\n",
    "\n",
    "我们使用200个点的均匀网格，并对网格上的每个点更新参数300次。\n",
    "\n",
    "$ h_{i} $ 是除最后一层外所有层的sigmoid激活函数，最后一层使用恒等函数，且 $ N=3 $。\n",
    "\n",
    "权重随机初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de741979",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -3 * x + 2\n",
    "\n",
    "M = 200\n",
    "grid = jnp.linspace(0.5, 3, num=M)\n",
    "f_val = f(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b85160",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "indices = jnp.arange(M)\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "def train(params, grid, f_val, key, num_epochs=300):\n",
    "    for epoch in range(num_epochs):\n",
    "        key, _ = random.split(key)\n",
    "        random_permutation = random.permutation(random.PRNGKey(1), indices)\n",
    "        for x, y in zip(grid[random_permutation], f_val[random_permutation]):\n",
    "            params = update_la(params, x, y)\n",
    "            \n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff8828",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "N = 3  # 层数\n",
    "layer_sizes = [1, ] * (N + 1)\n",
    "params_ex1 = init_network_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4977cf0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "params_ex1 = train(params_ex1, grid, f_val, key, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a5011",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "predictions = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex1, grid)[0][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b38046",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=grid, y=f_val, name=r'$-3x+2$'))\n",
    "fig.add_trace(go.Scatter(x=grid, y=predictions, name='近似值'))\n",
    "\n",
    "# 导出为PNG文件\n",
    "Image(fig.to_image(format=\"png\"))\n",
    "# 在本地运行notebook时\n",
    "# fig.show()将提供交互式图表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27aa18f",
   "metadata": {},
   "source": [
    "## 深度多少？\n",
    "\n",
    "思考上述例子中加深神经网络如何影响近似质量是一件很有趣的事\n",
    "\n",
    "- 如果网络太深，你会遇到[梯度消失问题](http://neuralnetworksanddeeplearning.com/chap5.html)  \n",
    "- 在本讲所考虑的情况下，步长和训练轮数等其他参数可能与网络层数一样重要或更重要  \n",
    "- 实际上，由于$ f $是$ x $的线性函数，使用恒等映射作为激活函数的单层网络可能效果最好。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e12ed",
   "metadata": {},
   "source": [
    "## 示例2\n",
    "\n",
    "我们使用与前一个示例相同的设置，其中\n",
    "\n",
    "$$\n",
    "f\\left(x\\right)=\\log\\left(x\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35930b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.log(x)\n",
    "\n",
    "grid = jnp.linspace(0.5, 3, num=M)\n",
    "f_val = f(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a51a0e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "N = 1  # 层数\n",
    "layer_sizes = [1, ] * (N + 1)\n",
    "params_ex2_1 = init_network_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e2ed0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "N = 2  # 层数\n",
    "layer_sizes = [1, ] * (N + 1)\n",
    "params_ex2_2 = init_network_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a7aed",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "N = 3  # 层数\n",
    "layer_sizes = [1, ] * (N + 1)\n",
    "params_ex2_3 = init_network_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dcc23",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "params_ex2_1 = train(params_ex2_1, grid, f_val, key, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4638e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "params_ex2_2 = train(params_ex2_2, grid, f_val, key, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f94041",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "params_ex2_3 = train(params_ex2_3, grid, f_val, key, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c1ca0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "predictions_1 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_1, grid)[0][:, -1]\n",
    "predictions_2 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_2, grid)[0][:, -1]\n",
    "predictions_3 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_3, grid)[0][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74859e09",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=grid, y=f_val, name=r'$\\log{x}$'))\n",
    "fig.add_trace(go.Scatter(x=grid, y=predictions_1, name='单层神经网络'))\n",
    "fig.add_trace(go.Scatter(x=grid, y=predictions_2, name='双层神经网络'))\n",
    "fig.add_trace(go.Scatter(x=grid, y=predictions_3, name='三层神经网络'))\n",
    "\n",
    "# 导出为PNG文件\n",
    "Image(fig.to_image(format=\"png\"))\n",
    "# 在本地运行notebook时，fig.show()将提供交互式图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c5c2e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "## 检查环境中是否启用了gpu\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fe55c",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">**云环境：** 此讲座网站是在无法访问`gpu`的服务器环境中构建的\n",
    "如果您在本地运行此讲座，这将让您知道代码是通过`cpu`还是`gpu`执行的"
   ]
  }
 ],
 "metadata": {
  "date": 1742685620.2369757,
  "filename": "back_prop.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "人工神经网络简介"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}