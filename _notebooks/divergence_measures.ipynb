{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e7257e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='divergence-measures'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bffdb",
   "metadata": {},
   "source": [
    "# 统计散度度量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd07b2",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [统计散度度量](#统计散度度量)  \n",
    "  - [概述](#概述)  \n",
    "  - [熵、交叉熵、KL散度入门](#熵、交叉熵、KL散度入门)  \n",
    "  - [两个Beta分布：运行示例](#两个Beta分布：运行示例)  \n",
    "  - [Kullback–Leibler散度](#Kullback–Leibler散度)  \n",
    "  - [Jensen-Shannon散度](#Jensen-Shannon散度)  \n",
    "  - [Chernoff 熵](#Chernoff-熵)  \n",
    "  - [比较散度度量](#比较散度度量)  \n",
    "  - [KL散度和最大似然估计](#KL散度和最大似然估计)  \n",
    "  - [相关讲座](#相关讲座)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34e4de",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "统计散度用于量化两个不同概率分布之间的差异，这些分布可能难以区分，原因如下：\n",
    "\n",
    "- 在一个分布下具有正概率的每个事件在另一个分布下也具有正概率  \n",
    "- 这意味着没有”确凿证据”事件的发生能让统计学家确定数据一定服从其中某一个概率分布  \n",
    "\n",
    "\n",
    "统计散度是一个将两个概率分布映射到非负实数的**函数**。\n",
    "\n",
    "统计散度函数在统计学、信息论和现在许多人称之为”机器学习”的领域中发挥着重要作用。\n",
    "\n",
    "本讲座描述了三种散度度量：\n",
    "\n",
    "- **库尔贝克-莱布勒(KL)散度**  \n",
    "- **Jensen-Shannon (JS) 散度**  \n",
    "- **切尔诺夫熵**  \n",
    "\n",
    "\n",
    "这些概念将在多个 quantecon 课程中出现。\n",
    "\n",
    "让我们首先导入必要的 Python 工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d38ed",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb7df8",
   "metadata": {},
   "source": [
    "## 熵、交叉熵、KL散度入门\n",
    "\n",
    "在深入之前,我们先介绍一些有用的基本概念。\n",
    "\n",
    "我们暂时假设 $ f $ 和 $ g $ 是离散随机变量在状态空间 $ I = \\{1, 2, \\ldots, n\\} $ 上的两个概率质量函数,满足 $ f_i \\geq 0, \\sum_{i} f_i =1, g_i \\geq 0, \\sum_{i} g_i =1 $。\n",
    "\n",
    "我们遵循一些统计学家和信息论学家的做法,将从分布 $ f $ 中观察到单次抽样 $ x = i $ 所关联的**惊奇度**或**惊奇量**定义为\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{1}{f_i}\\right)\n",
    "$$\n",
    "\n",
    "他们进一步将从单次实现中预期获得的**信息量**定义为期望惊奇度\n",
    "\n",
    "$$\n",
    "H(f) = \\sum_i f_i \\log\\left(\\frac{1}{f_i}\\right).\n",
    "$$\n",
    "\n",
    "Claude Shannon [[Shannon, 1948](https://python.quantecon.org/zreferences.html#id4)] 将 $ H(f) $ 称为分布 $ f $ 的**熵**。\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">通过对 $ \\{f_1, f_2, \\ldots, f_n\\} $ 在约束 $ \\sum_i f_i = 1 $ 下最大化 $ H(f) $,我们可以验证使熵最大化的分布是均匀分布\n",
    "$ f_i = \\frac{1}{n} . $\n",
    "均匀分布的熵 $ H(f) $ 显然等于 $ - \\log(n) $。\n",
    "\n",
    "Kullback 和 Leibler [[Kullback and Leibler, 1951](https://python.quantecon.org/zreferences.html#id5)] 将单次抽样 $ x $ 用于区分 $ f $ 和 $ g $ 所提供的信息量定义为对数似然比\n",
    "\n",
    "$$\n",
    "\\log \\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "以下两个概念被广泛用于比较两个分布 $ f $ 和 $ g $。\n",
    "\n",
    "**交叉熵:**\n",
    "\n",
    "\n",
    "<a id='equation-8e03cb73-ee78-44ad-97b3-15e2e5a0015f'></a>\n",
    "$$\n",
    "\\begin{equation}\n",
    "H(f,g) = -\\sum_{i} f_i \\log g_i\n",
    "\\end{equation} \\tag{21.1}\n",
    "$$\n",
    "\n",
    "**Kullback-Leibler (KL) 散度：**\n",
    "\n",
    "\n",
    "<a id='equation-54cb4d20-c761-4b9d-bc0b-5964e0e0309a'></a>\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_{KL}(f \\parallel g) = \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right]\n",
    "\\end{equation} \\tag{21.2}\n",
    "$$\n",
    "\n",
    "这些概念通过以下等式相关联。\n",
    "\n",
    "\n",
    "<a id='equation-eq-klcross'></a>\n",
    "$$\n",
    "D_{KL}(f \\parallel g) = H(f,g) - H(f) \\tag{21.3}\n",
    "$$\n",
    "\n",
    "要证明[(21.3)](#equation-eq-klcross)，注意到\n",
    "\n",
    "\n",
    "<a id='equation-453d615d-2bb2-4114-85ff-a949897b1e04'></a>\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(f \\parallel g) &= \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right] \\\\\n",
    "&= \\sum_{i} f_i \\left[\\log f_i - \\log g_i\\right] \\\\\n",
    "&= \\sum_{i} f_i \\log f_i - \\sum_{i} f_i \\log g_i \\\\\n",
    "&= -H(f) + H(f,g) \\\\\n",
    "&= H(f,g) - H(f)\n",
    "\\end{align} \\tag{21.4}\n",
    "$$\n",
    "\n",
    "记住$ H(f) $是从$ f $中抽取$ x $时的预期惊异度。\n",
    "\n",
    "那么上述等式告诉我们，KL散度是当预期$ x $是从$ f $中抽取而实际上是从$ g $中抽取时产生的预期”额外惊异度”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34c3dd",
   "metadata": {},
   "source": [
    "## 两个Beta分布：运行示例\n",
    "\n",
    "我们将广泛使用Beta分布来说明概念。\n",
    "\n",
    "Beta分布特别方便，因为它定义在$ [0,1] $上，并且通过适当选择其两个参数可以呈现多样的形状。\n",
    "\n",
    "具有参数$ a $和$ b $的Beta分布的密度函数为\n",
    "\n",
    "$$\n",
    "f(z; a, b) = \\frac{\\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\\Gamma(a) \\Gamma(b)}\n",
    "\\quad \\text{其中} \\quad\n",
    "\\Gamma(p) := \\int_{0}^{\\infty} x^{p-1} e^{-x} dx\n",
    "$$\n",
    "\n",
    "我们引入两个Beta分布$ f(x) $和$ g(x) $，我们将用它们来说明不同的散度度量。\n",
    "\n",
    "让我们在Python中定义参数和密度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ad91a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 两个Beta分布中的参数\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# 两个密度函数\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))\n",
    "\n",
    "# 绘制分布图\n",
    "x_range = np.linspace(0.001, 0.999, 1000)\n",
    "f_vals = [f(x) for x in x_range]\n",
    "g_vals = [g(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x) \\sim \\text{Beta}(1,1)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x) \\sim \\text{Beta}(3,1.2)$')\n",
    "\n",
    "# 填充重叠区域\n",
    "overlap = np.minimum(f_vals, g_vals)\n",
    "plt.fill_between(x_range, 0, overlap, alpha=0.3, color='purple', label='overlap')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('密度')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce218d82",
   "metadata": {},
   "source": [
    "\n",
    "<a id='rel-entropy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fff763",
   "metadata": {},
   "source": [
    "## Kullback–Leibler散度\n",
    "\n",
    "我们的第一个散度函数是**Kullback–Leibler (KL)散度**。\n",
    "\n",
    "对于概率密度（或概率质量函数）$ f $和$ g $，它的定义为\n",
    "\n",
    "$$\n",
    "D_{KL}(f\\|g) = KL(f, g) = \\int f(x) \\log \\frac{f(x)}{g(x)} \\, dx.\n",
    "$$\n",
    "\n",
    "我们可以将$ D_{KL}(f\\|g) $解释为当数据由$ f $生成而我们使用$ g $时产生的预期超额对数损失（预期超额意外性）。\n",
    "\n",
    "它有几个重要的性质：\n",
    "\n",
    "- 非负性（Gibbs不等式）：$ D_{KL}(f\\|g) \\ge 0 $，当且仅当$ f $几乎处处等于$ g $时取等号  \n",
    "- 不对称性：$ D_{KL}(f\\|g) \\neq D_{KL}(g\\|f) $（因此它不是度量）  \n",
    "- 信息分解：\n",
    "  $ D_{KL}(f\\|g) = H(f,g) - H(f) $，其中$ H(f,g) $是交叉熵，$ H(f) $是$ f $的Shannon熵  \n",
    "- 链式法则：对于联合分布$ f(x, y) $和$ g(x, y) $，\n",
    "  $ D_{KL}(f(x,y)\\|g(x,y)) = D_{KL}(f(x)\\|g(x)) + E_{f}\\left[D_{KL}(f(y|x)\\|g(y|x))\\right] $  \n",
    "\n",
    "\n",
    "KL散度在统计推断中扮演着核心角色，包括模型选择和假设检验。\n",
    "\n",
    "[似然比过程](https://python.quantecon.org/likelihood_ratio_process.html)描述了KL散度与预期对数似然比之间的联系，\n",
    "而讲座[让弥尔顿·弗里德曼困惑的问题](https://python.quantecon.org/wald_friedman.html)将其与序贯概率比检验的测试性能联系起来。\n",
    "\n",
    "让我们计算示例分布$ f $和$ g $之间的KL散度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba10183",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    通过数值积分计算KL散度KL(f, g)\n",
    "    \"\"\"\n",
    "    def integrand(w):\n",
    "        fw = f(w)\n",
    "        gw = g(w)\n",
    "        return fw * np.log(fw / gw)\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val\n",
    "\n",
    "# 计算我们示例分布之间的KL散度\n",
    "kl_fg = compute_KL(f, g)\n",
    "kl_gf = compute_KL(g, f)\n",
    "\n",
    "print(f\"KL(f, g) = {kl_fg:.4f}\")\n",
    "print(f\"KL(g, f) = {kl_gf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7284a5",
   "metadata": {},
   "source": [
    "KL散度的不对称性具有重要的实际意义。\n",
    "\n",
    "$ D_{KL}(f\\|g) $ 惩罚那些 $ f > 0 $ 但 $ g $ 接近零的区域，反映了使用 $ g $ 来建模 $ f $ 的代价，反之亦然。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8eaa0",
   "metadata": {},
   "source": [
    "## Jensen-Shannon散度\n",
    "\n",
    "有时我们需要一个对称的散度度量，用来衡量两个分布之间的差异，而不偏向任何一方。\n",
    "\n",
    "这种情况经常出现在聚类等应用中，我们想要比较分布，但不假设其中一个是真实模型。\n",
    "\n",
    "**Jensen-Shannon (JS) 散度**通过将两个分布与它们的混合分布进行比较来使KL散度对称化：\n",
    "\n",
    "$$\n",
    "JS(f,g) = \\frac{1}{2} D_{KL}(f\\|m) + \\frac{1}{2} D_{KL}(g\\|m), \\quad m = \\frac{1}{2}(f+g).\n",
    "$$\n",
    "\n",
    "其中 $ m $ 是对 $ f $ 和 $ g $ 取平均的混合分布\n",
    "\n",
    "让我们也可视化混合分布 $ m $："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a116c07",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    return 0.5 * (f(x) + g(x))\n",
    "\n",
    "m_vals = [m(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x)$')\n",
    "plt.plot(x_range, m_vals, 'g--', linewidth=2, label=r'$m(x) = \\frac{1}{2}(f(x) + g(x))$')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5eb05",
   "metadata": {},
   "source": [
    "JS散度具有以下几个有用的性质：\n",
    "\n",
    "- 对称性：$ JS(f,g)=JS(g,f) $。  \n",
    "- 有界性：$ 0 \\le JS(f,g) \\le \\log 2 $。  \n",
    "- 其平方根$ \\sqrt{JS} $在概率分布空间上是一个度量（Jensen-Shannon距离）。  \n",
    "- JS散度等于二元随机变量$ Z \\sim \\text{Bernoulli}(1/2) $（用于指示源）与样本$ X $之间的互信息，其中当$ Z=0 $时$ X $从$ f $抽样，当$ Z=1 $时从$ g $抽样。  \n",
    "\n",
    "\n",
    "Jensen-Shannon散度在某些生成模型的优化中起着关键作用，因为它是有界的、对称的，且比KL散度更平滑，通常能为训练提供更稳定的梯度。\n",
    "\n",
    "让我们计算示例分布$ f $和$ g $之间的JS散度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392a4f4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_JS(f, g):\n",
    "    \"\"\"计算Jensen-Shannon散度。\"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    js_div = 0.5 * compute_KL(f, m) + 0.5 * compute_KL(g, m)\n",
    "    return js_div\n",
    "\n",
    "js_div = compute_JS(f, g)\n",
    "print(f\"Jensen-Shannon散度 JS(f,g) = {js_div:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5aeead",
   "metadata": {},
   "source": [
    "我们可以使用带权重 $ \\alpha = (\\alpha_i)_{i=1}^{n} $ 的广义 Jensen-Shannon 散度轻松推广到两个以上的分布:\n",
    "\n",
    "$$\n",
    "JS_\\alpha(f_1, \\ldots, f_n) = \n",
    "H\\left(\\sum_{i=1}^n \\alpha_i f_i\\right) - \\sum_{i=1}^n \\alpha_i H(f_i)\n",
    "$$\n",
    "\n",
    "其中:\n",
    "\n",
    "- $ \\alpha_i \\geq 0 $ 且 $ \\sum_{i=1}^n \\alpha_i = 1 $，以及  \n",
    "- $ H(f) = -\\int f(x) \\log f(x) dx $ 是分布 $ f $ 的**香农熵**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc98baa",
   "metadata": {},
   "source": [
    "## Chernoff 熵\n",
    "\n",
    "Chernoff 熵源自[大偏差理论](https://en.wikipedia.org/wiki/Large_deviations_theory)的早期应用，该理论通过提供罕见事件的指数衰减率来改进中心极限近似。\n",
    "\n",
    "对于密度函数 $ f $ 和 $ g $，Chernoff 熵为\n",
    "\n",
    "$$\n",
    "C(f,g) = - \\log \\min_{\\phi \\in (0,1)} \\int f^{\\phi}(x) g^{1-\\phi}(x) \\, dx.\n",
    "$$\n",
    "\n",
    "注释：\n",
    "\n",
    "- 内部积分是 **Chernoff 系数**。  \n",
    "- 当 $ \\phi=1/2 $ 时，它变成 **Bhattacharyya 系数** $ \\int \\sqrt{f g} $。  \n",
    "- 在具有 $ T $ 个独立同分布观测的二元假设检验中，最优错误概率以 $ e^{-C(f,g) T} $ 的速率衰减。  \n",
    "\n",
    "\n",
    "我们将在 [似然比过程](https://python.quantecon.org/likelihood_ratio_process.html) 讲座中看到第三点的一个例子，\n",
    "我们将在模型选择的背景下研究 Chernoff 熵。\n",
    "\n",
    "让我们计算示例分布 $ f $ 和 $ g $ 之间的 Chernoff 熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf689a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def chernoff_integrand(ϕ, f, g):\n",
    "    \"\"\"计算给定 ϕ 的 Chernoff 熵中的积分。\"\"\"\n",
    "    def integrand(w):\n",
    "        return f(w)**ϕ * g(w)**(1-ϕ)\n",
    "    result, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return result\n",
    "\n",
    "def compute_chernoff_entropy(f, g):\n",
    "    \"\"\"计算 Chernoff 熵 C(f,g)。\"\"\"\n",
    "    def objective(ϕ):\n",
    "        return chernoff_integrand(ϕ, f, g)\n",
    "    result = minimize_scalar(objective, bounds=(1e-5, 1-1e-5), method='bounded')\n",
    "    min_value = result.fun\n",
    "    ϕ_optimal = result.x\n",
    "    chernoff_entropy = -np.log(min_value)\n",
    "    return chernoff_entropy, ϕ_optimal\n",
    "\n",
    "C_fg, ϕ_optimal = compute_chernoff_entropy(f, g)\n",
    "print(f\"Chernoff 熵 C(f,g) = {C_fg:.4f}\")\n",
    "print(f\"最优 ϕ = {ϕ_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565aa62f",
   "metadata": {},
   "source": [
    "## 比较散度度量\n",
    "\n",
    "我们现在比较几对Beta分布之间的这些度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9f910",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "distribution_pairs = [\n",
    "    # (f_params, g_params)\n",
    "    ((1, 1), (0.1, 0.2)),\n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (0.3, 0.4)),\n",
    "    ((1, 1), (0.5, 0.5)),\n",
    "    ((1, 1), (0.7, 0.6)),\n",
    "    ((1, 1), (0.9, 0.8)),\n",
    "    ((1, 1), (1.1, 1.05)),\n",
    "    ((1, 1), (1.2, 1.1)),\n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),\n",
    "    ((1, 1), (2.5, 1.8)),\n",
    "    ((1, 1), (3, 1.2)),\n",
    "    ((1, 1), (4, 1)),\n",
    "    ((1, 1), (5, 1))\n",
    "]\n",
    "\n",
    "# 创建比较表\n",
    "results = []\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "    kl_fg = compute_KL(f, g)\n",
    "    kl_gf = compute_KL(g, f)\n",
    "    js_div = compute_JS(f, g)\n",
    "    chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "    results.append({\n",
    "        'Pair (f, g)': f\"\\\\text{{Beta}}({f_a},{f_b}), \\\\text{{Beta}}({g_a},{g_b})\",\n",
    "        'KL(f, g)': f\"{kl_fg:.4f}\",\n",
    "        'KL(g, f)': f\"{kl_gf:.4f}\",\n",
    "        'JS': f\"{js_div:.4f}\",\n",
    "        'C': f\"{chernoff_ent:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# 按JS散度排序\n",
    "df['JS_numeric'] = df['JS'].astype(float)\n",
    "df = df.sort_values('JS_numeric').drop('JS_numeric', axis=1)\n",
    "\n",
    "columns = ' & '.join([f'\\\\text{{{col}}}' for col in df.columns])\n",
    "rows = ' \\\\\\\\\\n'.join(\n",
    "    [' & '.join([f'{val}' for val in row]) \n",
    "     for row in df.values])\n",
    "\n",
    "latex_code = rf\"\"\"\n",
    "\\begin{{array}}{{lcccc}}\n",
    "{columns} \\\\\n",
    "\\hline\n",
    "{rows}\n",
    "\\end{{array}}\n",
    "\"\"\"\n",
    "\n",
    "display(Math(latex_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ef5df",
   "metadata": {},
   "source": [
    "当我们改变Beta分布的参数时，我们可以清楚地看到各种散度测度之间的协同变化。\n",
    "\n",
    "接下来我们可视化KL散度、JS散度和切尔诺夫熵之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196377d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "kl_fg_values = [float(result['KL(f, g)']) for result in results]\n",
    "js_values = [float(result['JS']) for result in results]\n",
    "chernoff_values = [float(result['C']) for result in results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(kl_fg_values, js_values, alpha=0.7, s=60)\n",
    "axes[0].set_xlabel('KL散度 KL(f, g)')\n",
    "axes[0].set_ylabel('JS散度')\n",
    "axes[0].set_title('JS散度与KL散度的关系')\n",
    "\n",
    "axes[1].scatter(js_values, chernoff_values, alpha=0.7, s=60)\n",
    "axes[1].set_xlabel('JS散度')\n",
    "axes[1].set_ylabel('切尔诺夫熵')\n",
    "axes[1].set_title('切尔诺夫熵与JS散度的关系')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e928290",
   "metadata": {},
   "source": [
    "现在我们生成图表来直观展示随着差异度量的增加，重叠程度如何减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7c1e3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    ((1, 1), (1, 1)),   \n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),  \n",
    "    ((1, 1), (3, 1.2)),  \n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (5, 1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a60831",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_dist_diff(para_grid):\n",
    "    \"\"\"绘制选定Beta分布对的重叠图。\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    divergence_data = []\n",
    "    for i, ((f_a, f_b), (g_a, g_b)) in enumerate(param_grid):\n",
    "        row, col = divmod(i, 2)\n",
    "        f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "        g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "        kl_fg = compute_KL(f, g)\n",
    "        js_div = compute_JS(f, g)\n",
    "        chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "        divergence_data.append({\n",
    "            'f_params': (f_a, f_b),\n",
    "            'g_params': (g_a, g_b),\n",
    "            'kl_fg': kl_fg,\n",
    "            'js_div': js_div,\n",
    "            'chernoff': chernoff_ent\n",
    "        })\n",
    "        x_range = np.linspace(0, 1, 200)\n",
    "        f_vals = [f(x) for x in x_range]\n",
    "        g_vals = [g(x) for x in x_range]\n",
    "        axes[row, col].plot(x_range, f_vals, 'b-', \n",
    "                        linewidth=2, label=f'f ~ Beta({f_a},{f_b})')\n",
    "        axes[row, col].plot(x_range, g_vals, 'r-', \n",
    "                        linewidth=2, label=f'g ~ Beta({g_a},{g_b})')\n",
    "        overlap = np.minimum(f_vals, g_vals)\n",
    "        axes[row, col].fill_between(x_range, 0, \n",
    "                        overlap, alpha=0.3, color='purple', label='重叠')\n",
    "        axes[row, col].set_title(\n",
    "            f'KL(f,g)={kl_fg:.3f}, JS={js_div:.3f}, C={chernoff_ent:.3f}', \n",
    "            fontsize=12)\n",
    "        axes[row, col].legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return divergence_data\n",
    "\n",
    "divergence_data = plot_dist_diff(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159b53c",
   "metadata": {},
   "source": [
    "## KL散度和最大似然估计\n",
    "\n",
    "给定n个观测样本 $ X = \\{x_1, x_2, \\ldots, x_n\\} $，**经验分布**为\n",
    "\n",
    "$$\n",
    "p_e(x) = \\frac{1}{n} \\sum_{i=1}^n \\delta(x - x_i)\n",
    "$$\n",
    "\n",
    "其中 $ \\delta(x - x_i) $ 是中心在 $ x_i $ 的狄拉克德尔塔函数：\n",
    "\n",
    "$$\n",
    "\\delta(x - x_i) = \\begin{cases}\n",
    "+\\infty & \\text{如果 } x = x_i \\\\\n",
    "0 & \\text{如果 } x \\neq x_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **离散概率测度**：对每个观测数据点赋予概率 $ \\frac{1}{n} $  \n",
    "- **经验期望**：$ \\langle X \\rangle_{p_e} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{\\mu} $  \n",
    "- **支撑集**：仅在观测数据点 $ \\{x_1, x_2, \\ldots, x_n\\} $ 上  \n",
    "\n",
    "\n",
    "从经验分布 $ p_e $ 到参数模型 $ p_\\theta(x) $ 的KL散度为：\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta) = \\int p_e(x) \\log \\frac{p_e(x)}{p_\\theta(x)} dx\n",
    "$$\n",
    "\n",
    "利用狄拉克德尔塔函数的数学性质，可得\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta) = \\sum_{i=1}^n \\frac{1}{n} \\log \\frac{\\left(\\frac{1}{n}\\right)}{p_\\theta(x_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{1}{n} - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\log n - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "由于参数 $ \\theta $ 的对数似然函数为：\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; X) = \\sum_{i=1}^n \\log p_\\theta(x_i) ,\n",
    "$$\n",
    "\n",
    "因此最大似然选择参数以最小化\n",
    "\n",
    "$$\n",
    "D_{KL}(p_e \\parallel p_\\theta)\n",
    "$$\n",
    "\n",
    "因此，MLE等价于最小化从经验分布到统计模型$ p_\\theta $的KL散度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba63d7c",
   "metadata": {},
   "source": [
    "## 相关讲座\n",
    "\n",
    "本讲座介绍了我们将在其他地方遇到的工具。\n",
    "\n",
    "- 其他应用散度度量与统计推断之间联系的quantecon讲座包括[似然比过程](https://python.quantecon.org/likelihood_ratio_process.html)、[让弥尔顿·弗里德曼困惑的问题](https://python.quantecon.org/wald_friedman.html)和[错误模型](https://python.quantecon.org/mix_model.html)。  \n",
    "- 在研究Lawrence Blume和David Easley的异质信念和金融市场模型的[异质信念与金融市场](https://python.quantecon.org/likelihood_ratio_process_2.html)中，统计散度函数也占据核心地位。  "
   ]
  }
 ],
 "metadata": {
  "date": 1762408924.5929751,
  "filename": "divergence_measures.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "统计散度度量"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}