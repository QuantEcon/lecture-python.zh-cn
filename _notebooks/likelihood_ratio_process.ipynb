{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234ba366",
   "metadata": {},
   "source": [
    "\n",
    "<a id='likelihood-ratio-process'></a>\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05025d40",
   "metadata": {},
   "source": [
    "# 似然比过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde30fb",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [似然比过程](#似然比过程)  \n",
    "  - [概述](#概述)  \n",
    "  - [似然比过程](#似然比过程)  \n",
    "  - [当自然永久从密度g中抽取时](#当自然永久从密度g中抽取时)  \n",
    "  - [特殊性质](#特殊性质)  \n",
    "  - [自然永久从密度f中抽样](#自然永久从密度f中抽样)  \n",
    "  - [似然比检验](#似然比检验)  \n",
    "  - [假设检验和分类](#假设检验和分类)  \n",
    "  - [马尔可夫链](#马尔可夫链)  \n",
    "  - [相关讲座](#相关讲座)  \n",
    "  - [练习](#练习)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8962334",
   "metadata": {},
   "source": [
    "除了Anaconda中已有的库外，本讲座还需要以下库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a9217",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473e40e",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "本讲座介绍似然比过程及其一些用途。\n",
    "\n",
    "我们将研究与[可交换性讲座](https://python.quantecon.org/exchangeable.html)中相同的设定。\n",
    "\n",
    "我们将学习的内容包括：\n",
    "\n",
    "- 似然比过程如何成为频率派假设检验的关键要素  \n",
    "- **接收者操作特征曲线**如何总结频率派假设检验中的虚警概率和检验效能的信息  \n",
    "- 统计学家如何将第一类和第二类错误的频率派概率结合起来，形成模型选择或个体分类问题中的错误后验概率  \n",
    "- 如何使用Kullback-Leibler散度来量化具有相同支撑的两个概率分布之间的差异  \n",
    "- 二战期间美国海军如何设计一个决策规则来对弹药批次进行质量控制，这个话题为[这个讲座](https://python.quantecon.org/wald_friedman.html)做铺垫  \n",
    "- 似然比过程的一个特殊性质  \n",
    "\n",
    "\n",
    "让我们先导入一些Python工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455c092",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "FONTPATH = \"fonts/SourceHanSerifSC-SemiBold.otf\"\n",
    "mpl.font_manager.fontManager.addfont(FONTPATH)\n",
    "plt.rcParams['font.family'] = ['Source Han Serif SC']\n",
    "\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import brentq, minimize_scalar\n",
    "from scipy.stats import beta as beta_dist\n",
    "import pandas as pd\n",
    "from IPython.display import display, Math\n",
    "import quantecon as qe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734989b",
   "metadata": {},
   "source": [
    "## 似然比过程\n",
    "\n",
    "一个非负随机变量 $ W $ 具有两个概率密度函数之一，要么是 $ f $，要么是 $ g $。\n",
    "\n",
    "在时间开始之前，自然界一次性地决定是从 $ f $ 还是 $ g $ 中进行一系列独立同分布的抽样。\n",
    "\n",
    "我们有时用 $ q $ 表示自然界一次性选择的密度，所以 $ q $ 要么是 $ f $ 要么是 $ g $，且是永久性的。\n",
    "\n",
    "自然界知道它永久性地从哪个密度中抽样，但我们这些观察者并不知道。\n",
    "\n",
    "我们知道 $ f $ 和 $ g $ 两个密度，但不知道自然界选择了哪一个。\n",
    "\n",
    "但我们想要知道。\n",
    "\n",
    "为此，我们使用观测值。\n",
    "\n",
    "我们观察到一个序列 $ \\{w_t\\}_{t=1}^T $，包含 $ T $ 个独立同分布的抽样，我们知道这些抽样要么来自 $ f $ 要么来自 $ g $。\n",
    "\n",
    "我们想要利用这些观测值来推断自然界选择了 $ f $ 还是 $ g $。\n",
    "\n",
    "**似然比过程**是完成这项任务的有用工具。\n",
    "\n",
    "首先，我们定义似然比过程的一个关键组成部分，即时间 $ t $ 的似然比，作为随机变量：\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "我们假设 $ f $ 和 $ g $ 在随机变量 $ W $ 的相同可能实现区间上都具有正概率。\n",
    "\n",
    "这意味着在 $ g $ 密度下，$ \\ell (w_t)=\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)} $ 是一个均值为1的非负随机变量。\n",
    "\n",
    "序列 $ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ 的**似然比过程**定义为：\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "其中 $ w^t=\\{ w_1,\\dots,w_t\\} $ 是直到时间 $ t $ (包括 $ t $) 的观测历史。\n",
    "\n",
    "为简便起见，我们有时会写作 $ L_t = L(w^t) $。\n",
    "\n",
    "注意似然过程满足以下*递归*关系\n",
    "\n",
    "$$\n",
    "L(w^t) = \\ell (w_t) L (w^{t-1}) .\n",
    "$$\n",
    "\n",
    "似然比及其对数是 Neyman 和 Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id263)] 经典频率派推断方法中的关键工具。\n",
    "\n",
    "为了帮助我们理解其工作原理，以下 Python 代码将 $ f $ 和 $ g $ 定义为两个不同的 Beta 分布，然后通过从两个概率分布之一(例如，从 $ g $ 生成 IID 序列)生成序列 $ w^t $ 来计算和模拟相关的似然比过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360210ca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the two Beta distributions\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    \"\"\"Beta distribution density function.\"\"\"\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))\n",
    "\n",
    "def create_beta_density(a, b):\n",
    "    \"\"\"Create a beta density function with specified parameters.\"\"\"\n",
    "    return jit(lambda x: p(x, a, b))\n",
    "\n",
    "def likelihood_ratio(w, f_func, g_func):\n",
    "    \"\"\"Compute likelihood ratio for observation(s) w.\"\"\"\n",
    "    return f_func(w) / g_func(w)\n",
    "\n",
    "@jit\n",
    "def simulate_likelihood_ratios(a, b, f_func, g_func, T=50, N=500):\n",
    "    \"\"\"\n",
    "    Generate N sets of T observations of the likelihood ratio.\n",
    "    \"\"\"\n",
    "    l_arr = np.empty((N, T))\n",
    "    for i in range(N):\n",
    "        for j in range(T):\n",
    "            w = np.random.beta(a, b)\n",
    "            l_arr[i, j] = f_func(w) / g_func(w)\n",
    "    return l_arr\n",
    "\n",
    "def simulate_sequences(distribution, f_func, g_func, \n",
    "        F_params=(1, 1), G_params=(3, 1.2), T=50, N=500):\n",
    "    \"\"\"\n",
    "    Generate N sequences of T observations from specified distribution.\n",
    "    \"\"\"\n",
    "    if distribution == 'f':\n",
    "        a, b = F_params\n",
    "    elif distribution == 'g':\n",
    "        a, b = G_params\n",
    "    else:\n",
    "        raise ValueError(\"distribution must be 'f' or 'g'\")\n",
    "    \n",
    "    l_arr = simulate_likelihood_ratios(a, b, f_func, g_func, T, N)\n",
    "    l_seq = np.cumprod(l_arr, axis=1)\n",
    "    return l_arr, l_seq\n",
    "\n",
    "def plot_likelihood_paths(l_seq, title=\"Likelihood ratio paths\", \n",
    "                        ylim=None, n_paths=None):\n",
    "    \"\"\"Plot likelihood ratio paths.\"\"\"\n",
    "    N, T = l_seq.shape\n",
    "    n_show = n_paths or min(N, 100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(n_show):\n",
    "        plt.plot(range(T), l_seq[i, :], color='b', lw=0.8, alpha=0.5)\n",
    "    \n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('$L(w^t)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b06ae",
   "metadata": {},
   "source": [
    "\n",
    "<a id='nature-likeli'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004b625",
   "metadata": {},
   "source": [
    "## 当自然永久从密度g中抽取时\n",
    "\n",
    "我们首先模拟当自然永久从$ g $中抽取时的似然比过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228aaabb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 模拟当自然从g中抽取时的情况\n",
    "l_arr_g, l_seq_g = simulate_sequences('g', f, g, (F_a, F_b), (G_a, G_b))\n",
    "plot_likelihood_paths(l_seq_g, \n",
    "                     title=\"当自然从g中抽取时的$L(w^{t})$路径\",\n",
    "                     ylim=[0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9a63d",
   "metadata": {},
   "source": [
    "显然，随着样本长度 $ T $ 的增长，大部分概率质量\n",
    "向零靠近\n",
    "\n",
    "为了更清楚地看到这一点，我们绘制了随时间变化的\n",
    "路径 $ L\\left(w^{t}\\right) $ 落在区间\n",
    "$ \\left[0, 0.01\\right] $ 内的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ad3e0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_g.shape\n",
    "plt.plot(range(T), np.sum(l_seq_g <= 0.01, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f0b39",
   "metadata": {},
   "source": [
    "尽管大部分概率质量明显收敛到接近$ 0 $的一个很小区间内，但在概率密度$ g $下，$ L\\left(w^t\\right) $的无条件均值对所有$ t $恒等于$ 1 $。\n",
    "\n",
    "为了验证这个论断，首先注意到如前所述，对所有$ t $，无条件均值$ E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] $等于$ 1 $：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=\\int f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "    &=1,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这直接推出\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{1}\\right)\\bigm|q=g\\right]  &=E\\left[\\ell \\left(w_{1}\\right)\\bigm|q=g\\right]\\\\\n",
    "    &=1.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "因为$ L(w^t) = \\ell(w_t) L(w^{t-1}) $且$ \\{w_t\\}_{t=1}^t $是独立同分布序列，我们有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]  &=E\\left[L\\left(w^{t-1}\\right)\\ell \\left(w_{t}\\right)\\bigm|q=g\\right] \\\\\n",
    "         &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g,w^{t-1}\\right]\\bigm|q=g\\right] \\\\\n",
    "     &=E\\left[L\\left(w^{t-1}\\right)E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=g\\right]\\bigm|q=g\\right] \\\\\n",
    "    &=E\\left[L\\left(w^{t-1}\\right)\\bigm|q=g\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对任意$ t \\geq 1 $成立。\n",
    "\n",
    "数学归纳法表明对所有$ t \\geq 1 $，$ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1422902",
   "metadata": {},
   "source": [
    "## 特殊性质\n",
    "\n",
    "当似然比过程的大部分概率质量在 $ t \\rightarrow + \\infty $ 时堆积在 $ 0 $ 附近时，$ E\\left[L\\left(w^{t}\\right)\\bigm|q=g\\right]=1 $ 怎么可能成立？\n",
    "\n",
    "答案是，当 $ t \\rightarrow + \\infty $ 时，$ L_t $ 的分布变得越来越厚尾：足够多的质量向 $ L_t $ 的更大值移动，使得尽管大部分概率质量堆积在 $ 0 $ 附近，$ L_t $ 的均值仍然保持为1。\n",
    "\n",
    "为了说明这个特殊性质，我们模拟多条路径，并通过在每个时刻 $ t $ 对这些路径取平均来计算 $ L\\left(w^t\\right) $ 的无条件均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc2176",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "l_arr_g, l_seq_g = simulate_sequences('g', \n",
    "                f, g, (F_a, F_b), (G_a, G_b), N=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a0324",
   "metadata": {},
   "source": [
    "使用模拟来验证无条件期望值$ E\\left[L\\left(w^{t}\\right)\\right] $等于1(通过对样本路径取平均)会很有用。\n",
    "\n",
    "但是在这里使用标准蒙特卡洛模拟方法会消耗太多计算时间,因此我们不会这样做。\n",
    "\n",
    "原因是对于较大的$ t $值,$ L\\left(w^{t}\\right) $的分布极度偏斜。\n",
    "\n",
    "因为右尾部的概率密度接近于0,从右尾部采样足够多的点需要太多计算时间。\n",
    "\n",
    "我们在[这篇讲座](https://python.quantecon.org/imp_sample.html)中详细解释了这个问题。\n",
    "\n",
    "在那里我们描述了一种通过从不同的概率分布中采样来计算不同随机变量的均值,从而计算似然比均值的替代方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f99dc4",
   "metadata": {},
   "source": [
    "## 自然永久从密度f中抽样\n",
    "\n",
    "现在假设在时间0之前,自然界永久决定反复从密度f中抽样。\n",
    "\n",
    "虽然似然比$ \\ell \\left(w_{t}\\right) $在密度$ g $下的均值是1,但在密度$ f $下的均值超过1。\n",
    "\n",
    "为了说明这一点,我们计算:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\ell \\left(w_{t}\\right)\\bigm|q=f\\right]  &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}f\\left(w_{t}\\right)dw_{t} \\\\\n",
    "     &=\\int\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}\\frac{f\\left(w_{t}\\right)}{g\\left(w_{t}\\right)}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "     &=\\int \\ell \\left(w_{t}\\right)^{2}g\\left(w_{t}\\right)dw_{t} \\\\\n",
    "\n",
    "&=E\\left[\\ell \\left(w_{t}\\right)^{2}\\mid q=g\\right] \\\\\n",
    "     &=E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2}+Var\\left(\\ell \\left(w_{t}\\right)\\mid q=g\\right) \\\\\n",
    "     &>E\\left[\\ell \\left(w_{t}\\right)\\mid q=g\\right]^{2} = 1 \\\\\n",
    "       \\end{aligned}\n",
    "$$\n",
    "\n",
    "这反过来意味着似然比过程$ L(w^t) $的无条件均值将趋向于$ + \\infty $。\n",
    "\n",
    "下面的模拟验证了这个结论。\n",
    "\n",
    "请注意$ y $轴的刻度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f8606",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 模拟当自然从f中抽取时\n",
    "l_arr_f, l_seq_f = simulate_sequences('f', f, g, \n",
    "                        (F_a, F_b), (G_a, G_b), N=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55ae42",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N, T = l_arr_f.shape\n",
    "plt.plot(range(T), np.mean(l_seq_f, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e124687",
   "metadata": {},
   "source": [
    "我们还绘制了 $ L\\left(w^t\\right) $ 落入区间 $ [10000, \\infty) $ 的概率随时间的变化图，观察概率质量向 $ +\\infty $ 发散的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15b6dd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(T), np.sum(l_seq_f > 10000, axis=0) / N)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c3e37",
   "metadata": {},
   "source": [
    "## 似然比检验\n",
    "\n",
    "我们现在描述如何运用 Neyman 和 Pearson [[Neyman and Pearson, 1933](https://python.quantecon.org/zreferences.html#id263)] 的方法来检验历史数据 $ w^t $ 是否由密度函数 $ f $ 的独立同分布重复抽样生成。\n",
    "\n",
    "令 $ q $ 表示数据生成过程，因此 $ q=f \\text{ 或 } g $。\n",
    "\n",
    "在观察到样本 $ \\{W_i\\}_{i=1}^t $ 后，我们想通过执行（频率学派的）假设检验来判断自然是从 $ g $ 还是从 $ f $ 中抽样。\n",
    "\n",
    "我们指定：\n",
    "\n",
    "- 零假设 $ H_0 $：$ q=f $  \n",
    "- 备择假设 $ H_1 $：$ q=g $  \n",
    "\n",
    "\n",
    "Neyman 和 Pearson 证明了检验这个假设的最佳方法是使用**似然比检验**，形式如下：\n",
    "\n",
    "- 当 $ L(W^t) > c $ 时接受 $ H_0 $  \n",
    "- 当 $ L(W^t) < c $ 时拒绝 $ H_0 $  \n",
    "\n",
    "\n",
    "其中 $ c $ 是给定的判别阈值。\n",
    "\n",
    "设置 $ c=1 $ 是一个常见的选择。\n",
    "\n",
    "我们将在下面讨论其他 $ c $ 值选择的影响。\n",
    "\n",
    "这个检验是*最佳的*，因为它是**一致最优检验**。\n",
    "\n",
    "为了理解这一点，我们需要定义两个重要事件的概率，这些概率可以帮助我们描述与给定阈值 $ c $ 相关的检验。\n",
    "\n",
    "这两个概率是：\n",
    "\n",
    "- 第一类错误的概率（当 $ H_0 $ 为真时拒绝它）：  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- 第二类错误的概率（当 $ H_0 $ 为假时接受它）：  \n",
    "  $$\n",
    "  \\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)>c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "这两个概率是以下两个概念的基础：\n",
    "\n",
    "- 虚警概率（=显著性水平=第一类错误概率）：  \n",
    "  $$\n",
    "  \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\}\n",
    "  $$\n",
    "- 检测概率（=检验力=1减去第二类错误概率）：  \n",
    "  $$\n",
    "  1-\\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=g\\right\\}\n",
    "  $$\n",
    "\n",
    "\n",
    "[奈曼-皮尔逊引理](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma)指出，在所有可能的检验中，似然比检验在给定虚警概率的情况下能最大化检测概率。\n",
    "\n",
    "换句话说，在所有可能的检验中，似然比检验在给定**显著性水平**的情况下能最大化**检验力**。\n",
    "\n",
    "我们希望虚警概率小，检测概率大。\n",
    "\n",
    "当样本量$ t $固定时，我们可以通过调整$ c $来改变这两个概率。\n",
    "\n",
    "一个令人困扰的”现实”是，当我们改变临界值$ c $时，这两个概率会朝同一方向变化。\n",
    "\n",
    "如果不指定第一类和第二类错误的具体损失，我们很难说应该如何权衡这两种错误的概率。\n",
    "\n",
    "我们知道增加样本量$ t $可以改善统计推断。\n",
    "\n",
    "下面我们将绘制一些说明性图表来展示这一点。\n",
    "\n",
    "我们还将介绍一个用于选择样本量$ t $的经典频率派方法。\n",
    "\n",
    "让我们从将阈值$ c $固定为$ 1 $的情况开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cde65",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "c = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3236a",
   "metadata": {},
   "source": [
    "下面我们绘制上面模拟的累积似然比的对数的经验分布，这些分布是由$ f $或$ g $生成的。\n",
    "\n",
    "取对数不会影响概率的计算，因为对数是单调变换。\n",
    "\n",
    "随着$ t $的增加，第一类错误和第二类错误的概率都在减小，这是好事。\n",
    "\n",
    "这是因为当$ g $是数据生成过程时，log$ (L(w^t)) $的大部分概率质量向$ -\\infty $移动，而当数据由$ f $生成时，log$ (L(w^t)) $趋向于$ \\infty $。\n",
    "\n",
    "log$ (L(w^t)) $在$ f $和$ g $下的这种不同行为使得最终能够区分$ q=f $和$ q=g $成为可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd3d59",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_log_histograms(l_seq_f, l_seq_g, c=1, time_points=[1, 7, 14, 21]):\n",
    "    \"\"\"绘制对数似然比直方图。\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    for i, t in enumerate(time_points):\n",
    "        nr, nc = i // 2, i % 2\n",
    "        \n",
    "        axs[nr, nc].axvline(np.log(c), color=\"k\", ls=\"--\")\n",
    "        \n",
    "        hist_f, x_f = np.histogram(np.log(l_seq_f[:, t]), 200, density=True)\n",
    "        hist_g, x_g = np.histogram(np.log(l_seq_g[:, t]), 200, density=True)\n",
    "        \n",
    "        axs[nr, nc].plot(x_f[1:], hist_f, label=\"f下的分布\")\n",
    "        axs[nr, nc].plot(x_g[1:], hist_g, label=\"g下的分布\")\n",
    "        \n",
    "        # 填充错误区域\n",
    "        for j, (x, hist, label) in enumerate(\n",
    "            zip([x_f, x_g], [hist_f, hist_g], \n",
    "            [\"第一类错误\", \"第二类错误\"])):\n",
    "            ind = x[1:] <= np.log(c) if j == 0 else x[1:] > np.log(c)\n",
    "            axs[nr, nc].fill_between(x[1:][ind], hist[ind], \n",
    "                                    alpha=0.5, label=label)\n",
    "        \n",
    "        axs[nr, nc].legend()\n",
    "        axs[nr, nc].set_title(f\"t={t}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_log_histograms(l_seq_f, l_seq_g, c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533acf5",
   "metadata": {},
   "source": [
    "在上述图表中，\n",
    "\n",
    "- 蓝色区域与第一类错误的概率 $ \\alpha $ 相关但不相等，因为\n",
    "  它们是在拒绝域 $ L_t < 1 $ 上 $ \\log L_t $ 的积分，而不是 $ L_t $ 的积分  \n",
    "- 橙色区域与第二类错误的概率 $ \\beta $ 相关但不相等，因为\n",
    "  它们是在接受域 $ L_t > 1 $ 上 $ \\log L_t $ 的积分，而不是 $ L_t $ 的积分  \n",
    "\n",
    "\n",
    "当我们将 $ c $ 固定在 $ c=1 $ 时，下图显示：\n",
    "\n",
    "- 检测概率随着 $ t $ 的增加单调增加  \n",
    "- 虚警概率随着 $ t $ 的增加单调减少  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d78e30",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_error_probabilities(l_seq_f, l_seq_g, c=1):\n",
    "    \"\"\"\n",
    "    计算第一类和第二类错误概率。\n",
    "    \"\"\"\n",
    "    N, T = l_seq_f.shape\n",
    "    \n",
    "    # 第一类错误（虚警）- 在H0为真时拒绝H0\n",
    "    PFA = np.array([np.sum(l_seq_f[:, t] < c) / N for t in range(T)])\n",
    "    \n",
    "    # 第二类错误 - 在H0为假时接受H0\n",
    "    beta = np.array([np.sum(l_seq_g[:, t] >= c) / N for t in range(T)])\n",
    "    \n",
    "    # 检测概率（检验效能）\n",
    "    PD = np.array([np.sum(l_seq_g[:, t] < c) / N for t in range(T)])\n",
    "    \n",
    "    return {\n",
    "        'alpha': PFA,\n",
    "        'beta': beta, \n",
    "        'PD': PD,\n",
    "        'PFA': PFA\n",
    "    }\n",
    "\n",
    "def plot_error_probabilities(error_dict, T, c=1, title_suffix=\"\"):\n",
    "    \"\"\"绘制随时间变化的错误概率。\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(T), error_dict['PD'], label=\"检测概率\")\n",
    "    plt.plot(range(T), error_dict['PFA'], label=\"虚警概率\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"概率\")\n",
    "    plt.title(f\"错误概率 (c={c}){title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "error_probs = compute_error_probabilities(l_seq_f, l_seq_g, c=c)\n",
    "N, T = l_seq_f.shape\n",
    "plot_error_probabilities(error_probs, T, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d0570",
   "metadata": {},
   "source": [
    "对于给定的样本量 $ t $，阈值 $ c $ 唯一确定了两种类型错误的概率。\n",
    "\n",
    "如果在固定 $ t $ 的情况下，我们释放并移动 $ c $，我们将得到检测概率作为虚警概率的函数。\n",
    "\n",
    "这就产生了[接收者操作特征曲线（ROC曲线）](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)。\n",
    "\n",
    "下面，我们为不同的样本量 $ t $ 绘制接收者操作特征曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834119a1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curves(l_seq_f, l_seq_g, t_values=[1, 5, 9, 13], N=None):\n",
    "    \"\"\"绘制不同样本量的ROC曲线。\"\"\"\n",
    "    if N is None:\n",
    "        N = l_seq_f.shape[0]\n",
    "    \n",
    "    PFA = np.arange(0, 100, 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for t in t_values:\n",
    "        percentile = np.percentile(l_seq_f[:, t], PFA)\n",
    "        PD = [np.sum(l_seq_g[:, t] < p) / N for p in percentile]\n",
    "        plt.plot(PFA / 100, PD, label=f\"t={t}\")\n",
    "    \n",
    "    plt.scatter(0, 1, label=\"完美检测\")\n",
    "    plt.plot([0, 1], [0, 1], color='k', ls='--', label=\"随机检测\")\n",
    "    \n",
    "    plt.arrow(0.5, 0.5, -0.15, 0.15, head_width=0.03)\n",
    "    plt.text(0.35, 0.7, \"更好\")\n",
    "    plt.xlabel(\"虚警概率\")\n",
    "    plt.ylabel(\"检测概率\")\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC曲线\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_roc_curves(l_seq_f, l_seq_g, t_values=range(1, 15, 4), N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fe5ac",
   "metadata": {},
   "source": [
    "注意到随着 $ t $ 的增加，对于给定的判别阈值 $ c $，我们可以确保更高的检测概率和更低的虚警概率。\n",
    "\n",
    "对于给定的样本量 $ t $，当我们改变 $ c $ 时，$ \\alpha $ 和 $ \\beta $ 都会发生变化。\n",
    "\n",
    "当我们增加 $ c $ 时\n",
    "\n",
    "- $ \\alpha \\equiv  \\Pr\\left\\{ L\\left(w^{t}\\right)<c\\mid q=f\\right\\} $ 增加  \n",
    "- $ \\beta \\equiv \\Pr\\left\\{ L\\left(w^{t}\\right)>c\\mid q=g\\right\\} $ 减少  \n",
    "\n",
    "\n",
    "当 $ t \\rightarrow + \\infty $ 时，我们接近完美检测曲线，该曲线在蓝点处呈直角。\n",
    "\n",
    "对于给定的样本量 $ t $，判别阈值 $ c $ 决定了接收者操作特征曲线上的一个点。\n",
    "\n",
    "测试设计者需要权衡这两种类型错误的概率。\n",
    "\n",
    "但我们知道如何选择最小样本量来达到给定的概率目标。\n",
    "\n",
    "通常，频率学派的目标是在虚警概率有上限的情况下实现高检测概率。\n",
    "\n",
    "下面我们展示一个例子，其中我们将虚警概率固定在 $ 0.05 $。\n",
    "\n",
    "做出决定所需的样本量则由目标检测概率决定，例如 $ 0.9 $，如下图所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01475865",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "PFA = 0.05\n",
    "PD = np.empty(T)\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    c = np.percentile(l_seq_f[:, t], PFA * 100)\n",
    "    PD[t] = np.sum(l_seq_g[:, t] < c) / N\n",
    "\n",
    "plt.plot(range(T), PD)\n",
    "plt.axhline(0.9, color=\"k\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"检测概率\")\n",
    "plt.title(f\"虚警概率={PFA}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef6a06",
   "metadata": {},
   "source": [
    "美国海军显然在第二次世界大战期间使用类似这样的程序来选择质量控制测试的样本大小 $ t $。\n",
    "\n",
    "一位被命令执行此类测试的海军上校对此产生了疑虑，他向米尔顿·弗里德曼提出了这些疑虑，我们在[这篇讲座](https://python.quantecon.org/wald_friedman.html)中对此进行了描述。\n",
    "\n",
    "\n",
    "<a id='llr-h'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9be987",
   "metadata": {},
   "source": [
    "### 第三个分布 $ h $\n",
    "\n",
    "现在让我们考虑一种既不是 $ g $ 也不是 $ f $ 生成数据的情况。\n",
    "\n",
    "而是由第三个分布 $ h $ 生成。\n",
    "\n",
    "让我们研究当 $ h $ 支配数据时，累积似然比 $ L $ 的表现。\n",
    "\n",
    "这里的一个关键工具被称为**库尔贝克-莱布勒散度**，我们在[统计散度度量](https://python.quantecon.org/divergence_measures.html)中已经研究过。\n",
    "\n",
    "在我们的应用中，我们想要度量 $ f $ 或 $ g $ 与 $ h $ 的偏离程度。\n",
    "\n",
    "与我们相关的两个库尔贝克-莱布勒散度是 $ K_f $ 和 $ K_g $，定义如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{f} = D_{KL}\\bigl(h\\|f\\bigr) = KL(h, f)\n",
    "          &= E_{h}\\left[\\log\\frac{h(w)}{f(w)}\\right] \\\\\n",
    "          &= \\int \\log\\left(\\frac{h(w)}{f(w)}\\right)h(w)dw .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{g} = D_{KL}\\bigl(h\\|g\\bigr) = KL(h, g)\n",
    "          &= E_{h}\\left[\\log\\frac{h(w)}{g(w)}\\right] \\\\\n",
    "          &= \\int \\log\\left(\\frac{h(w)}{g(w)}\\right)h(w)dw .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "让我们使用[统计散度度量](https://python.quantecon.org/divergence_measures.html)中的相同代码来计算库尔贝克-莱布勒差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a433691",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    计算KL散度 KL(f, g)\n",
    "    \"\"\"\n",
    "    integrand = lambda w: f(w) * np.log(f(w) / g(w))\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val\n",
    "\n",
    "def compute_KL_h(h, f, g):\n",
    "    \"\"\"\n",
    "    计算相对于参考分布h的KL散度\n",
    "    \"\"\"\n",
    "    Kf = compute_KL(h, f)\n",
    "    Kg = compute_KL(h, g)\n",
    "    return Kf, Kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb24203",
   "metadata": {},
   "source": [
    "\n",
    "<a id='kl-link'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5750ab",
   "metadata": {},
   "source": [
    "### 一个有用的公式\n",
    "\n",
    "似然比和KL散度之间存在一个数学关系。\n",
    "\n",
    "当数据由分布$ h $生成时，期望对数似然比为：\n",
    "\n",
    "\n",
    "<a id='equation-eq-kl-likelihood-link'></a>\n",
    "$$\n",
    "\\frac{1}{t} E_{h}\\!\\bigl[\\log L_t\\bigr] = K_g - K_f \\tag{22.1}\n",
    "$$\n",
    "\n",
    "其中$ L_t=\\prod_{j=1}^{t}\\frac{f(w_j)}{g(w_j)} $是似然比过程。\n",
    "\n",
    "方程[(22.1)](#equation-eq-kl-likelihood-link)告诉我们：\n",
    "\n",
    "- 当$ K_g < K_f $（即$ g $比$ f $更接近$ h $）时，期望对数似然比为负，所以$ L\\left(w^t\\right) \\rightarrow 0 $。  \n",
    "- 当$ K_g > K_f $（即$ f $比$ g $更接近$ h $）时，期望对数似然比为正，所以$ L\\left(w^t\\right) \\rightarrow + \\infty $。  \n",
    "\n",
    "\n",
    "让我们通过模拟来验证这一点。\n",
    "\n",
    "在模拟中，我们使用Beta分布$ f $、$ g $和$ h $生成多条路径，并计算$ \\log(L(w^t)) $的路径。\n",
    "\n",
    "首先，我们编写一个函数来计算似然比过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198cc090",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_likelihood_ratios(sequences, f, g):\n",
    "    \"\"\"计算似然比和累积乘积。\"\"\"\n",
    "    l_ratios = f(sequences) / g(sequences)\n",
    "    L_cumulative = np.cumprod(l_ratios, axis=1)\n",
    "    return l_ratios, L_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a8fc0",
   "metadata": {},
   "source": [
    "我们考虑三种情况：(1) $ h $ 更接近 $ f $，(2) $ f $ 和 $ g $ 与 $ h $ 的距离大致相等，以及 (3) $ h $ 更接近 $ g $。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b084352",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"KL(h,g) > KL(h,f)\",\n",
    "        \"h_params\": (1.2, 1.1),\n",
    "        \"expected\": r\"$L_t \\to \\infty$\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KL(h,g) ≈ KL(h,f)\",\n",
    "        \"h_params\": (2, 1.35),\n",
    "        \"expected\": \"$L_t$ fluctuates\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KL(h,g) < KL(h,f)\", \n",
    "        \"h_params\": (3.5, 1.5),\n",
    "        \"expected\": r\"$L_t \\to 0$\"\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 12))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Define h\n",
    "    h = lambda x: p(x, scenario[\"h_params\"][0], \n",
    "                    scenario[\"h_params\"][1])\n",
    "    \n",
    "    # Compute KL divergences\n",
    "    Kf, Kg = compute_KL_h(h, f, g)\n",
    "    kl_diff = Kg - Kf\n",
    "    \n",
    "    # Simulate paths\n",
    "    N_paths = 100\n",
    "    T = 150\n",
    "\n",
    "    # Generate data from h\n",
    "    h_data = np.random.beta(scenario[\"h_params\"][0], \n",
    "                scenario[\"h_params\"][1], (N_paths, T))\n",
    "    l_ratios, l_cumulative = compute_likelihood_ratios(h_data, f, g)\n",
    "    log_l_cumulative = np.log(l_cumulative)\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax = axes[0, i]\n",
    "    x_range = np.linspace(0.001, 0.999, 200)\n",
    "    ax.plot(x_range, [f(x) for x in x_range], \n",
    "        'b-', label='f', linewidth=2)\n",
    "    ax.plot(x_range, [g(x) for x in x_range], \n",
    "        'r-', label='g', linewidth=2)\n",
    "    ax.plot(x_range, [h(x) for x in x_range], \n",
    "        'g--', label='h (data)', linewidth=2)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_title(scenario[\"name\"], fontsize=16)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot log likelihood ratio paths\n",
    "    ax = axes[1, i]\n",
    "    for j in range(min(20, N_paths)):\n",
    "        ax.plot(log_l_cumulative[j, :], alpha=0.3, color='purple')\n",
    "    \n",
    "    # Plot theoretical expectation\n",
    "    theory_line = kl_diff * np.arange(1, T+1)\n",
    "    ax.plot(theory_line, 'k--', linewidth=2, label=r'$t \\times (K_g - K_f)$')\n",
    "    \n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('$log L_t$')\n",
    "    ax.set_title(f'KL(h,f)={Kf:.3f}, KL(h,g)={Kg:.3f}\\n{scenario[\"expected\"]}', \n",
    "                 fontsize=16)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d3649",
   "metadata": {},
   "source": [
    "请注意\n",
    "\n",
    "- 在第一张图中，由于 $ K_g > K_f $，$ \\log L(w^t) $ 发散到 $ \\infty $。  \n",
    "- 在第二张图中，虽然仍然有 $ K_g > K_f $，但差值较小，所以 $ L(w^t) $ 发散到无穷的速度较慢。  \n",
    "- 在最后一张图中，由于 $ K_g < K_f $，$ \\log L(w^t) $ 发散到 $ -\\infty $。  \n",
    "- 黑色虚线 $ t \\left(D_{KL}(h\\|g) - D_{KL}(h\\|f)\\right) $ 与验证 [(22.1)](#equation-eq-kl-likelihood-link) 的路径紧密吻合。  \n",
    "\n",
    "\n",
    "这些观察结果与理论相符。\n",
    "\n",
    "在 [异质信念与金融市场](https://python.quantecon.org/likelihood_ratio_process_2.html) 中，我们将看到这些思想的一个应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddc785",
   "metadata": {},
   "source": [
    "## 假设检验和分类\n",
    "\n",
    "本节讨论似然比过程的另一个应用。\n",
    "\n",
    "我们描述统计学家如何结合第一类和第二类错误的频率主义概率来\n",
    "\n",
    "- 计算基于样本长度 $ T $ 选择错误模型的预期频率  \n",
    "- 计算分类问题中的预期错误率  \n",
    "\n",
    "\n",
    "我们考虑这样一种情况：自然界用已知的混合参数 $ \\pi_{-1} \\in (0,1) $ 混合已知密度 $ f $ 和 $ g $ 来生成数据，使得随机变量 $ w $ 从以下密度中抽取\n",
    "\n",
    "$$\n",
    "h (w) = \\pi_{-1} f(w) + (1-\\pi_{-1}) g(w)\n",
    "$$\n",
    "\n",
    "我们假设统计学家知道密度 $ f $ 和 $ g $ 以及混合参数 $ \\pi_{-1} $。\n",
    "\n",
    "下面，我们将设定 $ \\pi_{-1} = .5 $，尽管使用其他 $ \\pi_{-1} \\in (0,1) $ 的值进行分析也是可行的。\n",
    "\n",
    "我们假设 $ f $ 和 $ g $ 在随机变量 $ W $ 的相同可能实现区间上都赋予正概率。\n",
    "\n",
    "在下面的模拟中，我们指定 $ f $ 是 $ \\text{Beta}(1, 1) $ 分布，$ g $ 是 $ \\text{Beta}(3, 1.2) $ 分布。\n",
    "\n",
    "我们考虑两种替代的时序协议。\n",
    "\n",
    "- 时序协议1用于模型选择问题  \n",
    "- 时序协议2用于个体分类问题  \n",
    "\n",
    "\n",
    "**时序协议1：** 自然只在 $ t=-1 $ 时刻**一次性**掷硬币，以概率 $ \\pi_{-1} $ 从 $ f $ 生成一个 IID 序列 $ \\{w_t\\}_{t=1}^T $，以概率 $ 1-\\pi_{-1} $ 从 $ g $ 生成一个 IID 序列 $ \\{w_t\\}_{t=1}^T $。\n",
    "\n",
    "**时序协议2：** 自然**频繁**掷硬币。在每个时刻 $ t \\geq 0 $，自然掷一次硬币，以概率 $ \\pi_{-1} $ 从 $ f $ 中抽取 $ w_t $，以概率 $ 1-\\pi_{-1} $ 从 $ g $ 中抽取 $ w_t $。\n",
    "\n",
    "以下是我们用来实现时序协议1和2的Python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3878039",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def protocol_1(π_minus_1, T, N=1000, F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 1: Nature decides once at t=-1 which model to use.\n",
    "    \"\"\"\n",
    "    F_a, F_b = F_params\n",
    "    G_a, G_b = G_params\n",
    "    \n",
    "    # Single coin flip for the true model\n",
    "    true_models_F = np.random.rand(N) < π_minus_1\n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N - n_f\n",
    "    \n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F, :] = np.random.beta(F_a, F_b, (n_f, T))\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F, :] = np.random.beta(G_a, G_b, (n_g, T))\n",
    "    \n",
    "    return sequences, true_models_F\n",
    "\n",
    "def protocol_2(π_minus_1, T, N=1000, F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"\n",
    "    Simulate Protocol 2: Nature decides at each time step which model to use.\n",
    "    \"\"\"\n",
    "    F_a, F_b = F_params\n",
    "    G_a, G_b = G_params\n",
    "    \n",
    "    # Coin flips for each time step\n",
    "    true_models_F = np.random.rand(N, T) < π_minus_1\n",
    "    sequences = np.empty((N, T))\n",
    "    \n",
    "    n_f = np.sum(true_models_F)\n",
    "    n_g = N * T - n_f\n",
    "    \n",
    "    if n_f > 0:\n",
    "        sequences[true_models_F] = np.random.beta(F_a, F_b, n_f)\n",
    "    if n_g > 0:\n",
    "        sequences[~true_models_F] = np.random.beta(G_a, G_b, n_g)\n",
    "    \n",
    "    return sequences, true_models_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78baa92",
   "metadata": {},
   "source": [
    "**注释：** 在时序协议2下，$ \\{w_t\\}_{t=1}^T $ 是从 $ h(w) $ 中独立同分布(IID)抽取的序列。在时序协议1下，$ \\{w_t\\}_{t=1}^T $ 不是独立同分布的。它是**条件独立同分布**的 – 意味着以概率 $ \\pi_{-1} $ 它是从 $ f(w) $ 中独立同分布抽取的序列，以概率 $ 1-\\pi_{-1} $ 它是从 $ g(w) $ 中独立同分布抽取的序列。关于这一点的更多信息，请参见[这篇关于可交换性的讲座](https://python.quantecon.org/exchangeable.html)。\n",
    "\n",
    "我们再次部署一个**似然比过程**，其时间 $ t $ 分量是似然比\n",
    "\n",
    "$$\n",
    "\\ell (w_t)=\\frac{f\\left(w_t\\right)}{g\\left(w_t\\right)},\\quad t\\geq1.\n",
    "$$\n",
    "\n",
    "序列 $ \\left\\{ w_{t}\\right\\} _{t=1}^{\\infty} $ 的**似然比过程**是\n",
    "\n",
    "$$\n",
    "L\\left(w^{t}\\right)=\\prod_{i=1}^{t} \\ell (w_i),\n",
    "$$\n",
    "\n",
    "为简便起见，我们将写作 $ L_t = L(w^t) $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459afd0",
   "metadata": {},
   "source": [
    "### 模型选择错误概率\n",
    "\n",
    "我们首先研究假设时序协议1的问题。\n",
    "\n",
    "考虑一个决策者想要知道是模型 $ f $ 还是模型 $ g $ 支配着长度为 $ T $ 观测值的数据集。\n",
    "\n",
    "决策者已经观察到序列 $ \\{w_t\\}_{t=1}^T $。\n",
    "\n",
    "基于观察到的序列，似然比检验在 $ L_T \\geq 1 $ 时选择模型 $ f $，在 $ L_T < 1 $ 时选择模型 $ g $。\n",
    "\n",
    "当模型 $ f $ 生成数据时，似然比检验选择错误模型的概率是\n",
    "\n",
    "$$\n",
    "p_f = {\\rm Prob}\\left(L_T < 1\\Big| f\\right) = \\alpha_T .\n",
    "$$\n",
    "\n",
    "当模型 $ g $ 生成数据时，似然比检验选择错误模型的概率是\n",
    "\n",
    "$$\n",
    "p_g = {\\rm Prob}\\left(L_T \\geq 1 \\Big|g \\right) = \\beta_T.\n",
    "$$\n",
    "\n",
    "我们可以通过赋予自然选择模型 $ f $ 的贝叶斯先验概率 $ \\pi_{-1} = .5 $，然后对 $ p_f $ 和 $ p_g $ 取平均值来构造似然比选择错误模型的概率，从而得到检测错误的贝叶斯后验概率等于\n",
    "\n",
    "\n",
    "<a id='equation-eq-detectionerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{wrong decision}) = {1 \\over 2} (\\alpha_T + \\beta_T) . \\tag{22.2}\n",
    "$$\n",
    "\n",
    "现在让我们模拟时序协议1并计算错误概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f18e56",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_protocol_1_errors(π_minus_1, T_max, N_simulations, f_func, g_func, \n",
    "                              F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"\n",
    "    计算协议1的错误概率。\n",
    "    \"\"\"\n",
    "    sequences, true_models = protocol_1(\n",
    "        π_minus_1, T_max, N_simulations, F_params, G_params)\n",
    "    l_ratios, L_cumulative = compute_likelihood_ratios(sequences, \n",
    "                                    f_func, g_func)\n",
    "    \n",
    "    T_range = np.arange(1, T_max + 1)\n",
    "    \n",
    "    mask_f = true_models\n",
    "    mask_g = ~true_models\n",
    "    \n",
    "    L_f = L_cumulative[mask_f, :]\n",
    "    L_g = L_cumulative[mask_g, :]\n",
    "    \n",
    "    α_T = np.mean(L_f < 1, axis=0)\n",
    "    β_T = np.mean(L_g >= 1, axis=0)\n",
    "    error_prob = 0.5 * (α_T + β_T)\n",
    "    \n",
    "    return {\n",
    "        'T_range': T_range,\n",
    "        'alpha': α_T,\n",
    "        'beta': β_T, \n",
    "        'error_prob': error_prob,\n",
    "        'L_cumulative': L_cumulative,\n",
    "        'true_models': true_models\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ec226",
   "metadata": {},
   "source": [
    "以下代码可视化了时序协议1的错误概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe58959",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def analyze_protocol_1(π_minus_1, T_max, N_simulations, f_func, g_func, \n",
    "                      F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"分析协议1\"\"\"\n",
    "    result = compute_protocol_1_errors(π_minus_1, T_max, N_simulations, \n",
    "                                      f_func, g_func, F_params, G_params)\n",
    "    \n",
    "    # 绘制结果\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.plot(result['T_range'], result['alpha'], 'b-', \n",
    "             label=r'$\\alpha_T$', linewidth=2)\n",
    "    ax1.plot(result['T_range'], result['beta'], 'r-', \n",
    "             label=r'$\\beta_T$', linewidth=2)\n",
    "    ax1.set_xlabel('$T$')\n",
    "    ax1.set_ylabel('错误概率')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(result['T_range'], result['error_prob'], 'g-', \n",
    "             label=r'$\\frac{1}{2}(\\alpha_T+\\beta_T)$', linewidth=2)\n",
    "    ax2.set_xlabel('$T$')\n",
    "    ax2.set_ylabel('错误概率')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印总结\n",
    "    print(f\"在T={T_max}时:\")\n",
    "    print(f\"α_{T_max} = {result['alpha'][-1]:.4f}\")\n",
    "    print(f\"β_{T_max} = {result['beta'][-1]:.4f}\")\n",
    "    print(f\"模型选择错误概率 = {result['error_prob'][-1]:.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 分析协议1\n",
    "π_minus_1 = 0.5\n",
    "T_max = 30\n",
    "N_simulations = 10_000\n",
    "\n",
    "result_p1 = analyze_protocol_1(π_minus_1, T_max, N_simulations, \n",
    "                                f, g, (F_a, F_b), (G_a, G_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329ffd7",
   "metadata": {},
   "source": [
    "注意随着$ T $的增长，模型选择的错误概率趋近于零。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2851cec",
   "metadata": {},
   "source": [
    "### 分类\n",
    "\n",
    "我们现在考虑一个假设采用时序协议2的问题。\n",
    "\n",
    "决策者想要将观察序列$ \\{w_t\\}_{t=1}^T $的组成部分分类为来自$ f $或$ g $。\n",
    "\n",
    "决策者使用以下分类规则：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_t  & \\ {\\rm 来自 \\ }  f  \\ {\\rm 如果 \\ } l_t > 1 \\\\\n",
    "w_t  & \\ {\\rm 来自 \\ } g  \\ {\\rm 如果 \\ } l_t \\leq 1 . \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "在这个规则下，预期的错误分类率为\n",
    "\n",
    "\n",
    "<a id='equation-eq-classerrorprob'></a>\n",
    "$$\n",
    "p(\\textrm{misclassification}) = {1 \\over 2} (\\tilde \\alpha_t + \\tilde \\beta_t) \\tag{22.3}\n",
    "$$\n",
    "\n",
    "其中$ \\tilde \\alpha_t = {\\rm Prob}(l_t < 1 \\mid f) $且$ \\tilde \\beta_t = {\\rm Prob}(l_t \\geq 1 \\mid g) $。\n",
    "\n",
    "现在让我们编写一些代码来模拟它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ad7ab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_protocol_2_errors(π_minus_1, T_max, N_simulations, f_func, g_func,\n",
    "                              F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"\n",
    "    计算协议2的错误概率。\n",
    "    \"\"\"\n",
    "    sequences, true_models = protocol_2(π_minus_1, \n",
    "                        T_max, N_simulations, F_params, G_params)\n",
    "    l_ratios, _ = compute_likelihood_ratios(sequences, f_func, g_func)\n",
    "    \n",
    "    T_range = np.arange(1, T_max + 1)\n",
    "    \n",
    "    accuracy = np.empty(T_max)\n",
    "    for t in range(T_max):\n",
    "        predictions = (l_ratios[:, t] >= 1)\n",
    "        actual = true_models[:, t]\n",
    "        accuracy[t] = np.mean(predictions == actual)\n",
    "    \n",
    "    return {\n",
    "        'T_range': T_range,\n",
    "        'accuracy': accuracy,\n",
    "        'l_ratios': l_ratios,\n",
    "        'true_models': true_models\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c1da6",
   "metadata": {},
   "source": [
    "由于对于每个 $ t $，决策边界都是相同的，因此可以通过以下方式计算决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9560d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "root = brentq(lambda w: f(w) / g(w) - 1, 0.001, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208f7bc",
   "metadata": {},
   "source": [
    "我们可以绘制$ f $和$ g $的分布以及决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2c56b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "w_range = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "f_values = [f(w) for w in w_range]\n",
    "g_values = [g(w) for w in w_range]\n",
    "ratio_values = [f(w)/g(w) for w in w_range]\n",
    "\n",
    "ax.plot(w_range, f_values, 'b-', \n",
    "        label=r'$f(w) \\sim Beta(1,1)$', linewidth=2)\n",
    "ax.plot(w_range, g_values, 'r-', \n",
    "        label=r'$g(w) \\sim Beta(3,1.2)$', linewidth=2)\n",
    "\n",
    "type1_prob = 1 - beta_dist.cdf(root, F_a, F_b)\n",
    "type2_prob = beta_dist.cdf(root, G_a, G_b)\n",
    "\n",
    "w_type1 = w_range[w_range >= root]\n",
    "f_type1 = [f(w) for w in w_type1]\n",
    "ax.fill_between(w_type1, 0, f_type1, alpha=0.3, color='blue', \n",
    "                label=fr'$\\tilde \\alpha_t = {type1_prob:.2f}$')\n",
    "\n",
    "w_type2 = w_range[w_range <= root]\n",
    "g_type2 = [g(w) for w in w_type2]\n",
    "ax.fill_between(w_type2, 0, g_type2, alpha=0.3, color='red', \n",
    "                label=fr'$\\tilde \\beta_t = {type2_prob:.2f}$')\n",
    "\n",
    "ax.axvline(root, color='green', linestyle='--', alpha=0.7, \n",
    "            label=f'决策边界: $w=${root:.3f}')\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('概率密度')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71036ea",
   "metadata": {},
   "source": [
    "在绿色垂直线的左侧，$ g < f $，所以 $ l_t > 1 $；因此，落在绿线左侧的 $ w_t $ 被归类为 $ f $ 类型个体。\n",
    "\n",
    "- 红色阴影区域等于 $ \\beta $ – 将实际为 $ f $ 类型的个体错误分类为 $ g $ 类型的概率。  \n",
    "\n",
    "\n",
    "在绿色垂直线的右侧，$ g > f $，所以 $ l_t < 1 $；因此，落在绿线右侧的 $ w_t $ 被归类为 $ g $ 类型个体。\n",
    "\n",
    "- 蓝色阴影区域等于 $ \\alpha $ – 将实际为 $ g $ 类型的个体错误分类为 $ f $ 类型的概率。  \n",
    "\n",
    "\n",
    "这给了我们计算理论分类错误概率的线索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc2a30",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 计算理论 tilde α_t 和 tilde β_t\n",
    "def α_integrand(w):\n",
    "    \"\"\"用于计算 tilde α_t = P(l_t < 1 | f) 的积分\"\"\"\n",
    "    return f(w) if f(w) / g(w) < 1 else 0\n",
    "\n",
    "def β_integrand(w):\n",
    "    \"\"\"用于计算 tilde β_t = P(l_t >= 1 | g) 的积分\"\"\"\n",
    "    return g(w) if f(w) / g(w) >= 1 else 0\n",
    "\n",
    "# 计算积分\n",
    "α_theory, _ = quad(α_integrand, 0, 1, limit=100)\n",
    "β_theory, _ = quad(β_integrand, 0, 1, limit=100)\n",
    "\n",
    "theory_error = 0.5 * (α_theory + β_theory)\n",
    "\n",
    "print(f\"理论 tilde α_t = {α_theory:.4f}\")\n",
    "print(f\"理论 tilde β_t = {β_theory:.4f}\")\n",
    "print(f\"理论分类错误概率 = {theory_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9b2bc",
   "metadata": {},
   "source": [
    "现在我们模拟时序协议2并计算分类错误概率。\n",
    "\n",
    "在下一个单元格中，我们还将理论分类准确率与实验分类准确率进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522524c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def analyze_protocol_2(π_minus_1, T_max, N_simulations, f_func, g_func, \n",
    "                      theory_error=None, F_params=(1, 1), G_params=(3, 1.2)):\n",
    "    \"\"\"分析协议2。\"\"\"\n",
    "    result = compute_protocol_2_errors(π_minus_1, T_max, N_simulations, \n",
    "                                      f_func, g_func, F_params, G_params)\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(result['T_range'], result['accuracy'], \n",
    "            'b-', linewidth=2, label='实验准确率')\n",
    "    \n",
    "    if theory_error is not None:\n",
    "        plt.axhline(1 - theory_error, color='r', linestyle='--', \n",
    "                   label=f'理论准确率 = {1 - theory_error:.4f}')\n",
    "    \n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('准确率')\n",
    "    plt.legend()\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 分析协议2\n",
    "result_p2 = analyze_protocol_2(π_minus_1, T_max, N_simulations, f, g, \n",
    "                              theory_error, (F_a, F_b), (G_a, G_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9b7e1",
   "metadata": {},
   "source": [
    "让我们观察随着观测数据的不断累积，两种时序协议所做出的决策变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875422d4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compare_protocols(result1, result2):\n",
    "    \"\"\"比较两种协议的结果。\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(result1['T_range'], result1['error_prob'], linewidth=2, \n",
    "            label='协议1（模型选择）')\n",
    "    plt.plot(result2['T_range'], 1 - result2['accuracy'], \n",
    "            linestyle='--', linewidth=2, \n",
    "            label='协议2（分类）')\n",
    "    \n",
    "    plt.xlabel('$T$')\n",
    "    plt.ylabel('错误概率')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "compare_protocols(result_p1, result_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d16c1",
   "metadata": {},
   "source": [
    "从上图可以看出：\n",
    "\n",
    "- 对于两种时序协议，误差概率都从相同的水平开始，只是受到一些随机性的影响。  \n",
    "- 对于时序协议1，随着样本量的增加，误差概率会降低，因为我们只做**一个**决定 – 即选择是$ f $还是$ g $支配**所有**个体。更多的数据提供了更好的证据。  \n",
    "- 对于时序协议2，误差概率保持不变，因为我们在做**多个**决定 – 对每个观测都要做一个分类决定。  \n",
    "\n",
    "\n",
    "**注意：**思考一下大数定律是如何应用于计算模型选择问题和分类问题的误差概率的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426e92c",
   "metadata": {},
   "source": [
    "### 误差概率和散度度量\n",
    "\n",
    "一个合理的猜测是，似然比区分分布$ f $和$ g $的能力取决于它们有多”不同”。\n",
    "\n",
    "我们在[统计散度度量](https://python.quantecon.org/divergence_measures.html)中已经学习了一些衡量分布之间”差异”的度量。\n",
    "\n",
    "现在让我们研究两个在模型选择和分类背景下有用的分布之间”差异”的度量。\n",
    "\n",
    "回顾一下，概率密度$ f $和$ g $之间的Chernoff熵定义为：\n",
    "\n",
    "$$\n",
    "C(f,g) = - \\log \\min_{\\phi \\in (0,1)} \\int f^\\phi(x) g^{1-\\phi}(x) dx\n",
    "$$\n",
    "\n",
    "模型选择误差概率的上界是\n",
    "\n",
    "$$\n",
    "e^{-C(f,g)T} .\n",
    "$$\n",
    "\n",
    "让我们用Python代码来数值计算Chernoff熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d1e14",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def chernoff_integrand(ϕ, f, g):\n",
    "    \"\"\"\n",
    "    计算Chernoff熵的被积函数\n",
    "    \"\"\"\n",
    "    def integrand(w):\n",
    "        return f(w)**ϕ * g(w)**(1-ϕ)\n",
    "    \n",
    "    result, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return result\n",
    "\n",
    "def compute_chernoff_entropy(f, g):\n",
    "    \"\"\"\n",
    "    计算Chernoff熵C(f,g)\n",
    "    \"\"\"\n",
    "    def objective(ϕ):\n",
    "        return chernoff_integrand(ϕ, f, g)\n",
    "    \n",
    "    # 在(0,1)区间内找到最小值\n",
    "    result = minimize_scalar(objective, \n",
    "                             bounds=(1e-5, 1-1e-5), \n",
    "                             method='bounded')\n",
    "    min_value = result.fun\n",
    "    ϕ_optimal = result.x\n",
    "    \n",
    "    chernoff_entropy = -np.log(min_value)\n",
    "    return chernoff_entropy, ϕ_optimal\n",
    "C_fg, ϕ_optimal = compute_chernoff_entropy(f, g)\n",
    "print(f\"Chernoff熵C(f,g) = {C_fg:.4f}\")\n",
    "print(f\"最优ϕ = {ϕ_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6fb1f",
   "metadata": {},
   "source": [
    "现在让我们来研究 $ e^{-C(f,g)T} $ 作为 $ T $ 的函数时的表现，并将其与模型选择错误概率进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dc825",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "T_range = np.arange(1, T_max+1)\n",
    "chernoff_bound = np.exp(-C_fg * T_range)\n",
    "\n",
    "# 绘制比较图\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.semilogy(T_range, chernoff_bound, 'r-', linewidth=2, \n",
    "           label=f'$e^{{-C(f,g)T}}$')\n",
    "ax.semilogy(T_range, result_p1['error_prob'], 'b-', linewidth=2, \n",
    "           label='模型选择错误概率')\n",
    "\n",
    "ax.set_xlabel('T')\n",
    "ax.set_ylabel('错误概率（对数刻度）')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104ab93",
   "metadata": {},
   "source": [
    "显然，$ e^{-C(f,g)T} $是误差率的上界。\n",
    "\n",
    "在`{doc}`divergence_measures`中，我们还研究了**Jensen-Shannon散度**作为分布之间的对称距离度量。\n",
    "\n",
    "我们可以使用Jensen-Shannon散度来测量分布$ f $和$ g $之间的距离，并计算它与模型选择错误概率的协方差。\n",
    "\n",
    "我们还可以通过一些Python代码来数值计算Jensen-Shannon散度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002be177",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_JS(f, g):\n",
    "    \"\"\"\n",
    "    计算Jensen-Shannon散度\n",
    "    \"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    \n",
    "    js_div = 0.5 * compute_KL(f, m) + 0.5 * compute_KL(g, m)\n",
    "    return js_div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdef89",
   "metadata": {},
   "source": [
    "现在让我们回到我们的猜想，即在大样本量情况下的错误概率与两个分布之间的Chernoff熵有关。\n",
    "\n",
    "我们通过计算时序协议1下$ T=50 $时错误概率的对数与散度度量之间的相关性来验证这一点。\n",
    "\n",
    "在下面的模拟中，自然界从$ g $中抽取$ N/2 $个序列，从$ f $中抽取$ N/2 $个序列。\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">自然界采用这种方式，而不是在每次长度为$ T $的模拟之前抛一次公平硬币来决定是从$ g $还是$ f $中抽取。\n",
    "\n",
    "我们使用以下Beta分布对作为$ f $和$ g $的测试用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edd189",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "distribution_pairs = [\n",
    "    # (f_params, g_params)\n",
    "    ((1, 1), (0.1, 0.2)),\n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (0.3, 0.4)),\n",
    "    ((1, 1), (0.5, 0.5)),\n",
    "    ((1, 1), (0.7, 0.6)),\n",
    "    ((1, 1), (0.9, 0.8)),\n",
    "    ((1, 1), (1.1, 1.05)),\n",
    "    ((1, 1), (1.2, 1.1)),\n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),\n",
    "    ((1, 1), (2.5, 1.8)),\n",
    "    ((1, 1), (3, 1.2)),\n",
    "    ((1, 1), (4, 1)),\n",
    "    ((1, 1), (5, 1))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07530240",
   "metadata": {},
   "source": [
    "现在让我们运行模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916c903",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 模拟参数\n",
    "T_large = 50\n",
    "N_sims = 5000\n",
    "N_half = N_sims // 2\n",
    "\n",
    "# 初始化数组\n",
    "n_pairs = len(distribution_pairs)\n",
    "kl_fg_vals = np.zeros(n_pairs)\n",
    "kl_gf_vals = np.zeros(n_pairs) \n",
    "js_vals = np.zeros(n_pairs)\n",
    "chernoff_vals = np.zeros(n_pairs)\n",
    "error_probs = np.zeros(n_pairs)\n",
    "pair_names = []\n",
    "\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    # 创建密度函数\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "\n",
    "    # 计算散度度量\n",
    "    kl_fg_vals[i] = compute_KL(f, g)\n",
    "    kl_gf_vals[i] = compute_KL(g, f)\n",
    "    js_vals[i] = compute_JS(f, g)\n",
    "    chernoff_vals[i], _ = compute_chernoff_entropy(f, g)\n",
    "\n",
    "    # 生成样本\n",
    "    sequences_f = np.random.beta(f_a, f_b, (N_half, T_large))\n",
    "    sequences_g = np.random.beta(g_a, g_b, (N_half, T_large))\n",
    "\n",
    "    # 计算似然比和累积乘积\n",
    "    _, L_cumulative_f = compute_likelihood_ratios(sequences_f, f, g)\n",
    "    _, L_cumulative_g = compute_likelihood_ratios(sequences_g, f, g)\n",
    "    \n",
    "    # 获取最终值\n",
    "    L_cumulative_f = L_cumulative_f[:, -1]\n",
    "    L_cumulative_g = L_cumulative_g[:, -1]\n",
    "\n",
    "    # 计算错误概率\n",
    "    error_probs[i] = 0.5 * (np.mean(L_cumulative_f < 1) + \n",
    "                            np.mean(L_cumulative_g >= 1))\n",
    "    pair_names.append(f\"Beta({f_a},{f_b}) and Beta({g_a},{g_b})\")\n",
    "\n",
    "cor_data =  {\n",
    "    'kl_fg': kl_fg_vals,\n",
    "    'kl_gf': kl_gf_vals,\n",
    "    'js': js_vals, \n",
    "    'chernoff': chernoff_vals,\n",
    "    'error_prob': error_probs,\n",
    "    'names': pair_names,\n",
    "    'T': T_large}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fcbb8",
   "metadata": {},
   "source": [
    "现在让我们来可视化这些相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d62ada",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_error_divergence(data):\n",
    "    \"\"\"\n",
    "    绘制误差概率和散度测量之间的相关性。\n",
    "    \"\"\"\n",
    "    # 过滤掉接近零的误差概率以适应对数刻度\n",
    "    nonzero_mask = data['error_prob'] > 1e-6\n",
    "    log_error = np.log(data['error_prob'][nonzero_mask])\n",
    "    js_vals = data['js'][nonzero_mask]\n",
    "    chernoff_vals = data['chernoff'][nonzero_mask]\n",
    "\n",
    "    # 创建图形和坐标轴\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 绘制相关性的函数\n",
    "    def plot_correlation(ax, x_vals, x_label, color):\n",
    "        ax.scatter(x_vals, log_error, alpha=0.7, s=60, color=color)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(f'T={data[\"T\"]}时的对数误差概率')\n",
    "        \n",
    "        # 计算相关性和趋势线\n",
    "        corr = np.corrcoef(x_vals, log_error)[0, 1]\n",
    "        z = np.polyfit(x_vals, log_error, 2)\n",
    "        x_trend = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "        ax.plot(x_trend, np.poly1d(z)(x_trend), \n",
    "                \"r--\", alpha=0.8, linewidth=2)\n",
    "        ax.set_title(f'对数误差概率与{x_label}的关系\\n'\n",
    "                     f'相关性 = {corr:.3f}')\n",
    "    \n",
    "    # 绘制两个相关性图\n",
    "    plot_correlation(ax1, js_vals, 'JS散度', 'C0')\n",
    "    plot_correlation(ax2, chernoff_vals, 'Chernoff熵', 'C1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_error_divergence(cor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdbb37",
   "metadata": {},
   "source": [
    "显然，Chernoff熵和Jensen-Shannon熵都与模型选择错误概率密切相关。\n",
    "\n",
    "我们很快将在[让弥尔顿·弗里德曼困惑的问题](https://python.quantecon.org/wald_friedman.html)中遇到相关概念。\n",
    "\n",
    "\n",
    "<a id='lrp-markov'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c841e2",
   "metadata": {},
   "source": [
    "## 马尔可夫链\n",
    "\n",
    "现在让我们来看看一个非独立同分布随机变量序列的似然比过程。\n",
    "\n",
    "这里我们假设该序列是由有限状态空间上的马尔可夫链生成的。\n",
    "\n",
    "我们考虑在相同状态空间{1, 2, …, n}上的两个n状态不可约非周期马尔可夫链模型，它们具有正转移矩阵$ P^{(f)} $、$ P^{(g)} $和初始分布$ \\pi_0^{(f)} $、$ \\pi_0^{(g)} $。\n",
    "\n",
    "我们假设自然从链f中采样。\n",
    "\n",
    "对于样本路径$ (x_0, x_1, \\ldots, x_T) $，让$ N_{ij} $计算从状态i到j的转移次数。\n",
    "\n",
    "模型$ m \\in \\{f, g\\} $下的似然过程为\n",
    "\n",
    "$$\n",
    "L_T^{(m)} = \\pi_{0,x_0}^{(m)} \\prod_{i=1}^n \\prod_{j=1}^n \\left(P_{ij}^{(m)}\\right)^{N_{ij}}\n",
    "$$\n",
    "\n",
    "因此，\n",
    "\n",
    "$$\n",
    "\\log L_T^{(m)} =\\log\\pi_{0,x_0}^{(m)} +\\sum_{i,j}N_{ij}\\log P_{ij}^{(m)}\n",
    "$$\n",
    "\n",
    "对数似然比为\n",
    "\n",
    "\n",
    "<a id='equation-eq-llr-markov'></a>\n",
    "$$\n",
    "\\log \\frac{L_T^{(f)}}{L_T^{(g)}} = \\log \\frac{\\pi_{0,x_0}^{(f)}}{\\pi_{0,x_0}^{(g)}} + \\sum_{i,j}N_{ij}\\log \\frac{P_{ij}^{(f)}}{P_{ij}^{(g)}} \\tag{22.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e0234",
   "metadata": {},
   "source": [
    "### KL散度率\n",
    "\n",
    "根据不可约非周期马尔可夫链的遍历定理，我们有\n",
    "\n",
    "$$\n",
    "\\frac{N_{ij}}{T} \\xrightarrow{a.s.} \\pi_i^{(f)}P_{ij}^{(f)} \\quad \\text{当 } T \\to \\infty\n",
    "$$\n",
    "\n",
    "其中 $ \\boldsymbol{\\pi}^{(f)} $ 是满足 $ \\boldsymbol{\\pi}^{(f)} = \\boldsymbol{\\pi}^{(f)} P^{(f)} $ 的平稳分布。\n",
    "\n",
    "因此，\n",
    "\n",
    "$$\n",
    "\\frac{1}{T}\\log \\frac{L_T^{(f)}}{L_T^{(g)}} = \\frac{1}{T}\\log \\frac{\\pi_{0,x_0}^{(f)}}{\\pi_{0,x_0}^{(g)}} + \\frac{1}{T}\\sum_{i,j}N_{ij}\\log \\frac{P_{ij}^{(f)}}{P_{ij}^{(g)}}\n",
    "$$\n",
    "\n",
    "当 $ T \\to \\infty $ 时，我们有：\n",
    "\n",
    "- 第一项：$ \\frac{1}{T}\\log \\frac{\\pi_{0,x_0}^{(f)}}{\\pi_{0,x_0}^{(g)}} \\to 0 $  \n",
    "- 第二项：$ \\frac{1}{T}\\sum_{i,j}N_{ij}\\log \\frac{P_{ij}^{(f)}}{P_{ij}^{(g)}} \\xrightarrow{a.s.} \\sum_{i,j}\\pi_i^{(f)}P_{ij}^{(f)}\\log \\frac{P_{ij}^{(f)}}{P_{ij}^{(g)}} $  \n",
    "\n",
    "\n",
    "定义**KL散度率**为\n",
    "\n",
    "$$\n",
    "h_{KL}(f, g) = \\sum_{i=1}^n \\pi_i^{(f)} \\underbrace{\\sum_{j=1}^n P_{ij}^{(f)} \\log \\frac{P_{ij}^{(f)}}{P_{ij}^{(g)}}}_{=: KL(P_{i\\cdot}^{(f)}, P_{i\\cdot}^{(g)})}\n",
    "$$\n",
    "\n",
    "其中 $ KL(P_{i\\cdot}^{(f)}, P_{i\\cdot}^{(g)}) $ 是按行计算的KL散度。\n",
    "\n",
    "根据遍历定理，我们有\n",
    "\n",
    "$$\n",
    "\\frac{1}{T}\\log \\frac{L_T^{(f)}}{L_T^{(g)}} \\xrightarrow{a.s.} h_{KL}(f, g) \\quad \\text{当 } T \\to \\infty\n",
    "$$\n",
    "\n",
    "取期望并使用控制收敛定理，我们得到\n",
    "\n",
    "$$\n",
    "\\frac{1}{T}E_f\\left[\\log \\frac{L_T^{(f)}}{L_T^{(g)}}\\right] \\to h_{KL}(f, g) \\quad \\text{当 } T \\to \\infty\n",
    "$$\n",
    "\n",
    "在这里我们邀请读者停下来比较这个结果与[(22.1)](#equation-eq-kl-likelihood-link)。\n",
    "\n",
    "让我们在下面的模拟中验证这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980709f",
   "metadata": {},
   "source": [
    "### 模拟\n",
    "\n",
    "让我们通过三状态马尔可夫链的模拟来说明这些概念。\n",
    "\n",
    "首先编写函数来计算马尔可夫链模型的平稳分布和KL散度率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dd00d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def compute_stationary_dist(P):\n",
    "    \"\"\"\n",
    "    计算转移矩阵P的平稳分布\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    idx = np.argmax(np.abs(eigenvalues))\n",
    "    stationary = np.real(eigenvectors[:, idx])\n",
    "    return stationary / stationary.sum()\n",
    "\n",
    "def markov_kl_divergence(P_f, P_g, pi_f):\n",
    "    \"\"\"\n",
    "    计算两个马尔可夫链之间的KL散度率\n",
    "    \"\"\"\n",
    "    if np.any((P_f > 0) & (P_g == 0)):\n",
    "        return np.inf\n",
    "    \n",
    "    valid_mask = (P_f > 0) & (P_g > 0)\n",
    "    log_ratios = np.zeros_like(P_f)\n",
    "    log_ratios[valid_mask] = np.log(P_f[valid_mask] / P_g[valid_mask])\n",
    "    \n",
    "    # 用平稳概率加权并求和\n",
    "    kl_rate = np.sum(pi_f[:, np.newaxis] * P_f * log_ratios)\n",
    "    return kl_rate\n",
    "\n",
    "def simulate_markov_chain(P, pi_0, T, N_paths=1000):\n",
    "    \"\"\"\n",
    "    模拟马尔可夫链的N_paths条样本路径\n",
    "    \"\"\"\n",
    "    mc = qe.MarkovChain(P, state_values=None)\n",
    "    initial_states = np.random.choice(len(P), size=N_paths, p=pi_0)\n",
    "    paths = np.zeros((N_paths, T+1), dtype=int)\n",
    "    \n",
    "    for i in range(N_paths):\n",
    "        path = mc.simulate(T+1, init=initial_states[i])\n",
    "        paths[i, :] = path\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def compute_likelihood_ratio_markov(paths, P_f, P_g, π_0_f, π_0_g):\n",
    "    \"\"\"\n",
    "    计算马尔可夫链路径的似然比过程\n",
    "    \"\"\"\n",
    "    N_paths, T_plus_1 = paths.shape\n",
    "    T = T_plus_1 - 1\n",
    "    L_ratios = np.ones((N_paths, T+1))\n",
    "    \n",
    "    # 初始似然比\n",
    "    L_ratios[:, 0] = π_0_f[paths[:, 0]] / π_0_g[paths[:, 0]]\n",
    "    \n",
    "    # 计算序列似然比\n",
    "    for t in range(1, T+1):\n",
    "        prev_states = paths[:, t-1]\n",
    "        curr_states = paths[:, t]\n",
    "        \n",
    "        transition_ratios = (P_f[prev_states, curr_states] / \n",
    "                           P_g[prev_states, curr_states])\n",
    "        L_ratios[:, t] = L_ratios[:, t-1] * transition_ratios\n",
    "    \n",
    "    return L_ratios\n",
    "\n",
    "def analyze_markov_chains(P_f, P_g, \n",
    "                T=500, N_paths=1000, plot_paths=True, n_show=50):\n",
    "    \"\"\"\n",
    "    两个马尔可夫链的完整分析\n",
    "    \"\"\"\n",
    "    # 计算平稳分布\n",
    "    π_f = compute_stationary_dist(P_f)\n",
    "    π_g = compute_stationary_dist(P_g)\n",
    "    \n",
    "    print(f\"平稳分布 (f): {π_f}\")\n",
    "    print(f\"平稳分布 (g): {π_g}\")\n",
    "    \n",
    "    # 计算KL散度率\n",
    "    kl_rate_fg = markov_kl_divergence(P_f, P_g, π_f)\n",
    "    kl_rate_gf = markov_kl_divergence(P_g, P_f, π_g)\n",
    "    \n",
    "    print(f\"\\nKL散度率 h(f, g): {kl_rate_fg:.4f}\")\n",
    "    print(f\"KL散度率 h(g, f): {kl_rate_gf:.4f}\")\n",
    "    \n",
    "    if plot_paths:\n",
    "        # 模拟并绘制路径\n",
    "        paths_from_f = simulate_markov_chain(P_f, π_f, T, N_paths)\n",
    "        L_ratios_f = compute_likelihood_ratio_markov(\n",
    "            paths_from_f, P_f, P_g, π_f, π_g)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 绘制个别路径\n",
    "        for i in range(min(n_show, N_paths)):\n",
    "            plt.plot(np.log(L_ratios_f[i, :]), alpha=0.3, color='blue', lw=0.8)\n",
    "        \n",
    "        # 绘制理论期望\n",
    "        theory_line = kl_rate_fg * np.arange(T+1)\n",
    "        plt.plot(theory_line, 'k--', linewidth=2.5, \n",
    "                label=r'$T \\times h_{KL}(f,g)$')\n",
    "        \n",
    "        # 绘制经验平均值\n",
    "        avg_log_L = np.mean(np.log(L_ratios_f), axis=0)\n",
    "        plt.plot(avg_log_L, 'r-', linewidth=2.5, \n",
    "                label='经验平均值', alpha=0.7)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.xlabel(r'$T$')\n",
    "        plt.ylabel(r'$\\log L_T$')\n",
    "        plt.title('马尔可夫链似然比(本质 = f)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'stationary_f': π_f,\n",
    "        'stationary_g': π_g,\n",
    "        'kl_rate_fg': kl_rate_fg,\n",
    "        'kl_rate_gf': kl_rate_gf\n",
    "    }\n",
    "\n",
    "def compute_markov_selection_error(T_values, P_f, P_g, π_0_f, π_0_g, N_sim=1000):\n",
    "    \"\"\"\n",
    "    计算马尔可夫链的模型选择错误概率\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for T in T_values:\n",
    "        # 从两个模型中模拟\n",
    "        paths_f = simulate_markov_chain(P_f, π_0_f, T, N_sim//2)\n",
    "        paths_g = simulate_markov_chain(P_g, π_0_g, T, N_sim//2)\n",
    "        \n",
    "        # 计算似然比\n",
    "        L_f = compute_likelihood_ratio_markov(paths_f, P_f, P_g, π_0_f, π_0_g)\n",
    "        L_g = compute_likelihood_ratio_markov(paths_g, P_f, P_g, π_0_f, π_0_g)\n",
    "        \n",
    "        # 决策规则：如果L_T >= 1则选择f\n",
    "        error_f = np.mean(L_f[:, -1] < 1)   # 第一类错误\n",
    "        error_g = np.mean(L_g[:, -1] >= 1)  # 第二类错误\n",
    "        \n",
    "        total_error = 0.5 * (error_f + error_g)\n",
    "        errors.append(total_error)\n",
    "    \n",
    "    return np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9b81f",
   "metadata": {},
   "source": [
    "现在让我们创建一个包含两个不同的3状态马尔可夫链的示例。\n",
    "\n",
    "我们现在准备模拟路径并可视化似然比是如何演变的。\n",
    "\n",
    "我们通过绘制经验平均值和理论预测线来验证从平稳分布开始的 $ \\frac{1}{T}E_f\\left[\\log \\frac{L_T^{(f)}}{L_T^{(g)}}\\right] = h_{KL}(f, g) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce27187",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# 定义示例马尔可夫链转移矩阵\n",
    "P_f = np.array([[0.7, 0.2, 0.1],\n",
    "                [0.3, 0.5, 0.2],\n",
    "                [0.1, 0.3, 0.6]])\n",
    "\n",
    "P_g = np.array([[0.5, 0.3, 0.2],\n",
    "                [0.2, 0.6, 0.2],\n",
    "                [0.2, 0.2, 0.6]])\n",
    "\n",
    "markov_results = analyze_markov_chains(P_f, P_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc508d",
   "metadata": {},
   "source": [
    "## 相关讲座\n",
    "\n",
    "似然过程在贝叶斯学习中扮演重要角色，正如在[似然比过程和贝叶斯学习](https://python.quantecon.org/likelihood_bayes.html)中所描述的，并在[工作搜寻 VII: 带学习的搜索](https://python.quantecon.org/odu.html)中得到应用。\n",
    "\n",
    "似然比过程是Lawrence Blume和David Easley回答他们提出的问题”如果你那么聪明，为什么不富有？” [[Blume and Easley, 2006](https://python.quantecon.org/zreferences.html#id9)]的核心，这是讲座[异质信念与金融市场](https://python.quantecon.org/likelihood_ratio_process_2.html)的主题。\n",
    "\n",
    "似然比过程也出现在[Additive and Multiplicative Functionals](https://python-advanced.quantecon.org/additive_functionals.html)中，其中包含了另一个关于上述似然比过程**特殊性质**的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a02ccc",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855817c3",
   "metadata": {},
   "source": [
    "## Exercise 22.1\n",
    "\n",
    "考虑自然从第三个密度函数$ h $生成数据的情况。\n",
    "\n",
    "设$ \\{w_t\\}_{t=1}^T $是从$ h $中独立同分布抽取的样本，且$ L_t = L(w^t) $是如讲座中定义的似然比过程。\n",
    "\n",
    "证明：\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = K_g - K_f\n",
    "$$\n",
    "\n",
    "其中$ K_g, K_f $有限，$ E_h |\\log f(W)| < \\infty $且$ E_h |\\log g(W)| < \\infty $。\n",
    "\n",
    "*提示：* 首先将$ \\log L_t $表示为$ \\log \\ell(w_i) $项的和，并与$ K_f $和$ K_g $的定义进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364c17b",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 22.1](https://python.quantecon.org/#lr_ex1)\n",
    "\n",
    "由于$ w_1, \\ldots, w_t $是从$ h $中独立同分布抽取的样本，我们可以写成\n",
    "\n",
    "$$\n",
    "\\log L_t = \\log \\prod_{i=1}^t \\ell(w_i) = \\sum_{i=1}^t \\log \\ell(w_i) = \\sum_{i=1}^t \\log \\frac{f(w_i)}{g(w_i)}\n",
    "$$\n",
    "\n",
    "在$ h $下取期望\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] \n",
    "= E_h\\left[\\sum_{i=1}^t \\log \\frac{f(w_i)}{g(w_i)}\\right]\n",
    "\n",
    "= \\sum_{i=1}^t E_h\\left[\\log \\frac{f(w_i)}{g(w_i)}\\right]\n",
    "$$\n",
    "\n",
    "由于 $ w_i $ 是同分布的\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot E_h\\left[\\log \\frac{f(w)}{g(w)}\\right]\n",
    "$$\n",
    "\n",
    "其中 $ w \\sim h $。\n",
    "\n",
    "因此\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = E_h\\left[\\log \\frac{f(w)}{g(w)}\\right] = E_h[\\log f(w)] - E_h[\\log g(w)]\n",
    "$$\n",
    "\n",
    "根据 Kullback-Leibler 散度的定义\n",
    "\n",
    "$$\n",
    "K_f = \\int h(w) \\log \\frac{h(w)}{f(w)} dw = E_h[\\log h(w)] - E_h[\\log f(w)]\n",
    "$$\n",
    "\n",
    "这给出\n",
    "\n",
    "$$\n",
    "E_h[\\log f(w)] = E_h[\\log h(w)] - K_f\n",
    "$$\n",
    "\n",
    "类似地\n",
    "\n",
    "$$\n",
    "E_h[\\log g(w)] = E_h[\\log h(w)] - K_g\n",
    "$$\n",
    "\n",
    "代回得到\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{t} E_h[\\log L_t] &= E_h[\\log f(w)] - E_h[\\log g(w)] \\\\\n",
    "&= [E_h[\\log h(w)] - K_f] - [E_h[\\log h(w)] - K_g] \\\\\n",
    "&= K_g - K_f\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b89c0",
   "metadata": {},
   "source": [
    "## Exercise 22.2\n",
    "\n",
    "基于Exercise 22.1的结果，解释当 $ t \\to \\infty $ 时在以下情况下 $ L_t $ 会发生什么:\n",
    "\n",
    "1. 当 $ K_g > K_f $ 时(即 $ f $ 比 $ g $ 更”接近” $ h $)  \n",
    "1. 当 $ K_g < K_f $ 时(即 $ g $ 比 $ f $ 更”接近” $ h $)  \n",
    "\n",
    "\n",
    "将你的答案与[本节](#llr-h)中显示的模拟结果联系起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca968e7",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 22.2](https://python.quantecon.org/#lr_ex2)\n",
    "\n",
    "从Exercise 22.1中,我们知道:\n",
    "\n",
    "$$\n",
    "\\frac{1}{t} E_h[\\log L_t] = K_g - K_f\n",
    "$$\n",
    "\n",
    "**情况1:** 当 $ K_g > K_f $ 时\n",
    "\n",
    "这里, $ f $ 比 $ g $ 更”接近” $ h $。由于 $ K_g - K_f > 0 $\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot (K_g - K_f) \\to +\\infty \\text{ 当 } t \\to \\infty\n",
    "$$\n",
    "\n",
    "根据大数定律，$ \\frac{1}{t} \\log L_t \\to K_g - K_f > 0 $ 几乎必然成立。\n",
    "\n",
    "因此 $ L_t \\to +\\infty $ 几乎必然成立。\n",
    "\n",
    "**情况2:** 当 $ K_g < K_f $ 时\n",
    "\n",
    "这里，$ g $ 比 $ f $ “更接近” $ h $。由于 $ K_g - K_f < 0 $\n",
    "\n",
    "$$\n",
    "E_h[\\log L_t] = t \\cdot (K_g - K_f) \\to -\\infty \\text{ 当 } t \\to \\infty\n",
    "$$\n",
    "\n",
    "因此通过类似的推理 $ L_t \\to 0 $ 几乎必然成立。"
   ]
  }
 ],
 "metadata": {
  "date": 1761257072.4465337,
  "filename": "likelihood_ratio_process.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "似然比过程"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}