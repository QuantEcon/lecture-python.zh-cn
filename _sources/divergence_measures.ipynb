{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448d5063",
   "metadata": {},
   "source": [
    "(divergence_measures)=\n",
    "```{raw} jupyter\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# 统计散度度量\n",
    "\n",
    "```{contents} 目录\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## 概述\n",
    "\n",
    "统计散度用于量化两个不同概率分布之间的差异，这些分布可能难以区分，原因如下：\n",
    "\n",
    "  * 在一个分布下具有正概率的每个事件在另一个分布下也具有正概率\n",
    "\n",
    "  * 这意味着没有\"确凿证据\"事件的发生能让统计学家确定数据一定服从其中某一个概率分布\n",
    "\n",
    "统计散度是一个将两个概率分布映射到非负实数的**函数**。\n",
    "\n",
    "统计散度函数在统计学、信息论和现在许多人称之为\"机器学习\"的领域中发挥着重要作用。\n",
    "\n",
    "本讲座描述了三种散度度量：\n",
    "\n",
    "* **库尔贝克-莱布勒(KL)散度**\n",
    "\n",
    "* **Jensen-Shannon (JS) 散度**\n",
    "* **切尔诺夫熵**\n",
    "\n",
    "这些概念将在多个 quantecon 课程中出现。\n",
    "\n",
    "让我们首先导入必要的 Python 工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a422051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import vectorize, jit\n",
    "from math import gamma\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3aad8",
   "metadata": {},
   "source": [
    "## 熵、交叉熵、KL散度入门\n",
    "\n",
    "在深入之前,我们先介绍一些有用的基本概念。\n",
    "\n",
    "我们暂时假设 $f$ 和 $g$ 是离散随机变量在状态空间 $I = \\{1, 2, \\ldots, n\\}$ 上的两个概率质量函数,满足 $f_i \\geq 0, \\sum_{i} f_i =1, g_i \\geq 0, \\sum_{i} g_i =1$。\n",
    "\n",
    "我们遵循一些统计学家和信息论学家的做法,将从分布 $f$ 中观察到单次抽样 $x = i$ 所关联的**惊奇度**或**惊奇量**定义为\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{1}{f_i}\\right)\n",
    "$$\n",
    "\n",
    "他们进一步将从单次实现中预期获得的**信息量**定义为期望惊奇度\n",
    "\n",
    "$$\n",
    "H(f) = \\sum_i f_i \\log\\left(\\frac{1}{f_i}\\right).  \n",
    "$$\n",
    "\n",
    "Claude Shannon {cite}`shannon1948mathematical` 将 $H(f)$ 称为分布 $f$ 的**熵**。\n",
    "\n",
    "```{note}\n",
    "通过对 $\\{f_1, f_2, \\ldots, f_n\\}$ 在约束 $\\sum_i f_i = 1$ 下最大化 $H(f)$,我们可以验证使熵最大化的分布是均匀分布\n",
    "$\n",
    "f_i = \\frac{1}{n} .\n",
    "$\n",
    "均匀分布的熵 $H(f)$ 显然等于 $- \\log(n)$。\n",
    "```\n",
    "\n",
    "Kullback 和 Leibler {cite}`kullback1951information` 将单次抽样 $x$ 用于区分 $f$ 和 $g$ 所提供的信息量定义为对数似然比\n",
    "\n",
    "$$\n",
    "\\log \\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "以下两个概念被广泛用于比较两个分布 $f$ 和 $g$。\n",
    "\n",
    "**交叉熵:**\n",
    "\n",
    "\\begin{equation}\n",
    "H(f,g) = -\\sum_{i} f_i \\log g_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "**Kullback-Leibler (KL) 散度：** \n",
    "\\begin{equation}\n",
    "D_{KL}(f \\parallel g) = \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "这些概念通过以下等式相关联。\n",
    "\n",
    "$$\n",
    "D_{KL}(f \\parallel g) = H(f,g) - H(f)\n",
    "$$ (eq:KLcross)\n",
    "\n",
    "要证明{eq}`eq:KLcross`，注意到\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(f \\parallel g) &= \\sum_{i} f_i \\log\\left[\\frac{f_i}{g_i}\\right] \\\\\n",
    "&= \\sum_{i} f_i \\left[\\log f_i - \\log g_i\\right] \\\\\n",
    "&= \\sum_{i} f_i \\log f_i - \\sum_{i} f_i \\log g_i \\\\\n",
    "&= -H(f) + H(f,g) \\\\\n",
    "&= H(f,g) - H(f)\n",
    "\\end{align}\n",
    "\n",
    "记住$H(f)$是从$f$中抽取$x$时的预期惊异度。\n",
    "\n",
    "那么上述等式告诉我们，KL散度是当预期$x$是从$f$中抽取而实际上是从$g$中抽取时产生的预期\"额外惊异度\"。\n",
    "\n",
    "\n",
    "## 两个Beta分布：运行示例\n",
    "\n",
    "我们将广泛使用Beta分布来说明概念。\n",
    "\n",
    "Beta分布特别方便，因为它定义在$[0,1]$上，并且通过适当选择其两个参数可以呈现多样的形状。\n",
    "\n",
    "具有参数$a$和$b$的Beta分布的密度函数为\n",
    "\n",
    "$$\n",
    "f(z; a, b) = \\frac{\\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\\Gamma(a) \\Gamma(b)}\n",
    "\\quad \\text{其中} \\quad\n",
    "\\Gamma(p) := \\int_{0}^{\\infty} x^{p-1} e^{-x} dx\n",
    "$$\n",
    "\n",
    "我们引入两个Beta分布$f(x)$和$g(x)$，我们将用它们来说明不同的散度度量。\n",
    "\n",
    "让我们在Python中定义参数和密度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个Beta分布中的参数\n",
    "F_a, F_b = 1, 1\n",
    "G_a, G_b = 3, 1.2\n",
    "\n",
    "@vectorize\n",
    "def p(x, a, b):\n",
    "    r = gamma(a + b) / (gamma(a) * gamma(b))\n",
    "    return r * x** (a-1) * (1 - x) ** (b-1)\n",
    "\n",
    "# 两个密度函数\n",
    "f = jit(lambda x: p(x, F_a, F_b))\n",
    "g = jit(lambda x: p(x, G_a, G_b))\n",
    "\n",
    "# 绘制分布图\n",
    "x_range = np.linspace(0.001, 0.999, 1000)\n",
    "f_vals = [f(x) for x in x_range]\n",
    "g_vals = [g(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x) \\sim \\text{Beta}(1,1)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x) \\sim \\text{Beta}(3,1.2)$')\n",
    "\n",
    "# 填充重叠区域\n",
    "overlap = np.minimum(f_vals, g_vals)\n",
    "plt.fill_between(x_range, 0, overlap, alpha=0.3, color='purple', label='overlap')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('密度')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2092c1",
   "metadata": {},
   "source": [
    "(rel_entropy)=\n",
    "## Kullback–Leibler散度\n",
    "\n",
    "我们的第一个散度函数是**Kullback–Leibler (KL)散度**。\n",
    "\n",
    "对于概率密度（或概率质量函数）$f$和$g$，它的定义为\n",
    "\n",
    "$$\n",
    "D_{KL}(f\\|g) = KL(f, g) = \\int f(x) \\log \\frac{f(x)}{g(x)} \\, dx.\n",
    "$$\n",
    "\n",
    "我们可以将$D_{KL}(f\\|g)$解释为当数据由$f$生成而我们使用$g$时产生的预期超额对数损失（预期超额意外性）。\n",
    "\n",
    "它有几个重要的性质：\n",
    "\n",
    "- 非负性（Gibbs不等式）：$D_{KL}(f\\|g) \\ge 0$，当且仅当$f$几乎处处等于$g$时取等号\n",
    "- 不对称性：$D_{KL}(f\\|g) \\neq D_{KL}(g\\|f)$（因此它不是度量）\n",
    "- 信息分解：\n",
    "  $D_{KL}(f\\|g) = H(f,g) - H(f)$，其中$H(f,g)$是交叉熵，$H(f)$是$f$的Shannon熵\n",
    "- 链式法则：对于联合分布$f(x, y)$和$g(x, y)$，\n",
    "  $D_{KL}(f(x,y)\\|g(x,y)) = D_{KL}(f(x)\\|g(x)) + E_{f}\\left[D_{KL}(f(y|x)\\|g(y|x))\\right]$\n",
    "\n",
    "KL散度在统计推断中扮演着核心角色，包括模型选择和假设检验。\n",
    "\n",
    "{doc}`likelihood_ratio_process`描述了KL散度与预期对数似然比之间的联系，\n",
    "而讲座{doc}`wald_friedman`将其与序贯概率比检验的测试性能联系起来。\n",
    "\n",
    "让我们计算示例分布$f$和$g$之间的KL散度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(f, g):\n",
    "    \"\"\"\n",
    "    通过数值积分计算KL散度KL(f, g)\n",
    "    \"\"\"\n",
    "    def integrand(w):\n",
    "        fw = f(w)\n",
    "        gw = g(w)\n",
    "        return fw * np.log(fw / gw)\n",
    "    val, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return val\n",
    "\n",
    "# 计算我们示例分布之间的KL散度\n",
    "kl_fg = compute_KL(f, g)\n",
    "kl_gf = compute_KL(g, f)\n",
    "\n",
    "print(f\"KL(f, g) = {kl_fg:.4f}\")\n",
    "print(f\"KL(g, f) = {kl_gf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ae890",
   "metadata": {},
   "source": [
    "KL散度的不对称性具有重要的实际意义。\n",
    "\n",
    "$D_{KL}(f\\|g)$ 惩罚那些 $f > 0$ 但 $g$ 接近零的区域，反映了使用 $g$ 来建模 $f$ 的代价，反之亦然。\n",
    "\n",
    "## Jensen-Shannon散度\n",
    "\n",
    "有时我们需要一个对称的散度度量，用来衡量两个分布之间的差异，而不偏向任何一方。\n",
    "\n",
    "这种情况经常出现在聚类等应用中，我们想要比较分布，但不假设其中一个是真实模型。\n",
    "\n",
    "**Jensen-Shannon (JS) 散度**通过将两个分布与它们的混合分布进行比较来使KL散度对称化：\n",
    "\n",
    "$$\n",
    "JS(f,g) = \\frac{1}{2} D_{KL}(f\\|m) + \\frac{1}{2} D_{KL}(g\\|m), \\quad m = \\frac{1}{2}(f+g).\n",
    "$$\n",
    "\n",
    "其中 $m$ 是对 $f$ 和 $g$ 取平均的混合分布\n",
    "\n",
    "让我们也可视化混合分布 $m$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    return 0.5 * (f(x) + g(x))\n",
    "\n",
    "m_vals = [m(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f_vals, 'b-', linewidth=2, label=r'$f(x)$')\n",
    "plt.plot(x_range, g_vals, 'r-', linewidth=2, label=r'$g(x)$')\n",
    "plt.plot(x_range, m_vals, 'g--', linewidth=2, label=r'$m(x) = \\frac{1}{2}(f(x) + g(x))$')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437af01",
   "metadata": {},
   "source": [
    "JS散度具有以下几个有用的性质：\n",
    "\n",
    "- 对称性：$JS(f,g)=JS(g,f)$。\n",
    "- 有界性：$0 \\le JS(f,g) \\le \\log 2$。\n",
    "- 其平方根$\\sqrt{JS}$在概率分布空间上是一个度量（Jensen-Shannon距离）。\n",
    "- JS散度等于二元随机变量$Z \\sim \\text{Bernoulli}(1/2)$（用于指示源）与样本$X$之间的互信息，其中当$Z=0$时$X$从$f$抽样，当$Z=1$时从$g$抽样。\n",
    "\n",
    "Jensen-Shannon散度在某些生成模型的优化中起着关键作用，因为它是有界的、对称的，且比KL散度更平滑，通常能为训练提供更稳定的梯度。\n",
    "\n",
    "让我们计算示例分布$f$和$g$之间的JS散度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_JS(f, g):\n",
    "    \"\"\"计算Jensen-Shannon散度。\"\"\"\n",
    "    def m(w):\n",
    "        return 0.5 * (f(w) + g(w))\n",
    "    js_div = 0.5 * compute_KL(f, m) + 0.5 * compute_KL(g, m)\n",
    "    return js_div\n",
    "\n",
    "js_div = compute_JS(f, g)\n",
    "print(f\"Jensen-Shannon散度 JS(f,g) = {js_div:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d871b",
   "metadata": {},
   "source": [
    "我们可以使用带权重 $\\alpha = (\\alpha_i)_{i=1}^{n}$ 的广义 Jensen-Shannon 散度轻松推广到两个以上的分布:\n",
    "\n",
    "$$\n",
    "JS_\\alpha(f_1, \\ldots, f_n) = \n",
    "H\\left(\\sum_{i=1}^n \\alpha_i f_i\\right) - \\sum_{i=1}^n \\alpha_i H(f_i)\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- $\\alpha_i \\geq 0$ 且 $\\sum_{i=1}^n \\alpha_i = 1$，以及\n",
    "- $H(f) = -\\int f(x) \\log f(x) dx$ 是分布 $f$ 的**香农熵**\n",
    "\n",
    "## Chernoff 熵\n",
    "\n",
    "Chernoff 熵源自[大偏差理论](https://en.wikipedia.org/wiki/Large_deviations_theory)的早期应用，该理论通过提供罕见事件的指数衰减率来改进中心极限近似。\n",
    "\n",
    "对于密度函数 $f$ 和 $g$，Chernoff 熵为\n",
    "\n",
    "$$\n",
    "C(f,g) = - \\log \\min_{\\phi \\in (0,1)} \\int f^{\\phi}(x) g^{1-\\phi}(x) \\, dx.\n",
    "$$\n",
    "\n",
    "注释：\n",
    "\n",
    "- 内部积分是 **Chernoff 系数**。\n",
    "- 当 $\\phi=1/2$ 时，它变成 **Bhattacharyya 系数** $\\int \\sqrt{f g}$。\n",
    "- 在具有 $T$ 个独立同分布观测的二元假设检验中，最优错误概率以 $e^{-C(f,g) T}$ 的速率衰减。\n",
    "\n",
    "我们将在 {doc}`likelihood_ratio_process` 讲座中看到第三点的一个例子，\n",
    "我们将在模型选择的背景下研究 Chernoff 熵。\n",
    "\n",
    "让我们计算示例分布 $f$ 和 $g$ 之间的 Chernoff 熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chernoff_integrand(ϕ, f, g):\n",
    "    \"\"\"计算给定 ϕ 的 Chernoff 熵中的积分。\"\"\"\n",
    "    def integrand(w):\n",
    "        return f(w)**ϕ * g(w)**(1-ϕ)\n",
    "    result, _ = quad(integrand, 1e-5, 1-1e-5)\n",
    "    return result\n",
    "\n",
    "def compute_chernoff_entropy(f, g):\n",
    "    \"\"\"计算 Chernoff 熵 C(f,g)。\"\"\"\n",
    "    def objective(ϕ):\n",
    "        return chernoff_integrand(ϕ, f, g)\n",
    "    result = minimize_scalar(objective, bounds=(1e-5, 1-1e-5), method='bounded')\n",
    "    min_value = result.fun\n",
    "    ϕ_optimal = result.x\n",
    "    chernoff_entropy = -np.log(min_value)\n",
    "    return chernoff_entropy, ϕ_optimal\n",
    "\n",
    "C_fg, ϕ_optimal = compute_chernoff_entropy(f, g)\n",
    "print(f\"Chernoff 熵 C(f,g) = {C_fg:.4f}\")\n",
    "print(f\"最优 ϕ = {ϕ_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d80057",
   "metadata": {},
   "source": [
    "## 比较散度度量\n",
    "\n",
    "我们现在比较几对Beta分布之间的这些度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddabb3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "distribution_pairs = [\n",
    "    # (f_params, g_params)\n",
    "    ((1, 1), (0.1, 0.2)),\n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (0.3, 0.4)),\n",
    "    ((1, 1), (0.5, 0.5)),\n",
    "    ((1, 1), (0.7, 0.6)),\n",
    "    ((1, 1), (0.9, 0.8)),\n",
    "    ((1, 1), (1.1, 1.05)),\n",
    "    ((1, 1), (1.2, 1.1)),\n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),\n",
    "    ((1, 1), (2.5, 1.8)),\n",
    "    ((1, 1), (3, 1.2)),\n",
    "    ((1, 1), (4, 1)),\n",
    "    ((1, 1), (5, 1))\n",
    "]\n",
    "\n",
    "# 创建比较表\n",
    "results = []\n",
    "for i, ((f_a, f_b), (g_a, g_b)) in enumerate(distribution_pairs):\n",
    "    f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "    g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "    kl_fg = compute_KL(f, g)\n",
    "    kl_gf = compute_KL(g, f)\n",
    "    js_div = compute_JS(f, g)\n",
    "    chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "    results.append({\n",
    "        'Pair (f, g)': f\"\\\\text{{Beta}}({f_a},{f_b}), \\\\text{{Beta}}({g_a},{g_b})\",\n",
    "        'KL(f, g)': f\"{kl_fg:.4f}\",\n",
    "        'KL(g, f)': f\"{kl_gf:.4f}\",\n",
    "        'JS': f\"{js_div:.4f}\",\n",
    "        'C': f\"{chernoff_ent:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# 按JS散度排序\n",
    "df['JS_numeric'] = df['JS'].astype(float)\n",
    "df = df.sort_values('JS_numeric').drop('JS_numeric', axis=1)\n",
    "\n",
    "columns = ' & '.join([f'\\\\text{{{col}}}' for col in df.columns])\n",
    "rows = ' \\\\\\\\\\n'.join(\n",
    "    [' & '.join([f'{val}' for val in row]) \n",
    "     for row in df.values])\n",
    "\n",
    "latex_code = rf\"\"\"\n",
    "\\begin{{array}}{{lcccc}}\n",
    "{columns} \\\\\n",
    "\\hline\n",
    "{rows}\n",
    "\\end{{array}}\n",
    "\"\"\"\n",
    "\n",
    "display(Math(latex_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4527b2",
   "metadata": {},
   "source": [
    "当我们改变Beta分布的参数时，我们可以清楚地看到各种散度测度之间的协同变化。\n",
    "\n",
    "接下来我们可视化KL散度、JS散度和切尔诺夫熵之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ce885",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_fg_values = [float(result['KL(f, g)']) for result in results]\n",
    "js_values = [float(result['JS']) for result in results]\n",
    "chernoff_values = [float(result['C']) for result in results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(kl_fg_values, js_values, alpha=0.7, s=60)\n",
    "axes[0].set_xlabel('KL散度 KL(f, g)')\n",
    "axes[0].set_ylabel('JS散度')\n",
    "axes[0].set_title('JS散度与KL散度的关系')\n",
    "\n",
    "axes[1].scatter(js_values, chernoff_values, alpha=0.7, s=60)\n",
    "axes[1].set_xlabel('JS散度')\n",
    "axes[1].set_ylabel('切尔诺夫熵')\n",
    "axes[1].set_title('切尔诺夫熵与JS散度的关系')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3d2de",
   "metadata": {},
   "source": [
    "现在我们生成图表来直观展示随着差异度量的增加，重叠程度如何减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b281d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    ((1, 1), (1, 1)),   \n",
    "    ((1, 1), (1.5, 1.2)),\n",
    "    ((1, 1), (2, 1.5)),  \n",
    "    ((1, 1), (3, 1.2)),  \n",
    "    ((1, 1), (0.3, 0.3)),\n",
    "    ((1, 1), (5, 1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af4172",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_dist_diff(para_grid):\n",
    "    \"\"\"绘制选定Beta分布对的重叠图。\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    divergence_data = []\n",
    "    for i, ((f_a, f_b), (g_a, g_b)) in enumerate(param_grid):\n",
    "        row, col = divmod(i, 2)\n",
    "        f = jit(lambda x, a=f_a, b=f_b: p(x, a, b))\n",
    "        g = jit(lambda x, a=g_a, b=g_b: p(x, a, b))\n",
    "        kl_fg = compute_KL(f, g)\n",
    "        js_div = compute_JS(f, g)\n",
    "        chernoff_ent, _ = compute_chernoff_entropy(f, g)\n",
    "        divergence_data.append({\n",
    "            'f_params': (f_a, f_b),\n",
    "            'g_params': (g_a, g_b),\n",
    "            'kl_fg': kl_fg,\n",
    "            'js_div': js_div,\n",
    "            'chernoff': chernoff_ent\n",
    "        })\n",
    "        x_range = np.linspace(0, 1, 200)\n",
    "        f_vals = [f(x) for x in x_range]\n",
    "        g_vals = [g(x) for x in x_range]\n",
    "        axes[row, col].plot(x_range, f_vals, 'b-', \n",
    "                        linewidth=2, label=f'f ~ Beta({f_a},{f_b})')\n",
    "        axes[row, col].plot(x_range, g_vals, 'r-', \n",
    "                        linewidth=2, label=f'g ~ Beta({g_a},{g_b})')\n",
    "        overlap = np.minimum(f_vals, g_vals)\n",
    "        axes[row, col].fill_between(x_range, 0, \n",
    "                        overlap, alpha=0.3, color='purple', label='重叠')\n",
    "        axes[row, col].set_title(\n",
    "            f'KL(f,g)={kl_fg:.3f}, JS={js_div:.3f}, C={chernoff_ent:.3f}', \n",
    "            fontsize=12)\n",
    "        axes[row, col].legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return divergence_data\n",
    "\n",
    "divergence_data = plot_dist_diff(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289af19a",
   "metadata": {},
   "source": [
    "## KL散度和最大似然估计\n",
    "\n",
    "给定n个观测样本 $X = \\{x_1, x_2, \\ldots, x_n\\}$，**经验分布**为\n",
    "\n",
    "$$p_e(x) = \\frac{1}{n} \\sum_{i=1}^n \\delta(x - x_i)$$\n",
    "\n",
    "其中 $\\delta(x - x_i)$ 是中心在 $x_i$ 的狄拉克德尔塔函数：\n",
    "\n",
    "$$\n",
    "\\delta(x - x_i) = \\begin{cases}\n",
    "+\\infty & \\text{如果 } x = x_i \\\\\n",
    "0 & \\text{如果 } x \\neq x_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **离散概率测度**：对每个观测数据点赋予概率 $\\frac{1}{n}$\n",
    "- **经验期望**：$\\langle X \\rangle_{p_e} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{\\mu}$\n",
    "- **支撑集**：仅在观测数据点 $\\{x_1, x_2, \\ldots, x_n\\}$ 上\n",
    "\n",
    "从经验分布 $p_e$ 到参数模型 $p_\\theta(x)$ 的KL散度为：\n",
    "\n",
    "$$D_{KL}(p_e \\parallel p_\\theta) = \\int p_e(x) \\log \\frac{p_e(x)}{p_\\theta(x)} dx$$\n",
    "\n",
    "利用狄拉克德尔塔函数的数学性质，可得\n",
    "\n",
    "$$D_{KL}(p_e \\parallel p_\\theta) = \\sum_{i=1}^n \\frac{1}{n} \\log \\frac{\\left(\\frac{1}{n}\\right)}{p_\\theta(x_i)}$$\n",
    "\n",
    "$$= \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{1}{n} - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)$$\n",
    "\n",
    "$$= -\\log n - \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(x_i)$$\n",
    "\n",
    "由于参数 $\\theta$ 的对数似然函数为：\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; X) = \\sum_{i=1}^n \\log p_\\theta(x_i) ,\n",
    "$$\n",
    "\n",
    "因此最大似然选择参数以最小化\n",
    "\n",
    "$$ D_{KL}(p_e \\parallel p_\\theta) $$\n",
    "\n",
    "因此，MLE等价于最小化从经验分布到统计模型$p_\\theta$的KL散度。\n",
    "\n",
    "## 相关讲座\n",
    "\n",
    "本讲座介绍了我们将在其他地方遇到的工具。\n",
    "\n",
    "- 其他应用散度度量与统计推断之间联系的quantecon讲座包括{doc}`likelihood_ratio_process`、{doc}`wald_friedman`和{doc}`mix_model`。\n",
    "\n",
    "- 在研究Lawrence Blume和David Easley的异质信念和金融市场模型的{doc}`likelihood_ratio_process_2`中，统计散度函数也占据核心地位。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   52,
   61,
   152,
   183,
   214,
   232,
   254,
   269,
   282,
   292,
   326,
   347,
   353,
   410,
   416,
   435,
   439,
   450,
   491
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}