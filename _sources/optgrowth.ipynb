{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315f4fe5",
   "metadata": {},
   "source": [
    "(optgrowth)=\n",
    "```{raw} jupyter\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# {index}`最优增长 I：随机最优增长模型 <single: Optimal Growth I: The Stochastic Optimal Growth Model>`\n",
    "\n",
    "```{contents} 目录\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## 概述\n",
    "\n",
    "在本讲中，我们将研究一个简单的单个个体最优增长模型。\n",
    "\n",
    "该模型是标准的单部门无限期增长模型的一种版本，常见于以下文献：\n",
    "\n",
    "* {cite}`StokeyLucas1989`的第2章\n",
    "* {cite}`Ljungqvist2012`的第3.1节\n",
    "* [EDTC](http://johnstachurski.net/edtc.html)的第1章\n",
    "\n",
    "* {cite}`Sundaram1996`的第12章\n",
    "\n",
    "它是我们之前讨论过的简单{doc}`吃蛋糕问题 <cake_eating_problem>`的扩展。\n",
    "\n",
    "这一拓展包括：\n",
    "\n",
    "* 通过生产函数实现的非线性储蓄回报，以及\n",
    "* 由于生产冲击导致的随机回报。\n",
    "\n",
    "尽管增加了这些设定，该模型依然相对简单。\n",
    "\n",
    "我们将其视为通向更复杂模型的“垫脚石”。\n",
    "\n",
    "我们将使用动态规划和一系列数值方法来求解该模型。\n",
    "\n",
    "在这第一节最优增长的课程中，我们采用的方法是价值函数迭代（VFI）。\n",
    "\n",
    "尽管本讲代码运行较慢，但在后续几讲中，我们会使用多种技术大幅提升执行速度。\n",
    "\n",
    "让我们从一些基本导入开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "FONTPATH = \"fonts/SourceHanSerifSC-SemiBold.otf\"\n",
    "mpl.font_manager.fontManager.addfont(FONTPATH)\n",
    "plt.rcParams['font.family'] = ['Source Han Serif SC']\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #设置默认图形大小\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f265de",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "```{index} single: Optimal Growth; Model\n",
    "```\n",
    "\n",
    "考虑一个主体在时间 $t$ 拥有数量为 $y_t \\in \\mathbb R_+ := [0, \\infty)$ 的消费品。\n",
    "\n",
    "这些产出可以被消费或投资。\n",
    "\n",
    "当产出被投资时，它会一比一地转化为资本。\n",
    "\n",
    "由此产生的资本存量，记为 $k_{t+1}$，将在下一期投入生产。\n",
    "\n",
    "生产过程是随机的，因为它还取决于在当期结束时实现的一个冲击 $\\xi_{t+1}$。\n",
    "\n",
    "下一期的产出为\n",
    "\n",
    "$$\n",
    "y_{t+1} := f(k_{t+1}) \\xi_{t+1}\n",
    "$$\n",
    "\n",
    "其中 $f \\colon \\mathbb R_+ \\to \\mathbb R_+$ 被称为生产函数。\n",
    "\n",
    "资源约束为\n",
    "\n",
    "```{math}\n",
    ":label: outcsdp0\n",
    "\n",
    "k_{t+1} + c_t \\leq y_t\n",
    "```\n",
    "\n",
    "且所有变量都必须为非负数。\n",
    "\n",
    "### 假设和说明\n",
    "\n",
    "在接下来的设定中，\n",
    "\n",
    "* 序列 $\\{\\xi_t\\}$ 被假定为独立同分布(IID)。\n",
    "* 每个 $\\xi_t$ 的共同分布记为 $\\phi$。\n",
    "* 假设生产函数 $f$ 是递增且连续的。\n",
    "* 资本折旧并未明确表示，但可以被整合到生产函数中。\n",
    "\n",
    "虽然许多其他随机增长模型的处理方法是将 $k_t$ 作为状态变量，我们采用 $y_t$ 作为状态变量。\n",
    "\n",
    "这样做使我们能够在只有一个状态变量的情况下处理随机增长模型。\n",
    "\n",
    "在我们的其他讲义中，我们会考虑替代性的状态变量设定以及事件发生时点的不同选择。\n",
    "\n",
    "### 优化问题\n",
    "\n",
    "给定初始状态 $y_0$，个体希望最大化\n",
    "\n",
    "```{math}\n",
    ":label: texs0_og2\n",
    "\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\right]\n",
    "```\n",
    "\n",
    "约束条件为\n",
    "\n",
    "```{math}\n",
    ":label: og_conse\n",
    "\n",
    "y_{t+1} = f(y_t - c_t) \\xi_{t+1}\n",
    "\\quad \\text{和} \\quad\n",
    "0 \\leq c_t \\leq y_t\n",
    "\\quad \\text{对所有 } t\n",
    "```\n",
    "\n",
    "其中\n",
    "\n",
    "* $u$ 是有界、连续且严格递增的效用函数，且\n",
    "* $\\beta \\in (0, 1)$ 是贴现因子。\n",
    "\n",
    "在{eq}`og_conse`中，我们假设资源约束{eq}`outcsdp0`是以等式形式成立的——这是合理的，因为 $u$ 是严格递增的，在最优状态下不会浪费任何产出。\n",
    "\n",
    "总的来说，个体的目标是选择一个消费路径 $c_0, c_1, c_2, \\ldots$，使其：\n",
    "\n",
    "1. 非负，\n",
    "1. 满足资源约束{eq}`outcsdp0`，\n",
    "1. 最优，即相对于所有其他可行的消费序列，所选路径使目标函数{eq}`texs0_og2`达到最大化，以及\n",
    "1. *适应性*，即行动 $c_t$ 只依赖于可观察的结果，而不依赖于未来的结果，如 $\\xi_{t+1}$。\n",
    "\n",
    "在当前语境下：\n",
    "\n",
    "* $y_t$ 被称为*状态*变量——它刻画了每一期开始时的\"世界状态\"。\n",
    "* $c_t$ 被称为*控制*变量——它表示代理人在观察到状态之后所做出的消费决策。\n",
    "\n",
    "### 策略函数方法\n",
    "\n",
    "```{index} single: Optimal Growth; Policy Function Approach\n",
    "```\n",
    "\n",
    "解决该问题的一种方法是寻找最佳的**策略函数**。\n",
    "\n",
    "策略函数是一个从过去和现在的可观察变量映射到当前行动的函数。\n",
    "\n",
    "我们特别关注**马尔可夫策略**，它是从当前状态 $y_t$ 映射到当前行动 $c_t$ 的函数。\n",
    "\n",
    "对于像这样的动态规划问题（实际上对于任何[马尔可夫决策过程](https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/5824810)），最优策略总是一个马尔可夫策略。\n",
    "\n",
    "换句话说，当前状态 $y_t$ 为历史提供了一个[充分统计量](https://baike.baidu.com/item/%E5%85%85%E5%88%86%E7%BB%9F%E8%AE%A1%E9%87%8F/12715941)，使得在做出最优决策时，不需要依赖过去的历史。\n",
    "\n",
    "这个结论相当直观，其证明可以参考{cite}`StokeyLucas1989`（第4.1节）。\n",
    "\n",
    "在接下来的分析中，我们将专注于寻找最优的马尔可夫策略。\n",
    "\n",
    "在我们的情况下，马尔可夫策略是一个函数 $\\sigma \\colon \\mathbb R_+ \\to \\mathbb R_+$，其含义是：将状态映射到行动，即\n",
    "\n",
    "$$\n",
    "c_t = \\sigma(y_t) \\quad \\text{对所有 } t\n",
    "$$\n",
    "\n",
    "我们称 $\\sigma$ 为*可行消费策略*，当且仅当它满足\n",
    "\n",
    "```{math}\n",
    ":label: idp_fp_og2\n",
    "\n",
    "0 \\leq \\sigma(y) \\leq y\n",
    "\\quad \\text{对所有} \\quad\n",
    "y \\in \\mathbb R_+\n",
    "```\n",
    "\n",
    "换句话说，可行消费策略是满足资源约束的马尔可夫策略。\n",
    "\n",
    "所有可行消费策略的集合记为 $\\Sigma$。\n",
    "\n",
    "每个 $\\sigma \\in \\Sigma$ 都决定了一个[连续状态马尔可夫过程](https://python-advanced.quantecon.org/stationary_densities.html) $\\{y_t\\}$ ，其动态为\n",
    "\n",
    "```{math}\n",
    ":label: firstp0_og2\n",
    "\n",
    "y_{t+1} = f(y_t - \\sigma(y_t)) \\xi_{t+1},\n",
    "\\quad y_0 \\text{ 给定}\n",
    "```\n",
    "\n",
    "这就描述了在选定并遵循策略 $\\sigma$ 时，产出的时间路径。\n",
    "\n",
    "将策略函数代入目标函数，可以得到\n",
    "\n",
    "```{math}\n",
    ":label: texss\n",
    "\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\,\n",
    "\\right] =\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(y_t)) \\,\n",
    "\\right]\n",
    "```\n",
    "\n",
    "这就是在给定初始收入 $y_0$ 时，永远遵循某一策略 $\\sigma$ 的期望现值。\n",
    "\n",
    "目标是选择一个策略，使得该数值尽可能大。\n",
    "\n",
    "下一节将更正式地介绍这些思想。\n",
    "\n",
    "### 最优性\n",
    "\n",
    "与给定策略 $\\sigma$ 相关的 映射 $v_\\sigma$ 定义为\n",
    "\n",
    "```{math}\n",
    ":label: vfcsdp00\n",
    "\n",
    "v_{\\sigma}(y) =\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(y_t)) \\right]\n",
    "```\n",
    "\n",
    "其中 $\\{y_t\\}$ 由方程 {eq}`firstp0_og2` 给出，且 $y_0 = y$。\n",
    "\n",
    "换句话说，这是从初始条件 $y$ 开始遵循策略 $\\sigma$ 的终身价值。\n",
    "\n",
    "**价值函数**定义为\n",
    "\n",
    "```{math}\n",
    ":label: vfcsdp0\n",
    "\n",
    "v^*(y) := \\sup_{\\sigma \\in \\Sigma} \\; v_{\\sigma}(y)\n",
    "```\n",
    "\n",
    "价值函数给出了在状态 $y$ 下，考虑所有可行策略后所能获得的最大价值。\n",
    "\n",
    "如果一个策略 $\\sigma \\in \\Sigma$ 在所有 $y \\in \\mathbb R_+$ 上都能达到 {eq}`vfcsdp0` 中的上确界，则称其为**最优**策略。\n",
    "\n",
    "### 贝尔曼方程\n",
    "\n",
    "在我们对效用函数和生产函数的假设下，按照{eq}`vfcsdp0` 定义的价值函数也满足一个**贝尔曼方程**。\n",
    "\n",
    "对于本问题，贝尔曼方程的形式为\n",
    "\n",
    "```{math}\n",
    ":label: fpb30\n",
    "\n",
    "v(y) = \\max_{0 \\leq c \\leq y}\n",
    "    \\left\\{\n",
    "        u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "    \\right\\}\n",
    "\\qquad (y \\in \\mathbb R_+)\n",
    "```\n",
    "\n",
    "这是一个关于 $v$ 的*函数方程*。\n",
    "\n",
    "其中项 $\\int v(f(y - c) z) \\phi(dz)$ 可以理解为在以下条件下的下一期期望价值：\n",
    "\n",
    "* 使用 $v$ 来衡量价值\n",
    "* 当前状态为 $y$\n",
    "* 消费选择为 $c$\n",
    "\n",
    "如 [EDTC](http://johnstachurski.net/edtc.html) 的定理10.1.11和其他文献所示：\n",
    "\n",
    "> *价值函数* $v^*$ *满足贝尔曼方程*\n",
    "\n",
    "换句话说，当 $v=v^*$ 时，{eq}`fpb30` 成立。\n",
    "\n",
    "直观上来说，从给定状态出发的最大价值可以通过在以下两者之间进行最优权衡获得：\n",
    "\n",
    "* 当前行动带来的即时回报，与\n",
    "* 由该行动导致的未来状态所带来的贴现期望价值。\n",
    "\n",
    "贝尔曼方程的重要性在于，它为我们提供了更多关于价值函数的信息。\n",
    "\n",
    "它同时也是一种计算价值函数的方式，我们将在后续进一步讨论。\n",
    "\n",
    "### 逐期最优策略\n",
    "\n",
    "价值函数的主要意义在于：它可以帮助我们计算最优策略。\n",
    "\n",
    "具体如下：\n",
    "\n",
    "设 $v$ 是 $\\mathbb R_+$ 上的一个连续函数。我们称某一策略 $\\sigma \\in \\Sigma$ 是 $v$-**逐期最优的**，如果对所有 $y \\in \\mathbb R_+$，$\\sigma(y)$ 是下式的解：\n",
    "\n",
    "```{math}\n",
    ":label: defgp20\n",
    "\n",
    "\\max_{0 \\leq c \\leq y}\n",
    "    \\left\\{\n",
    "    u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "    \\right\\}\n",
    "```\n",
    "\n",
    "换句话说，当 $v$ 被视为价值函数时，如果 $\\sigma \\in \\Sigma$ 能够最优地权衡当前和未来回报，那么它就是 $v$-逐期最优的。\n",
    "\n",
    "在我们的设定中，有如下关键结果：\n",
    "\n",
    "* 一个可行的消费策略是最优的，当且仅当它是$v^*$-逐期最优的。\n",
    "\n",
    "这一结论的直观解释与贝尔曼方程的类似（参见{eq}`fpb30`之后的说明）。\n",
    "\n",
    "参见[EDTC](http://johnstachurski.net/edtc.html)的定理10.1.11。\n",
    "\n",
    "因此，一旦我们得到了对 $v^*$ 的一个良好近似，就可以通过计算相应的逐期最优策略来获得（近似）最优策略。\n",
    "\n",
    "这样做的优势在于：我们现在解决的是一个低维优化问题，而不是原始的高维动态优化问题。\n",
    "\n",
    "### 贝尔曼算子\n",
    "\n",
    "那么，如何计算价值函数呢？\n",
    "\n",
    "一种方法是使用所谓的**贝尔曼算子**。\n",
    "\n",
    "（算子是一类将函数映射到函数的映射。）\n",
    "\n",
    "贝尔曼算子记为 $T$，其定义为\n",
    "\n",
    "```{math}\n",
    ":label: fcbell20_optgrowth\n",
    "\n",
    "Tv(y) := \\max_{0 \\leq c \\leq y}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v(f(y - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "\\qquad (y \\in \\mathbb R_+)\n",
    "```\n",
    "\n",
    "换句话说，$T$ 将函数 $v$ 映射为新的函数 $Tv$， 其形式由{eq}`fcbell20_optgrowth`给出。\n",
    "\n",
    "根据构造，贝尔曼方程{eq}`fpb30`的解集*恰好等于* $T$ 的不动点集。\n",
    "\n",
    "例如，如果 $Tv = v$，那么对于任意 $y \\geq 0$，\n",
    "\n",
    "$$\n",
    "v(y)\n",
    "= Tv(y)\n",
    "= \\max_{0 \\leq c \\leq y}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v^*(f(y - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "这正好说明 $v$ 是贝尔曼方程的一个解。\n",
    "\n",
    "由此可知 $v^*$ 是 $T$ 的一个不动点。\n",
    "\n",
    "### 理论结果回顾\n",
    "\n",
    "```{index} single: Dynamic Programming; Theory\n",
    "```\n",
    "\n",
    "可以证明，算子 $T$ 在连续有界函数空间上是一个压缩映射，其度量是上确界距离：\n",
    "\n",
    "$$\n",
    "\\rho(g, h) = \\sup_{y \\geq 0} |g(y) - h(y)|\n",
    "$$\n",
    "\n",
    "参见 [EDTC](http://johnstachurski.net/edtc.html)的引理10.1.18。\n",
    "\n",
    "因此，在该空间中，算子 $T$ 仅有一个不动点，而我们知道该不动点就是价值函数。\n",
    "\n",
    "由此可知\n",
    "\n",
    "* 值函数 $v^*$ 是有界且连续的。\n",
    "* 从任意有界且连续的 $v$ 开始，通过迭代应用 $T$ 生成的序列 $v, Tv, T^2v, \\ldots$ 将一致收敛到 $v^*$。\n",
    "\n",
    "这种迭代方法被称为**价值函数迭代**。\n",
    "\n",
    "我们还知道：若某一可行消费策略是 $v^*$-逐期最优的，那么它就是最优策略。\n",
    "\n",
    "[EDTC](http://johnstachurski.net/edtc.html) 的定理10.1.11）可以证明一个$v^*$-逐期最优策略是存在的。\n",
    "\n",
    "因此，至少存在一个最优策略。\n",
    "\n",
    "我们的问题现在转变为：如何计算它。\n",
    "\n",
    "### {index}`无界效用 <single: Unbounded Utility>`\n",
    "\n",
    "```{index} single: Dynamic Programming; Unbounded Utility\n",
    "```\n",
    "\n",
    "前面所述结果都假设效用函数是有界的。\n",
    "\n",
    "但在实际研究中，经济学家往往使用无界效用函数 —— 我们也会如此。\n",
    "\n",
    "在无界情形下，仍然存在一些最优性理论。\n",
    "\n",
    "但遗憾的是，这些理论往往具有针对性，而不是适用于大范围应用。\n",
    "\n",
    "尽管如此，它们的主要结论通常与有界情形下的一致（只需去掉“有界”的限定）。\n",
    "\n",
    "可以参考，例如[EDTC](http://johnstachurski.net/edtc.html) 的第12.2节、{cite}`Kamihigashi2012` 或 {cite}`MV2010`。\n",
    "\n",
    "## 计算\n",
    "\n",
    "```{index} single: Dynamic Programming; Computation\n",
    "```\n",
    "\n",
    "现在让我们来看看如何计算值函数和最优策略。\n",
    "\n",
    "本讲中的实现将着重于清晰性和灵活性。\n",
    "\n",
    "这两点都很有帮助,但会牺牲一些运行速度 —— 当你运行代码时就会看到这一点。\n",
    "\n",
    "在{doc}`后续讲义 <optgrowth_fast>`在，我们会牺牲一些清晰性和灵活性，通过即时编译(JIT compilation)来加速代码。\n",
    "\n",
    "本讲所用的算法是拟合价值函数迭代法，它在之前的{doc}`McCall模型<mccall_fitted_vfi>`和{doc}`吃蛋糕问题<cake_eating_numerical>`中已经介绍过。\n",
    "\n",
    "算法步骤如下：\n",
    "\n",
    "(fvi_alg)=\n",
    "1. 从一组值 $\\{ v_1, \\ldots, v_I \\}$ 开始，这些值代表初始函数 $v$ 在网格点 $\\{ y_1, \\ldots, y_I \\}$ 上的取值。\n",
    "1. 基于这些数据点，通过线性插值在状态空间 $\\mathbb R_+$ 上构建函数 $\\hat v$。\n",
    "1. 通过重复求解{eq}`fcbell20_optgrowth`，获取并记录每个网格点 $y_i$ 上的值 $T \\hat v(y_i)$。\n",
    "1. 除非满足某些停止条件，否则设置 $\\{ v_1, \\ldots, v_I \\} = \\{ T \\hat v(y_1), \\ldots, T \\hat v(y_I) \\}$ 并返回步骤2。\n",
    "\n",
    "### 标量最大化\n",
    "\n",
    "为了最大化贝尔曼方程{eq}`fpb30`的右侧，我们将使用SciPy中的`minimize_scalar`程序。\n",
    "\n",
    "由于我们需要求最大值而不是最小值，因此会利用以下事实：在区间 $[a, b]$ 上函数 $g$ 的最大化等价于 $-g$ 的最小化。\n",
    "\n",
    "为此，并且为了保持接口的整洁，我们将把 `minimize_scalar` 封装在一个外部函数中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    在区间 [a, b] 上最大化函数 g。\n",
    "\n",
    "    我们利用了在任何区间上 g 的最大值点也是 -g 的最小值点这一事实。\n",
    "    元组 args 收集了传递给 g 的任何额外参数。\n",
    "\n",
    "    返回最大值和最大值点。\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ff16b",
   "metadata": {},
   "source": [
    "### 最优增长模型\n",
    "\n",
    "我们暂且假设 $\\phi$ 是 $\\xi := \\exp(\\mu + s \\zeta)$ 的分布，其中\n",
    "\n",
    "* $\\zeta$ 是标准正态分布随机变量，\n",
    "* $\\mu$ 是冲击的位置参数，\n",
    "* $s$ 是冲击的尺度参数。\n",
    "\n",
    "我们将这些和最优增长模型的其他基本要素存储在一个类中。\n",
    "\n",
    "下面定义的类结合了参数，以及一个用于实现贝尔曼方程{eq}`fpb30`右侧的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f032c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalGrowthModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 u,            # 效用函数\n",
    "                 f,            # 生产函数\n",
    "                 β=0.96,       # 贴现因子\n",
    "                 μ=0,          # 冲击位置参数\n",
    "                 s=0.1,        # 冲击尺度参数\n",
    "                 grid_max=4,\n",
    "                 grid_size=120,\n",
    "                 shock_size=250,\n",
    "                 seed=1234):\n",
    "\n",
    "        self.u, self.f, self.β, self.μ, self.s = u, f, β, μ, s\n",
    "\n",
    "        # 设置网格\n",
    "        self.grid = np.linspace(1e-4, grid_max, grid_size)\n",
    "\n",
    "        # 存储冲击（设定随机种子，使结果可重现）\n",
    "        np.random.seed(seed)\n",
    "        self.shocks = np.exp(μ + s * np.random.randn(shock_size))\n",
    "\n",
    "    def state_action_value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        贝尔曼方程的右侧。\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, β, shocks = self.u, self.f, self.β, self.shocks\n",
    "\n",
    "        v = interp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c) + β * np.mean(v(f(y - c) * shocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335f0a6",
   "metadata": {},
   "source": [
    "在倒数第二行中，我们使用线性插值。\n",
    "\n",
    "在最后一行中，{eq}`fcbell20_optgrowth`中的期望值通过[蒙特卡洛法](https://baike.baidu.com/item/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%B3%95/1225057)计算，近似为：\n",
    "\n",
    "$$\n",
    "\\int v(f(y - c) z) \\phi(dz) \\approx \\frac{1}{n} \\sum_{i=1}^n v(f(y - c) \\xi_i)\n",
    "$$\n",
    "\n",
    "其中 $\\{\\xi_i\\}_{i=1}^n$ 是从$\\phi$ 中独立同分布抽取的样本。\n",
    "\n",
    "蒙特卡洛并不总是计算积分最有效的数值方法，但在当前设定下，它确实具有一些理论优势。\n",
    "\n",
    "（例如，它保持了贝尔曼算子的压缩映射性质 -- 参见{cite}`pal2013`。）\n",
    "\n",
    "### 贝尔曼算子\n",
    "\n",
    "下面的函数实现了贝尔曼算子。\n",
    "\n",
    "（我们本可以将其作为`OptimalGrowthModel`类的一个方法，但对于这种数值计算工作，我们更倾向于使用小型类而不是大而全的单一类。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2876a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(v, og):\n",
    "    \"\"\"\n",
    "    贝尔曼算子。\n",
    "    更新值函数的猜测值，并同时计算v-逐期最优策略。\n",
    "\n",
    "      * og是OptimalGrowthModel的一个实例\n",
    "      * v是表示价值函数猜测的数组\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "    v_greedy = np.empty_like(v)\n",
    "\n",
    "    for i in range(len(grid)):\n",
    "        y = grid[i]\n",
    "\n",
    "        # 在状态y下最大化贝尔曼方程右侧\n",
    "        c_star, v_max = maximize(og.state_action_value, 1e-10, y, (y, v))\n",
    "        v_new[i] = v_max\n",
    "        v_greedy[i] = c_star\n",
    "\n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded32fb1",
   "metadata": {},
   "source": [
    "(benchmark_growth_mod)=\n",
    "### 一个示例\n",
    "\n",
    "假设现在\n",
    "\n",
    "$$\n",
    "f(k) = k^{\\alpha}\n",
    "\\quad \\text{且} \\quad\n",
    "u(c) = \\ln c\n",
    "$$\n",
    "\n",
    "在这一特定问题中，可以得到一个精确的解析解（参见{cite}`Ljungqvist2012`第3.1.2节），其形式为\n",
    "\n",
    "```{math}\n",
    ":label: dpi_tv\n",
    "\n",
    "v^*(y) =\n",
    "\\frac{\\ln (1 - \\alpha \\beta) }{ 1 - \\beta} +\n",
    "\\frac{(\\mu + \\alpha \\ln (\\alpha \\beta))}{1 - \\alpha}\n",
    " \\left[\n",
    "     \\frac{1}{1- \\beta} - \\frac{1}{1 - \\alpha \\beta}\n",
    " \\right] +\n",
    " \\frac{1}{1 - \\alpha \\beta} \\ln y\n",
    "```\n",
    "\n",
    "最优消费策略为\n",
    "\n",
    "$$\n",
    "\\sigma^*(y) = (1 - \\alpha \\beta ) y\n",
    "$$\n",
    "\n",
    "有这些封闭形式解的价值在于：它们使我们能够检验代码在这一具体情形下是否正确。\n",
    "\n",
    "在Python中，上述函数可以表示为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62738298",
   "metadata": {
    "load": "_static/lecture_specific/optgrowth/cd_analytical.py"
   },
   "outputs": [],
   "source": [
    "\n",
    "def v_star(y, α, β, μ):\n",
    "    \"\"\"\n",
    "    真实价值函数\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(y)\n",
    "\n",
    "def σ_star(y, α, β):\n",
    "    \"\"\"\n",
    "    真实最优策略\n",
    "    \"\"\"\n",
    "    return (1 - α * β) * y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f178dc",
   "metadata": {},
   "source": [
    "接下来让我们用上述基本要素创建一个模型实例，并将其赋值给变量`og`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114624ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.4\n",
    "def fcd(k):\n",
    "    return k**α\n",
    "\n",
    "og = OptimalGrowthModel(u=np.log, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e227426",
   "metadata": {},
   "source": [
    "现在让我们看看当我们将贝尔曼算子应用于这种情况下的精确解 $v^*$ 时会发生什么。\n",
    "\n",
    "理论上，由于 $v^*$ 是一个不动点，得到的函数应该仍然是 $v^*$。\n",
    "\n",
    "在实践中，我们预计会有一些小的数值误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = og.grid\n",
    "\n",
    "v_init = v_star(grid, α, og.β, og.μ)    # 从解开始\n",
    "v_greedy, v = T(v_init, og)             # 应用一次T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(-35, -24)\n",
    "ax.plot(grid, v, lw=2, alpha=0.6, label='$Tv^*$')\n",
    "ax.plot(grid, v_init, lw=2, alpha=0.6, label='$v^*$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188a09d",
   "metadata": {},
   "source": [
    "这两个函数本质上没有区别，所以我们开始得很顺利。\n",
    "\n",
    "现在让我们看看从任意初始条件开始，如何用贝尔曼算子进行迭代。\n",
    "\n",
    "我们随意地将初始条件设定为 $v(y) = 5 \\ln (y)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 5 * np.log(grid)  # 初始条件\n",
    "n = 35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v, color=plt.cm.jet(0),\n",
    "        lw=2, alpha=0.6, label='初始条件')\n",
    "\n",
    "for i in range(n):\n",
    "    v_greedy, v = T(v, og)  # 应用贝尔曼算子\n",
    "    ax.plot(grid, v, color=plt.cm.jet(i / n), lw=2, alpha=0.6)\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, og.β, og.μ), 'k-', lw=2,\n",
    "        alpha=0.8, label='真实的价值函数')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(ylim=(-40, 10), xlim=(np.min(grid), np.max(grid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa7260",
   "metadata": {},
   "source": [
    "图中显示了\n",
    "\n",
    "1. 由拟合值迭代算法生成的前36个函数，颜色越热表示迭代次数越高\n",
    "1. 用黑色线条绘制真实的价值函数 $v^*$\n",
    "\n",
    "迭代序列逐渐收敛至 $v^*$。\n",
    "\n",
    "我们显然正在接近目标。\n",
    "\n",
    "### 迭代至收敛\n",
    "\n",
    "我们可以编写一个函数，使其迭代直到差异小于特定的容差水平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da57495",
   "metadata": {
    "load": "_static/lecture_specific/optgrowth/solve_model.py"
   },
   "outputs": [],
   "source": [
    "def solve_model(og,\n",
    "                tol=1e-4,\n",
    "                max_iter=1000,\n",
    "                verbose=True,\n",
    "                print_skip=25):\n",
    "    \"\"\"\n",
    "    通过迭代贝尔曼算子求解\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 设置迭代循环\n",
    "    v = og.u(og.grid)  # 初始条件\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_greedy, v_new = T(v, og)\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"第 {i} 次迭代的误差为 {error}。\")\n",
    "        v = v_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"未能收敛！\")\n",
    "    elif verbose:\n",
    "        print(f\"\\n在 {i} 次迭代后收敛。\")\n",
    "\n",
    "    return v_greedy, v_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255fa6f",
   "metadata": {},
   "source": [
    "让我们使用这个函数在默认设置下计算一个近似解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_greedy, v_solution = solve_model(og)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0cb97",
   "metadata": {},
   "source": [
    "现在我们通过将结果与真实值作图比较来检验其准确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_solution, lw=2, alpha=0.6,\n",
    "        label='近似的价值函数')\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, og.β, og.μ), lw=2,\n",
    "        alpha=0.6, label='真实的价值函数')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(-35, -24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfd29e",
   "metadata": {},
   "source": [
    "图表显示我们的结果非常准确。\n",
    "\n",
    "### 策略函数\n",
    "\n",
    "```{index} single: Optimal Growth; Policy Function\n",
    "```\n",
    "\n",
    "上面计算的策略`v_greedy`对应于一个近似最优策略。\n",
    "\n",
    "下图将其与精确解进行比较，如上所述，精确解为 $\\sigma(y) = (1 - \\alpha \\beta) y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='近似的策略函数')\n",
    "\n",
    "ax.plot(grid, σ_star(grid, α, og.β), '--',\n",
    "        lw=2, alpha=0.6, label='真实的策略函数')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d0288",
   "metadata": {},
   "source": [
    "图表显示我们在这个例子中很好地近似了真实的策略。\n",
    "\n",
    "## 练习\n",
    "\n",
    "\n",
    "```{exercise}\n",
    ":label: og_ex1\n",
    "\n",
    "在这类模型中，效用函数的常见选择是CRRA形式\n",
    "\n",
    "$$\n",
    "u(c) = \\frac{c^{1 - \\gamma}} {1 - \\gamma}\n",
    "$$\n",
    "\n",
    "保持其他默认设置（包括柯布-道格拉斯生产函数），用这个效用函数求解最优增长模型。\n",
    "\n",
    "设定 $\\gamma = 1.5$，计算并绘制最优策略的估计值。\n",
    "\n",
    "记录这个函数运行所需的时间，以便与{doc}`下一讲<optgrowth_fast>`中开发的更快代码进行比较。\n",
    "```\n",
    "\n",
    "```{solution-start} og_ex1\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "首先，设置模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 1.5   # 偏好参数\n",
    "\n",
    "def u_crra(c):\n",
    "    return (c**(1 - γ) - 1) / (1 - γ)\n",
    "\n",
    "og = OptimalGrowthModel(u=u_crra, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466347d2",
   "metadata": {},
   "source": [
    "现在让我们运行它，并计时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v_greedy, v_solution = solve_model(og)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394e33d",
   "metadata": {},
   "source": [
    "让我们绘制策略函数，看看它的样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='近似的最优策略')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172ab33",
   "metadata": {},
   "source": [
    "```{solution-end}\n",
    "```\n",
    "\n",
    "```{exercise}\n",
    ":label: og_ex2\n",
    "\n",
    "计时从初始条件 $v(y) = u(y)$ 开始，使用贝尔曼算子迭代20次所需的时间。\n",
    "\n",
    "使用前一个练习中的模型设置。\n",
    "\n",
    "(和之前一样，我们会将这个数字与{doc}`下一讲<optgrowth_fast>`中更快的代码进行比较。)\n",
    "```\n",
    "\n",
    "```{solution-start} og_ex2\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "让我们设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = OptimalGrowthModel(u=u_crra, f=fcd)\n",
    "v = og.u(og.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e577e",
   "metadata": {},
   "source": [
    "这是计时结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(20):\n",
    "    v_greedy, v_new = T(v, og)\n",
    "    v = v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cf69c",
   "metadata": {},
   "source": [
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   10,
   58,
   69,
   445,
   460,
   474,
   507,
   529,
   551,
   588,
   590,
   594,
   600,
   608,
   620,
   628,
   647,
   662,
   664,
   668,
   670,
   674,
   686,
   699,
   710,
   739,
   746,
   751,
   754,
   758,
   766,
   787,
   790,
   794,
   800
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}